

AWS Region encompasses multiple Availability Zones, and each AZ is designed to be independent and resilient



Region:
   A physical location around the world where AWS clusters data centers

AZ (Availability Zone)
  A set of data centers located within an AWS Region





/////////////////////
[IAM]


Group only contain users, no sub-Group
IAM is a global service, not regional
AMIs are built for a specific AWS Region, they're unique for each AWS Region. 


# Security Group:

Security Group can be attached to multiple EC2 instances within the same AWS Region/VPC

Security Group vs ACL:
- Security group is the firewall of EC2 Instances.
- Network ACL is the firewall of the VPC Subnets

Security Group	                            Network Access Control List
--------------------------------------------------------------------------
operate at instance level.	                operate subnet level.
support only allow rules.	                support allow & deny rules.
stateful                                    stateless, it return traffic must be allowed explicitly.
cannot block specific IP                    can block specific IP Address using NACL.
All rules evaluated before decision.     	Rules are processed in number order when deciding whether allow traffic.
start with instance launch configuration.	assigned to subnet for all instance.
applies when someone specifies SGs          do not depend on user it automatically apply all instances with subnet.
  when launching the instance.	



//////////////////////
[EC2]

Options:


- Spot Fleet
  a set of Spot Instances and optionally On-demand Instances. It allows you to automatically request Spot Instances with the lowest price.



Elastic IP:  5 per account
But suggest to use random public IP + DNS, or use LB.

- EC2 Instance Placement Groups:
  Cluster:   in a single AZ (Availability Zone)
             High network performance, 
  Spread:    instances spread across hardware, limit max 7 instance per AZ per PlacementGroup
             High availability
  Partition: instances spread across particions across AZs, (max 100 instance, up to 7 partitions per AZ)
             Medium network, medium availability

- ENI
  logic Network Card in VPC bound to a specific AZ.
  Can attach to EC2 instances on the fly.  (failover case)

- Hibernate
  Dump RAM to EBS.

- EBS (Elastic Block Store)
  EBS is a network drive attach to an EC2 instance.
  One EBS can only be mounted to one instance at a time (at CCP level), but EC2 can have multiple EBS attached.
  EBS is bound to a specific AZ
  Delete on termination:
    root EBS is deleted by default on EC2 termination (but canb e configured to preserve)
    other EBS is not deleted by default on EC2 termination.
  Use EBS Snapshot to transfer (copy) volume across AZ or Region.

  EBS Volume Type
  - gp2/gp3 (SSD):               General Purpose.   (gp3: independent IOPS/Throughput)
                                 IOPS: 3000 ~ 16000, 
                                 throughput: 125~1000 MB/s
  - io1/io2 Block Express (SSD): Low latency High throughput
                                 IOPS: [io1] 32000/64000(Nitro EC2), [io2] 256000
                                 Throughput: 
                                 Support Multi-attach
  - sl (HDD):                    Low cost throughput indensive
                                 IOPS: 500
  - sc (HDD):                    Lowest cost low frequent access (Cold)
  Bootable EBS:gp2/gp3, io1/io2; (HDD cannot be used as bootable disk)


  - EBS instance-store,          ephemeral, physically attached to EC2, very high IOPS/Throughput
                                 IPOS: ~1400000
                                 Throughput
  - EBS Multi-attach (io1/io2)   EBS:EC2(within same AZ) = 1:N, 
                                 max=16, 
                                 cluster-aware file system

- EFS (Elastic File System)
  EFS,  multi EC2 concurrent access, across-AZ/Region, scalable, 

  Storage-class:
  - Standard
  - EFS-IA (Infrequent Access),  with Lifecycle Policy
           Availability: Regional (Across AZ) | One-Zone (single AZ)

  - EBS               vs               EFS    
    one instnce                      multi-instances (100)
    (except multi-attach io1/io2)
    same AZ                          Across AZ




///////////////////////////
[ELB]

LB listener:
  A listener is a logical entity within a load balancer that checks for incoming traffic based on the configured protocol and port.

Type of LB
- CLB (Classic LB),  old Gen, to obsolete
  HTTP/HTTPS, SSL, TCP
- ALB (Application LB),  L7
  LB to multiple application within or across machines
  >> Great fit to micro-service/containerized-application
- NLB (Netowrk LB)
  TCP, UDP, TLS,
- GWLB (Gateway LB)
  L3 (IP)


LB SG (Security Group)
EC2 only allow traffic from LB.

ALB (Application LB)
  L7 HTTP/HTTPS, websocket
  Supprot port-mapping
  Direct traffic to Target-groups: EC2 instances/EC2 tasks/Lambda functions/Private-IP
    >> ALBs can route traffic to different Target Groups based on URL Path, Hostname, HTTP Headers, and Query Strings.
    >> ALB can redirect all HTTP requests to HTTPS by config listener rule -
       on HTTP lister, redict HTTP to HTTPS, and create HTTPS listener to handle client requests (been redirected)
  ALB terminate client IP, (using ALB IP to communicate with target executor)
  ALB comes with a fixed hostname/url
  Latency:  400ms
  >> You can't attach an Elastic IP address to Application Load Balancers.

NLB (Network LB),  
  L4 TCP/UDP
  IP: One static IP per AZ, support Elastic IP   (==> support 1,2,3 IPs to access)
  Direct traffic to Target-groups: EC2 instances, IP addresses (Private), ALB
  Health check support: TCP, HTTP, HTTPS
  RPS:     million
  Latency: 100ms
  >> NLB provides the highest performance and lowest latency if your application needs it.

  Use cases:   We can deploy NLB infront of ALB.   NLB provide fixed static IP

GWLB
  L3 IP
  Function: GW and LB
            Deploy a fleet of 3rd party Network Virtual Apps in AWS (eg. FW, IDP, DPI)
  Protocol: GENEVE on port 6081,
  Target Groups:  EC2 instances, IP Addresses (private)

  Users (source) ----> =GWLB= ---> Application (dest)
                       |    ^
              (GENEVE) |    |
                       |  Security APPs
                        > (Firewall, Intrution Detect, Deep Packet Instpection ...)


Access LB via DNS/IP
Only Network Load Balancer provides both static DNS name and static IP. 
Application Load Balancer provides a static DNS name but it does NOT provide a static IP.


ELB reserved cookie names:  AWSALB, AWSALBAPP, AWSALBTG


Sticky Sessions
  client --stick--> backend instances
    (cookie)
  work for CLB, ALB, NLB.

  Cookie type:
  - Application-based Cookies
    >> Customer cookie:
       Generated by target, 
       use target group specific name. (not LB reserved name)
    >> Application cookie: 
       Generated by LB.
       name: AWSALBAPP
  - Duration-based Cookies
    Generted by LB
    name: AWSALB (for ALB), AWSELB (for CLB)

Client IP:
  When using an ALB, the IP address the backend receives requests from will be the ALB's private IP .
  To get the client's IP, ALB adds an additional header called "X-Forwarded-For" contains the client's IP address.


Cross-Zone LB
  each LB distribute evently accross all the registered instances in all AZ.

  By default:
    ALB: enabled. no charge for inter-AZ data
    CLB: disabled. no charge for inter-AZ data
    NLB: disabled. charge for inter-AZ data
    GWLB: disabled. charge for inter-AZ data

  Cross-Zone enabled:  LB per instances of all AZs
             disabled: LB per AZs

  eg.
    AZ1 (2 instances), AZ2 (8 instances):
    Cross-zone LB enabled:  each instance gets 1/10
                 disabled:  each AZ get 1/2


ELB SSL Certificate
  ACM (AWS Certicifate Manager)
  SNI (Server Name Indication) 
    SNI allows you to expose multiple HTTPS applications each with its own SSL certificate behind the same listener (LB). 
       by binding multiple certs to the same secure listener on the LB
    >> allow loading multiple SSL cert for one web server
       only works for ALB, NLB, CloudFront.

  CLB(v1): support only one SSL Certificate
  ALB(v2): support multiple listensers with multiple SSL certs, with SNI
  NLB(v2): support multiple listensers with multiple SSL certs, with SNI


Connection Draining:
(The timer for graceful termination of existing connection)
typical use cases:
  scaling (up/down), graceful shutdown, deregistration at health-check failure 


ASG (Auto-Scaling-Group)
  goal:
    - scale out (+) / scale in (-) per load
    - add/remove EC2 instances on health check
    - ensure min/max number of EC2 instances
  setting: 
    min-capacity, desired-capacity, max-capacity
  Lauch Template:
    - AMI, Instance type, EBS volumes, User data, SG, IAM role, ssh key-pairs, network/subnet, LB, 
  Scale triggering:
    - metrics,  e.g. EC2 requests target, network I/O, 
    - CloudWatch Alarms (e.g. Avg CPU, or customize metrics)
    - health check
  Scaling Policy
    - Dynamic Scaling:
        Target Tracking Scaling (e.g avg CPU stay at 40%)
        Simple/Step Scaling: CloudWatch alram trigger. e.g (CPU>70% +1, CPU<30% -1)
    - Scheduled Scaling:
    - Predictive Scaling:(based on continous forcast)
  Scaling Cooldown (stablize).   default 300s

  >> You can configure the Auto Scaling Group to determine the EC2 instances' health based on Application Load Balancer Health Checks instead of EC2 Status Checks (default).
     When an EC2 instance fails the ALB Health Checks, it is marked unhealthy and will be terminated while the ASG launches a new EC2 instance



////////////////////////////////
[RDS]
(Amazon Relational Database Service)

Support:
  Postgres, MySQL, MariaDB, Oracle, Microsft SQL Server, IBM DB2, Aurora (AWS)

Advantages:
  - Auto provision, OS patch
  - Auto backup/restore
  - Read replicas
  - Auto scaling (Horizontal & Vertical)
  - Multi Availability zones and DR (Disaster recovery)
  - Storage backed by EBS (gp2 or io1)
  - Mornitoring Dashboard
Limitation:
  - User cannot ssh into the DB instances

Storage Auto-Scaling.
  trigger: e.g.
  - free storage < ?%
  - low storage at least 5 min
  - xx hours since last modifiction


RDS Read Replicas 
  - up to 15
  - within AZ or cross AZ/Region
  - Replication is Asynchronous, -- eventually consistent   *
  - can be promoted to their own DB.
  - need to update the connection string to leverage the read replicas   *
    (however, Multi-AZ keeps the same connection string regardless of which database is up.)

  replication traffic is free within same region, even if across AZ

RDS Multiple AZ 
(for Disaster Recovery)
  - Synchronous Replication.   *
    When enable Multi-AZ, RDS automatically creates Standby replica in different AZ within the same Region.
  - One same DNS name, auto APP failover
  - Not used for scaling

  >> Multi-AZ deployments are not designed for cross-region HA; they focus on ensuring availability within a single Region12
  
  >> The read replicas can be setup as Multi-AZ for Disaster Recovery.
  >> How to make RDS from single-AZ to multi-AZ, with zero downtime
     - modify DB setting to change SZ to MZ. behind the scene
       1> DB snapshot taken
       2> Standby DB created from DB snapshot
       3> Sync replication start b/w master & standby
  >> Read Replicas will help when analytics query slows down the main RDS DB,
     as your analytics application can now perform queries against read-replica(s), and these queries won't impact the main production RDS database.


Synchronous or Asynchronous Replication:
  RDS support both Synchronous and Asynchronous replication.
  - RDS Multi-AZ uses synchronous replication for high availability on multi-AZs within the same AWS Region.
    Replication occurs synchronously between the primary and standby replica WITHIN the same Region
    (Multi-AZ only support same Region)
  - Read replica:
    RDS Read Replica use Asynchronous replication -- enventual consistent
  - Cross-region read replica: 
    Asynchronous replication.
    Cross-region read replicas provide global read scalability and disaster recovery, but with replication lag.

  >> For multi-AZ RDS replication, synchronous replication is used.
  >> For non-Multi-AZ deployments or cross-region replication, asynchronous replication is used.
  >> In contrast, Aurora uses synchronous replication for both within-region and cross-region deployments.

RDS Custom
  support Oracle and MS SQL DB.
  Access the underlying DB and OS, to
  - config setting, instal patches, enable native features, 
  - access the underlying EC2 instance using SSH or SSM Session Manager
  Need to de-activate automation Mode to perform the customization
  RDS s RDS custom:
  - RDS: DB and OS fully managed by AWS
  - RDS Custom: full admin access to the underlying OS and DB.


Amazon Aurora
  AWS proprietary .
  Aurora support both Postgres and MySQL
  AWS cloud optimized:  Performance x5 (MySQL on RDS), x3 (Postgres on RDS)
  Auto expanding (storage grow), 10G ~ upto 128T.
  Master + up to 15 replicas, faster replicatin process than MySQL (sub 10 ms replica lag)
  Auto scaling (read replicas) and push-button scaling
  HA native, auto failover/backup & recovery (backtrack: to restore at any point of time)
  cost +20% than RDS.

  Aurora HA:
  - 6 copies across 3 AZ (1 primary, 5 secondary DB cluster)
  - 4/6 for Write, 3/3 for read
  - self healing with peer-peer replication
  - storage striped across 100s of volumes. (Shared storage volume). Autom expand.
  - within-region replication: synchronous between primary and secondary clusters
  - support cross region replication (Synchronous)
  - failover < 30s

  RDS Multi-AZ vs Aurora Global Database:
  - Choose RDS Multi-AZ for simpler HA needs within a single region.
    RDS Provides HA on different AZ within the same AWS Region
    RDS cross region read replicas are Asynchronous and serve READ workloads.  (so it is not a fully R/W HA across region)
  - Choose Aurora Global Database for global HA, low-latency reads, and disaster recovery across multiple AWS Regions. (Synchronous replication)


  Write Endpoint (point to Master)
  Reader Endpoint (Conntion Load Balancing)
             Client 
           /        \ 
      (Write)      (Read)
        /              \
    Write EP     Read EP1, EP2, ...
       |               |   |
     [===Shared Storage Volume=== ]


  Aurora Replicas Auto Scaling

  Custom Endpoint
  - define a subnset of Aurora instances as a Custom Endpoint
    e.g. Run analytical queries on specific replicas.
  - Reader Endpoint is generally not used after defining Custom Endpoints
  >> we can setup multiple Custom Endpoints for different type (subsets) of queries.

  Aurora Serverless
  - 'managed Aurora'
  - Automated DB instantiation and auto-scaling ased on actual usage
  - no capacity planning needed.
  - pay per sec
  - good for in-frequent, intermittent or unpredicatable workloads.

  Global Aurora
  - Cross Region Read Replicas
  - 1 Primary Region (R/W) + up to 5 Secondary (RO) regions. Replica lag < 1s cross region
    up to 15 replicas in each secondary region
  - Promoting another region (for disaster recovery), RTO < 1min
  >> max 5 secondary region, max 15 replicas in each region.   *

  Aurora Machine Learning
  - ML based prediction to applications
  - optimized and secure integration b/w Aurora and AWS ML services
  - Support: Amazon SageMaker (any ML model), Comprehend (for sentiment analysis)
  use cases: frad detection, ads targeting, product recommendation, sentiment analysis etc

  Aurora DB cloning
  - Create a new DB cluster from the existing one
  - Faster than Snapshot & restore, 
  - using copy-on-write protocol
    init new DB using the same DB volume (fast and no copying)
    when updates are made, additional storage is allocated and data is copied to be separated.
  - Fast & cost-effective
  >> Use case: user create 'staging' DB from a 'production' DB without impacting the prod DB

RDS & Aurara Backup & restore
  Backup
  - Automated backups.  (1~35 days of retention), 
        Can be disabled in RDS, cannot be disabled for Aurora.
    Full backup (daily)
    Transaction log backup every 5s
    Restore to any point
  - Manual DB Snapshots. (retain as long as you want)
  >> You will still pay for storage in a stopped RDS DB.
     Use snapshot & restore if you plan to stop it for long time. (cost much less)
  Restore
  - Restore RDS/Aurora backup or snapshot to create a new DB.
  - Restore MySQL RDS DB/Aurora cluster from S3
    Use case:
      Create backup of on-premises DB, (For Aurora needs to use Percona XtraBackup)
      store in AWS S3
      Restore the backup into a new RDS instance/Aurora cluster
  
RDS & Aurora Security
  - At-rest encryption
    DB master & replicas encryptioned using AWS-KMS
    must define at launch time
    if master is not encrypted, the read replicas can NOT be encrytped
  - In-flight encryption
    TLS-ready by default.
    use AWS TLS root certificate clients
  - IAM Authentication - connect DB using IAM roles (instead of usr/pwd)
  - Security Groups -  control network access to RDS/Aurora
  - No SSH except on RDS Custom
  - Audit logs to CloudWatch


RDS Proxy
  Fully managed DB proxy for RDS
  - Allows apps to pool and share the DB connections
  - improve efficiency
  - Serverless, auto-scaling, HA (multi-AZ)
  - Redulce RDS & Aurora failover time by up 66%
  - No code change required for most apps
  - Enforce IAM AUthentication for DB, securely store credentials in AWS Secrets Manager
  - RDS Proxy is NEVER public accessible. must access from VPC
    VPC:  Lambda functions ---(private subnet)--- RDS Proxy -- RDS DB Instance(s)


ElastiCache
  Managed Redis or Memcached.
  - In-memory DB (High performance, low latency)
  - Reduce read-intensive workloads
  - Helps to make your application stateless
  - AWS take care of OS maintentance/patching, optimization, setup, config, monitor, backup, failure recovery
  - Using ElastiCache involves HEAVY APP code changes.

  Solution
  - DB Cache
  - User session store (App writes session data into ElastiCache)
  >> Storing Session Data in ElastiCache is a common pattern to ensuring different EC2 instances can retrieve your user's state if needed.

  Redis vs Memcached
  Redis is an in memory DB with HA (using Replication);   
    Rich feature, support complex data types. 
    Replication, Data persistent, HA, Backup & Restore
  Memcached is a distributed in memory DB WITHOUT HA (using Sharding).  
    Simiple (only string data), Fast Key-Value storage. Multi-threaded performance
    Sharding, No Data-persistent, No HA, No Backup & Restore

      REDIS                                 MEMCACHED
    ----------------------------------------------------------  
                                            Sharding (Multi-node for Data partitioning)
                                            Multi-thread architecture
    Multi-AZ/Auto-Failover                  No multi-AZ/Failover
    ReadReplicas & HA                       No replication, No HA
    Data Durability with AOF persistance,   No data persistent. Data lost at restart
    Backup & Restore                        No backup & restore
    Support Set/Sorted-sets

  'AOF': Append Only File.  -- every write operation is logged into file, the cache can be re-constructed automatically at catastrophic event.

  ElastiCache Security
  - support IAM Authentication
    to enhance the security of ElastiCache Redis Cluster by allowing users to access your ElastiCache Redis Cluster using IAM Identities
  - IAM policy on ElastiCache is only used for AWS API-level Security
  - Redis AUTH:    set password/token for Redis Cluster
  - MemCached: suport SASL-based authentication 

  Patterns for ElastiCache
  - Lazy loading.  all read data is cached. (data can become stale in cache)
  - Write Throughput.  (no stale data)
  - Session Store.   store temporary session data (using TTL feature)

  Use Case:
  Redis:
    Gaming Leaderboards.
    Redis Sorted set




//////////////////////////////
Route53
  A HA, Sclable, fully managed and Authoritative DNS
  Route53 is also a Domain Registrar
  Support reource health check
  The only AWS service with 100% availabiity SLA

Concept:
  TLD: Top Level Domain
  SLD: Second Level Domain
  Authoritative: => the customer can update the DNS records.


Route53 - records
  Record contain:
  - Domain/sub-domain name
  - Record Type: (basic) A/AAAA/CNAME/NS     *
                 (advanced) CAA/DS/MX/NAPTR/SOA/TXT/SPF/SRV
  - Value,  e.g. 1.2.3.4
  - Routing Policy
  - TTL

DNS Records Type:
- A:     IPv4
- AAAA:  IPv6
- CNAME: map a hostname to another hostname
    The target domain must have an A or AAAA record
    Can NOT create a CNAME record for the top node of a DNS namspace (Zone Apex)
    e.g.  you cannot create CNAME for example.com,
          you can create CNAME for www.example.com
- NS:    Name Servers for the hosted Zone

Route53 - Hosted Zones
  A container for records that define how to route traffic to a domain and its subdomains
  - Public Hosted Zones: 
    contains records specify how to route to the Internet (public domain names)
  - Private Hosted Zones:
    contains records specify how to route within VPC(s) (private domain names)
  - cost:  $0.5 per month per Hosted Zone

  Public vs Private Hosted Zones

Route53 - Register A Domain

Route53 - Records TTL (Time To Live)
  The time the client cache the DNS query result.
  TTL is mandatory for DNS record except for Alias records.

Route53 - Alias Records
  - map a hostname to an AWS resource
  - An extension to DNS functions
  - Automatically recorgnizes changes in the resource's IP address
  - Can be used for the TOP(Root) node of a DNS namespace (Zone Apex). e.g. example.com
  - Can NOT set TTL for Alias record. (Instead it is set automatically by route53)
Alias Records Targets:
  ELB, CloudFront Distributions, API Gateway, Elastic Beanstalk Env, S3 websites, VPC Interface Endpoint, Global Accelerator, Route53 record in the same hosted zone
  >> you CANNOT set an Alias record for an EC2 DNS name

Route53 CNAME vs Alias
  CNAME:  point the hostname to any other hostname. (ONLY for NON Root domain)
  Alias:  point the hostname to an AWS Resource. (works for both Root and Non-Root domain)
          Free


Route53 - Routing Policies
  Define how Route53 responds to DNS queries

  Policies:
  - Simple: 
    route traffic to a single resource
    can specify multiple values in the same record -- client pick a random one
    when Alias enabled, specify only one AWS resource
    cannot associate with Health Checks
  - Weighted
    control the x% of the requests that go to each specific resource
      weight 0 to a record will stop the traffic to a specific resource
      all records weight 0 means all records same weight, will be returned equally.
    weights don't need to sum up to 100
    DNS records must have the same name and type         
    Can be associated with Health Checks
    Use cases: load balancing between regions
  - Latency-based
    redirect the traffic with the least latency
    based on the traffic between users and AWS regions    *
    can be associated with Health Check (has failover capability)
  - Multiple-value
  - Geolocation
  - Failover
  - 

  Route53 - Health Checks
  - HTTP Checks are only for public resourcs
  - Health Check => Automated DNS Failover
    monitor an endpoint (app, server, AWS resource)
    monitor other Gealth Checks (Calculated Health Check)
    monitor CloudWatch Alarms
  - Health Checks are integrated with CW (CloudWatch) metrics


    Health Checks - Monitor an Endpoint
    - 15 global health checkers for endopint health
      Failure Threshold - 3 (default)
      interval - 30 sec (standard), 10 sec (fast)
      supoorted protocol: HTTP, HTTPS, TCP
      if > 18% Health Checkers report the endpoint is healthy, Route 53 consider it Healthy.
      Ability to choose the location you want Route 53 to use
    - Health Checks pass only when the endpoint responds with 2xx or 3xx status codes
    - Health Checks can be setup to pass/fail based on the first 5120 Bytes of the response
    - Configure router/firewall to allow incoming requests from Route53 Health Checkers

    Calculated Health Checks
    - Combine the results of multiple Health Checks into a single Health Check
    - Support OR/And/Not logic
    - Can monitor up to 256 Child Health Checks.
      (Parent Health Checke -- (1:N) -- Child Health Check)
    - specify how many of the health checks need to pass to make the parent pass
    >> Use case:  perform maitenance to website without causing all health checks to fail.

    Health Checks - Private Hosted Zones
    - Route53 Health Checkers are outside the VPC
      Health Checkers cannot access private endpoint (in private VPC or on-premises resource)
    - Solution: Create CloudWatch Metric and associate CloudWatch Alarm,
                then create Health Check on the alarm


  Routing Policies - Failover (Active-Passive)
    Primary/Secondary EC2 Instances are associated with Route53 Health Check.
    When DNS requests comes, the response is based on Health Check + Primary/Secondary


  Routing Policies - Geolocation
    Routing is based on user location
    Specify location by Continent, Country or US State (If overlapping, most precise location selected)
    Should create 'Default' record (in case there is no match on location)
    Can be associated with Health Checks
    >> Use cases: website localization, restrict content distribution, load balancing

  Routing Policies - Geoproximity Routing
    Route traffic based on the geographic location of users and resources.
    Ability to shift more traffic to resources based on the defined bias
      bias: expand (1~99) - more traffic to the resource
            shrink (-1~-99) - less traffic to the resource
    Resources can be:
      AWS resource (specify region)
      Non-AWS resource (specify Latitude & Longitude)
    Must use Route 53 Traffic Flow to use this Feature. 
 
  Routing Policies - IP-based Routing
    based on client IP
    client CIDRs -> endpoint/location (user-IP-to-endpoint mapping)
    >> Use cases: route particular clients to specific endpoints

  Routing Policies - Multi-Value
    Route traffic to multiple resource
    Route 53 return multiple values/resourcs
    Can be associated with Health Check (return only values for healthy resources)
    Up to 8 healthy records are returned / query
    Multi-Value is NOT suitable for ELB            *
  

  Domain Registar vs DNS Service
    they are different services
      Domain Registar -- register your domain name, typically payiing annual charges.
                         Domain Registar usually provides DNS service as well
      DNS Service -- manage DNS records
    You can buy Domain Registar from SP A and use DNS service from SP B. 
      -- Configure the DNS servers in the Domain Registar
    So you can combine Amazon Route53 with 3rd party Domain Registar.
      - create a Public Hosted Zone in Route53.   (Has to be Public Host Zone since it needs to be accessible by 3rd party)
      - config NS (NameServer) records on 3rd party Domain Registar to use Route53 Name Servers


Elastic Beanstalk
  an orchestrated service for deploying and scaling web application and servcies.
  a managed service than automatically handles the
    capacity provisioning, LB, scaling, app health monitor, instance configuration, ...
  so that allow user to focus on the Web App itself.
  User still have full control over the configuration.
  Beanstalk is free, but user needs to pay the underlying instance/components

  Elastic Beanstalk - components
  - Application
    collection of ElasticBeanstalk componets: env, versions, configurations, ...
  - Application Version
    a iteration of app code
  - Environment
    collection of AWS resources running an application versions
    Tiers: Web Server (FE) Environment Tier & Worker (BE) Environment Tier
    Support multiple Env (Dev, prod, test, ...)
    Flow:   Create Application -- Upload Version -- Launch Env -- Manage Env
  
  Elastic Beanstalk - Supported Platforms
    Go, Java, .NET (Lnx/Win), Node.js, PHP, Python, Ruby, Packer Builder, 
    Single-Container Docker, Multi-Container Docker, Preconfigured Docker

  Elastic Beanstalk Deployment Modes
    - Single Instance (e.g for Dev)
    - HA & LB  (for Prod)



Instantiating Applications quickly
  EC2 Instances:
  - Golden AMI. 
    Ready made AMI with App&Env, lauch EC2 instances from the Golden AMI directly.
  - Bootstrap using User Data. 
    For Dynamic configuration, use User Data Scripts
  - Hybrid: Elasitc Beanstalk (mix Golden AMI and User Data)
  RDS Databases:
  - Restore from snapshot (Schemas & data ready)
  EBS Volumes:
  - Restore from snapshot.

Reduce the cost for the deployment with a known min# of EC2 instance --> Reserved Instances.




///////////////////////////////
S3
  Infinitly scaling storage


  S3 - Buckets
    Store 'objects' (files) in 'Buckets' (directories)
    Buckets have globally unique name (accross regions all accounts) -- the only globally unique thing in AWS
    Buckets are defined at the region level
    Name convention:  
      No uppercase, no undersore
      3-63 char long
      not an IP
      must start with lowercase letter or number
      must not start with the prefix 'xn--'
      must not end with the suffix '-s3alias'
    
    
  S3 - Objects
    - Objects (files) have a Key -- a Full Path
      Key is a long name that contains '/'
      Objects Key = Prefix + ObjectName.   eg. s3://my-bucket/my-folder/sub-folder/my-file.TXT
                                                              ---------------------'''''''''''
                                                                      prefix        objectName
    - Object value -- the content of the body
      max object size: 5TB
      if uploading more than 5GB, must use 'multi-part' uploading
    - Metadata: list of text key/value pairs, system or user Metadata
    - Tags:  unicode key/value pair, up to 10.  useful for security / Lifecycle
    - Version ID (if versioning is enabled)

  S3 - Security
    - User-based
      IAM Policy
    - Resource-based
      Bucket Policies: allows cross account
      Object ACL -- finer grain   (can be disabled)
      Bucket ACL -- less common   (can be disabled)
    - Encryption
      encrypt objects in S3 using encryption keys
    >> an IAM principle can access an S3 object if
       (The user IAM permission Allows it OR the resource policy Allows it) AND (there is no explicit DENY)

    S3 - Bucket Policies
      JSON based Policies 
      - Resources: buckets and object
      - Effect: Allow / deny
      - Actions: set of API to Allow or Deny
      - Principal: the account or use to apply the policy to
      e.g.
          {
            "Version":"2012-10-17",
            "Statement":[
              {
                "Sid":"AddPerm",
                "Effect":"Allow",
                "Principal": "*",
                "Action":["s3:GetObject"],
                "Resource":["arn:aws:s3:::examplebucket/*"]
              }
            ]
          }
      Use S3 bucket policy to 
      - Grant public access to the bucket
      - Grant access to another account (Cross Account)
      - Force object to be encrypted at uploading

      Bucket settings for Block Public Access: to prevent company data leaks    
        (a default overall setting, disable this and apply other access policies)
        Can be set at account level

    Use cases
    >> Public Access - Use Bucket Policy
    >> User Access to S3 - IAM permissions
    >> EC2 instance access - User IAM Roles
    >> Cross-Account Access - User Bucket Policy
  

  S3 - Static Website Hosting
    S3 can host static websites accessible on Internet

    the website URL will be:
      http://<bucket-name>.s3-website-<aws-region>.amazonaws.com    or
      http://<bucket-name>.s3-website.<aws-region>.amazonaws.com

    >> if get 403 Forbidden error, typically that's access issue and you need to set the bucket policy to allow public read.


  S3 - versioning
    Version files in S3
    enabled at bucket level
    same key overwrite will change the version: 1,2,3...
    protect against unintended deleted
    can roll back to previous version
    Any file that is not versioned prior enabling versioning will havbe version 'null'
    suspending versioning does not delete the previous versions
    delete operation is 'adding a delete marker' if versioning enbled.


  S3 - Replication (CRR & SRR)  
    Must enable versioning in source and dest buckets
    Must hav eproper IAM permission to S3
    CRR (Cross-Region Replication)
    SRR (Same-Region Replication)
    Buckets can be in different AWS accounts
    Copying is asynchronous

    S3 Back Replication:
      After enabling Replication, only NEW objects are replicated.
      to replicate existing objects, use S3 Batch Replication

    delete marker can be configured to be replicated or Not be replicated.
    when replicating, delete operation is not replicated, only delete marker is. 
      so when deleting a specific version (which is a permenant deletion), this operation will NOT be replicated.    *

    Use Case
    >> CRR - Compliance, low latency access, replication across accounts
    >> SRR - log aggregation, live replication b/w production and test accounts


  S3 Storage Classes
    Standard - General Purpose
    Standard-Infrequent Access (IA)
    One Zone-Infrequent Access
    Glacier Instant Retrieval
    Glacier Flexible Retrieval
    Glacier Deep Archive
    Intelligent Tiering

    Can move between classes manually using S3 Lifecycle configurations

  
  S3 Durability & Availability
    Durability:
      High Durability (99.999999999% 9s) of objects across multi AZ 
      same for all storage classes
    Availability:
      Measure how readily available a service is
      Varies depend on storage class


  S3 Standard
    99.99% availabiity
    Used for frequently accessed data
    Low latency and high throughput
    Sustain 2 concurrent facility failures
    >> Use cases:  big data analystics, mobile/gaming app, content distribution

  S3 Standard - IA  (infrequent Access)
    99.9% Availability
    lower cost than Standard but has retrieve cost
    >> Use cases: Disaster Recovery, backups

  S3 One Zone-IA  (infrequent Access)
    99.5% Availability
    High Durability in a single AZ only 99.999999999%
    >> Use case: store secondary backup copies of on-premise data, or data that can be recreated.

  S3 Glacier
    Low cost for archiving / backup
    Pricing:  storage + object retrieval cost

    Glacier Instant Retrieval
      ms retrieval, 
      mini storage duration: 90 days
      >> Use case: data accessed once a quarter  

    Glacier Flexible Retrieval
      Expedited (1~5 min), Standard (3~5 hours), Bulk (5~12 hours) - free
      mini storage duration: 90 days
      >> Use cases:

    Glacier Deep Archive
      Standard (12 hours), Bulk (48 hours)
      Mini storage duration 180 days
      >> Use cases:

  S3 Intelligent-Tiering
    Small monthly monitoring and auto-tiering fee
    Move objects automatically between Access Tiers based on usage
    No retrieval charges

    Freqent Access tier (auto): default
    Infreqent Access tier (auto): object not accessed for 30 days
    Archive Instant Access tier (auto):  object not accessed for 90 days
    Archive Access tier (optional):  configurable from 90~700+ days
    Deep Archive Access tier (optional): confgurable from 180-700+ days


  S3 - Lifecycle Rules 
    Transition Action:  move objects to <Class XX> <days> after creasion
    Expiration Action:  config object to expire (delete) after some time
      - Access log can be deleted after 365 days
      - Can be used to delete old version of file (if versioning is enabled)
      - Can be used to delete incomplete multi-part uploads
    Rules can be created for a certain prefix
    Rules can be created for a certain objects Tags
    >> Use cases:  
       S3 objects need to be recoverable immediately for 30 days (rarely happen); 
       then after 365 days, deleted objects should be recoverable within 48 hours.
       Solution:
         Enable S3 versioning -> deletion recoverable
         Transition rule 1: non-current versions to Standard IA,
         Transition rule 2: (after rule 1) non-concurrent versions to Glacier Deep archieve
    >> Use case: cleanup the old unfinished parts from multi-part upload
       Solution: S3 Lifecycle Policy to automate the old/unfinished parts deletion
       

  S3 Analytics - Storage Class analysis  
    Report updated daily, with the objects StorageClass, date, age, etc
    Help to decide when to transition objects so the right storage class
    Recomendations for Standard and Standard IA. (Not work for One-zone or Glacier)
    24-48 hours to start seeing the data analysis

S3 - Requester Pays.
  'Requester Pays buckets'
  The requester must be authenticated in AWS (not anonymous)


S3 Event Notification
  Events: 
    e.g. S3:ObjectCreated, S3:ObjectRemoved, S3:ObjectREstore, S3:Repication
  Object Name filtering possible
  Can create as many S3 events as desired
  S3 events can be delivered to SNS, SQS, Lambda Function, etc
  S3 event notification typically deliver events in seconds, but can take a min or longer sometimes.
  for the notification: define  Event, object Filter, destinations (with Access Policy)

  >> Use case: generate thumbnails for images uploaded to S3.


  S3 Event Notification - IAM permissions
    Defie Resource Access Policy on the target resources (SNS, SQS, Lambda Funciton, etc)
    e.g 
      SNS Resource (Access) Policy
      SQS Resource (Access) Policy
      Lambda Resource Policy

  S3 Event Notification - Amazon EventBridge
    EventBridge can be turned on/off
      when enabled, all S3 Event Notification are sent to Amazon EventBridge
    Features:
      - Advanced filtering options with JSON rules (object name, size, metadata, ...)
      - Multiple Destinations
      - EventBridge Capabilities - Archive, Replay Events, Reliable delivery, ...

  S3 - Baseline Performance
    S3 auto scales to high reqeust, latency 100-200ms
    3500 PUT/COOPY/POST/DELETE or 5500 GET/HEAD Requests per sec per prefix in a bucket
    No limits to the number of prefixes in a bucket
    Spreading reads across N prefixes evently, you can achieve N * 5500 reqeust/s for GET/HEAD.

    Multi-part upload:
      Parallelize uploads
      Recommended for file > 100MB, Must use for files > 5GB
      >> Speed up uploading
    
    Transfer Acceleration:
      Increase transfer spped by transferring file to AWS Edge location, which will forward data to S3 bucket in the target region.
      Files ---(public)---> Edge location in target Region ---(private AWS)---> Bucket in target Region
      Compatible with Multi-part upload
      >> Speed up uploading

    S3 Byte-Range Fetches
      Parallelize GETs by requesting byte ranges
      better resilience in case of failures
      >> speed up downloading
         or used in retrieving only partial data (e.g. File header)

  S3 Select & Glacier Select
    Retrive less data using SQL by performing Server-Side Filtering
    Can filter by rows & columns (simple SQL statements)
    less data -> less network transfer / CPU / Cost


  S3 Batch Operations
    Perform bulk operation on S3 objets with a single request.
    A job consists of a list of objets, the action to perform and optional parameters
    -- Use S3 Inventory to get object list and    *
       Use S3 Select to filter object
       S3 Inventory --(Objects list)--> S3 Select --(filtered objects)--> S3 Batch Operations --(processed Objects)
    benefits:
      S3 Batch Operation manages retries, tracks progress, send completion notifications, generate reports.

    e.g.
      Modify object metadata/properties
      Copy objects
      Encrypt un-encrypted objects
      Modify ACLs, tags, ..
      Restore object form Glacier
      Invoke Lambda function for custom action on each object


  S3 - Storage Lens
    Analyze, optimize storage
    Aggregate data for Organization/Accounts/Regions/Buckets/prefixes
    Default/Customized dashboard
    can be configured to export metrics daily to an S3 bucket (CSV, Parquet)

    Default Dashboard
      Visualized summarized insights
      Shows multi-Region & multi-accounts data
      Cannot be deleted but can be disabled

    metrics
      Summary metrics
        e.g. StorageBytes, Object counts
        >> Use case:  identify fastset-growing or not-used buckets/prefixes

      Cost-Optimizaiton metrics
        NonCurrentVersionStorageBytess, etc
        >> Use case: identify buckets incomplete multi-part uploaded loder than 7 days
                     idnetify the objects could transition to lower-cost storage class

      Data-Proetction metrics 
        e.g. VersioningEnabledBucketCount
        >> Use case: identify buckets that are not following data-protection best practices

      Access-management metrics
        insight for S3 Object Ownership
        >> Use case: which object Owership setting the buckets use

      Event metrics
        >> 

      Performance metrics

      Activity metrics

      Status Code metrics

      Free vs Paid metrics
      - Free metrics
        contains 38 usage metrics
        available for query for 14 days
      - Advanced metrics
        Advance metrics: Activity, advanced cost optimizatin, advanced data protection, status code
        Cloudwatch publishing
        Prefix aggregation (Collect metrics at the prefix level)
        Data available for query for 15 months

  S3 - Object Encryption
    S3 Object Encryption methods:
    SSE: (Server-Side Encryption)
    - SSE-S3:  (Server-Side Encryption with S3 Manged Keys) 
      Encrypt S3 Objects using keys handled, managed and owned by AWS
      Enabled by Default
    - SSE-KMS: (Server-sdie Encryption with KMS Keys stored in AWS KMS)
      Leverage AWS Key Management SErvice to manage encryption keys
    - DSSE-KMS: (Dual-layer Server-Side Encryptin with KMS)
    - SSE-C:   (Server-side Encryption with Customer-Provided Keys)
      customers manage their own encryption keys.
    CSE (Client-Side Encryption)

    S3 Encryption - SSE
    - Keys handled, managed and owned by AWS
    - Server-Side encryption
    - AES-256
    - Must set header 'x-amz-server-side-encryption':'AES256'
    - Enabled by default for new buckets & new objects

    S3 Encryption - SSE-KMS
    - Keys handled and managed by KMS
    - KMS advantages:  user control + audit key usage using CloudTrail
    - Server-Side encryption
    - Must set header 'x-amz-server-side-encryption':'aws:kms'
    Limitation:
      impacted by KMS limit
      when uplad, call GenerateDataKey KMS API
      Count towards the KMS quota per second (5500,10000,30000 req/s based on region)
      Can request a quota increase using the Service Quota Console

    S3 Encryption - SSE-C
    - Keys handled and managed by Customer outside AWS
    - AWS S3 does NOT store the encryption key
    - Server-Side encryption
      AWS S3 use the client provided key (in every HTTPS header) to encrypt the data
    - HTTPS must be used
    - Encryption Key must provided in HTTP headers, for every HTTP request mode
    - HTTP header set 'x-amz-server-side-encryption-customer-algorithm':'true'

    S3 Encryption - CSE
    - Client fully manage the keys and encryptioned
    - Client encrypt data before sending to S3
    - Client decrypt data when retrieving from S3
    - Use client side lib e.g. AWS S3 Client-Side Encryption Library

  S3 - Encryption in transit (SSL/TLS)
    AWS S3 exposes 2 endpoints:  
      HTTP Endpoint
      HTTPS Endpoint 
    HTTPS is mandatory for SSE-C
    Most client use HTTPS by default

  Force Encryption in Transit:  
    set S3 bucket policy to deny HTTP
    connection option of 'aws:SecureTransport' == True|False


  S3 - Default Encryption vs Bucket Policies
    Default Encryption: SSE-S3
    Optionally, force encryption using bucket policy
      -> Refuse API call of PUT (S3 Object) without encryption header
      -> Deny condition: 'x-amz-server-side-encryption-customer-algorithm':'true'
    >> Bucket Policies and evaluated before 'Default Encryption' setting


  CORES (Cross-Origin Resource Sharing)
    -- Web browser based mechanism to allow requests to other origins while visiting the main origin
    "Acces-Control-Allow-Origin: <origion>"
    "Access-Control-Allow-Method: <Methods>"
      Origin = schema (protocol) + host (domain) + port
         eg:    https               example.com     443  

    Alow the 'other origin' requests using CORES Headers (e.g. Access-Control-Allow-Origin)

            Origin Web Server                          Other Web Server
    (origin.com)                                          (other.combine)
    HTTPS Req ---->
                    --------- Preflight Request -------->
                    OPTIONS/
                      Host: other.com
                      Origin: origin.com
                    
                    <-------- Prefilight Response ---------
                     Access-Control-Allow-Origin: https://origin.com
                     Access-Control-Allow-Method: GET, PUT, DELETE

                    <---(CORS Header received by the origin)--> 
                    GET / 
                      Host: other.com
                      Origin: origin.com
    
    S3 - CORS
      Needs to enable the correct CORS headers for client requests to S3 bucket.      *
      allow specific origin or * (all origin)

      S3 CORS setting:
      e.g
        [
          {
            "AllowedHeaders"  [ "Authorization" ],
            "AllowedMethods": [ "GET" ],
            "AllowedOrigins": [ "<url of origin (without slash at the end)>" ],
            "ExposeHeaders":  [],
            "MaxAgeSeconds": 3000
          }
        ]


  S3 - MFA Delete
    MFA (Multi-Factor Authentication)
      force user to generate a code on device beofre doing important operations on S3
    To use MFA delete, Versioning must be enabled on the bucket
    Only the bucket owner (root account) can enable/disable MFA Delete

    MFA will be required when, e.g.
    - permanent delete on an object version
    - Suspend Versioning on the bucket


  S3 Access logs
    log can be enabled on S3 buckets
    The log date can be analyzed using data analysis tool
    the target logging bucket must be in the same AWS Region
    Do NOT set the logging bucket to be the monitoring bucket! -- will create logging loop

  S3 - Pre-Signed URLs
    Users given a pre-signed URL inherit the permissions of the user that generated the URL for GET/PUT
    Generate pre-signed URLs using S3 console, AWS CLI or SDK
    URL Expiration
      - S3 console - 1~720 min (12 hours)
      - AWS CLI - configure expiratin with --expires-in parameter in sec (default 3600 sec, max 604800sec)

    >> Use cases:
       allow only logged-in users to download a premium video from you S3 bucket
       allow an ever-changing list of users to download files by generating URLs dynamically
       allow temporarily a user to upload a file to a precise location in your S3 bucket


  S3 Glacier Vault Lock
    adopt WORM (Write Once Read Many) model
    Create a Vault Lock Policy
    Lock the policy for future edits (no longer be changed or deleted)
    Helpful for compliance and data retention

  S3 Object Lock (versioning must be enabled)
    adopt WORM model
    Block an object deletion for a specificed amount of time
    modes:
    - Retention mode - Compliance
      Object version can't be overwritten or deleted by any user, incl. root
      Object retentin modes can't be changed, and retention period can't be shortened
    - Retention mode - Governance:
      Most users can't overwrite or delete an object vertion or alter its lock settings
      Some users have special permissions to change the retention or delete the object
    Retention Period: protect the object for a fixed period, it can be extended
    Legal Hold:
      protect the object indefinitely, independent from retention period
      can be freely placed and removed using the s3:PutObjectLegalHold IAM permission


  S3 - Access Points
    (A solution to group user/access, a layer on top of bucket (bucket policy) that can organize the access control)
    
    Access Policy simplify security management for S3 buckets
    Each access Point has:
      - its own DNS name (internet origin or VPN origin)
      - an access point policy (similar to bucket policy) - manage security at scale

    S3 - Access Points - VPC Origin
      access point only accessible from within the VPC
      must create a VPC Endpoint (Gateway or Interface Endpoint)
      the VPC Endpoint Policy must allow access to the target bucket and Access Point

      --------------------------
      |          VPC           |    Access Point
      | EC2 --- VPC Endpoint --+-->  VPC Origin  ----> S3 Bucket
      |      (Endpoint policy) |   (Access Point)   (Bucket Policy)
      |              |         |      Policy   
      ---------------+----------
                     |
          Policy to allow:
          - S3 bucket access
          - Access Endpoint access


  S3 Object Lambda
    Use AWS Lambda Functions to change the obejct before it is retrieved by the caller application
    Only one s3 bucket is needed, on top of which we can create S3 Access Point and S3 Object Lambda Access Point(s)

         [S3 Bucket]
              |
      [S3 Access Point]
              |
              |---- Lambda Function 1 ---- [S3 Object Lambda Access Pint] ---- [APP 1]
              |
              |---- Lambda Function 2 ---- [S3 Object Lambda Access Pint] ---- [APP 2]

    >> Use case:
       convert across data formats, like XML to JSON
       resizing and water-mark images on the fly using the caller-specific details




///////////////////////////////////
AWS CloudFront

  Content Delivery Netowrk (CDN)

  Improve read performance by caching the content at the Edge, improve user experience.
  216 Point of persence globally (edge location)
  DDoS protection (because worldwide), integration with Shield, AWS WAF

  Origins:
  - S3 Bucket
    Enhanced security with CloudFront Origin Access Control (OAC)
      OAC is replacing OAI (Origin Access Identity)\
    CloudFront can be used as an ingress (for files uploading to S3)
  - Custom Origin (HTTP)
    ALB (Application Load Balancer)
    EC2 instance
    S3 website
    Any HTTP backend


    ------------ CloudFront -------------
    |                                   |
                            _____ Edge-1:    clients (public)
                           /       ...         ...
      [Origin (S3 bucket)] ------ Edge-x:
               |           \_____ Edge-n:    clients (public)
               |
      {Origin Access Control}
        (S3 bucket policy)

    
  CloudFront - ALB or EC2 as origin

    EC2 origin
      The EC2 instance MUST be Public (for CloudFront to access)
    ALB origin
      The ALB MUST be public, The EC2 instance can be private (behind ALB)


  CloudFront Geo Restriction          *
    Restrict who can access the CloudFront distribution.
      AllowList: to allow users from the listed countries
      BlockList: to block users from the listed countries
    The 'country' is determind using a 3rd party Geo-IP database
    >> Use case: Copyright Laws to control access to the content      


  CloudFront - Pricing
    The cost of data out per edge location varies
    Can recude the edge locations for cost reduction

    Price Classes:
      Price Class All: all regions, best performance
      Price Class 200: most regions, excluding the most expensive regions
      Price Calss 100: only the least expensive regions


  AWS Global Accelerator
    Leverage AWX internal global network to route traffic to application.
    -- User traffic been directed (via Anycast IP/route) to the nearest Edge Location,
       then from the Edge location to then ALB then the Application throught AWX global network
    work with Elastic IP, EC2 instances, ALB, NLB, public or private.
    consistent performance
      - intelligent routing to lowest latency and fast failover
      - No issue with client side cache (IP doesn't change)
      - use internal AWS network
    Health Checks
      - Global Accelerator performs health check to the applications
      - helps make the application global (failover < 1 min ofor unhealthy)
      - greate for disaster recovery
    Security
      - only 2 external IP need to be whitelisted
      - DDoS protection (AWS Shield)


    Global Accelerator vs CloudFront
      Same part:
        - both use AWS global netowrk and its edge locations
        - both integrate with AWS shield for DDoS
      differece:
        CloudFront:
        - content cache, (content served from the edge)
        - improve performance for both cacheable content and Dynamic content 
          e.g API acceleration (cache API result) and site delivery
        Global Accelerator
        - No caching (proxy packets at the edge to application in AWS)
        - improves performance for applications over TCP/UDP
        - Good for non-HTTP use cases, e.g.  VOIP, gaming, IOT
        - Good for HTTP use cases that require static IP addresses
        - Good for HTTP use cases that require deterministic, fast regional failover



/////////////////////////////
AWS SNOW

  Highly-secure, portable devices to collect and process data at the Edge and migrate data into/out of AWS.

  AWS Snow Family: offline devices to perform data migrations, 
  if it takes more than a week to transfer over the network, use Snowball devices.

  Data migration: Snowcone, Snowball Edge, Snowmobile
    For large data migratoin
  Edge computing: Snowcone, Snowball Edge
    For the scenario where there is lack of internet access or computing power
    Can run EC2 Instances & AWS Lambda functions (using AWS IoT greengrass)


    Snowcone:   up to 24TB, online or offline (pre-installed DataSync agent)
    Snowball:   up to PB,  offline
    Snowmobile: up to EB,  offline


    Snowball Edge (for data transfer)
      Physical data transprt solution.    (large)
      Pay per data transfer job
      
      Storage:
      - Snowball Edge Storage Optimized: 
        40 vCPUs, 80G mem 80T HDD 
        104 vCPUs, 416G mem, 210T NVMe for block volume and S3 compatible object storage
      - Snowball Edge Compute Optimized: 
        104 vCPUs, 416G mem
        42T HDD or 28T NVMe for block volumn and S3 compatible object storage
      Storage Clustering available (up to 16 nodes)
      >> Use case:
        large data cloud migrations
        DC decommision
        disaster recovery

    Snowcone & Snowcone SSD
      Device for edge computing, storage and data transfer.      (Small)
      Small, portable computing, anywhere, rugged & secure, withstands harsh environments

      2 CPUs, 4G mem, USB-C power or optinal battery
      - Snowcone: 8T HDD
      - Snowcone SSD: 14T SSD
      Must provide your own battery/Cables
      Can sent back to AWS offline, or connect to internet and use AWS DataSync to send data

      >> Use cases:
        space contrained environment where Snowball does not fit.

    Snowmobile
      A Truck
      Transfer exabytes of data (1EB = 1000PB = 1000000TB)

  Process:
    Request snowball device
    Install Snowball client / AWS OpsHub on your servers
    Connect snowball and copy files
    Shipback the device


  AWS OpsHub
    a software with UI for Snow Family


  Snowball into Glacier
    Snowball can NOT import to Glacier directly
    It can go to S3 first, then use S3 Lifecycle Policy to transfer into Glacier




//////////////////////////////////////
Amazon FSx

    Lauch 3rd party high-performance file system on AWS
    Fully managed service
      FSx for Lustre, FSx for NetApp ONTAP, FSx for Windows File Server, FSx for OpenZFS
    

  FSx for Windows
    Fully managed Windows FS share drive
    Support SMB & NTFS
    Microsoft Active Directory integration, ACL, user quotas
    Can be mounted on Linux EC2 instances
    Support Microsoft DFS (Distributed File System) Namespaces -- group files across multiple FS
    Can be accessed from on-premises infrastructure (VPN or Direct Connect)
    Can be configured to be Multi-AZ (HA)
    Data is backed-up daily to S3
    Scale up to 10s of GB/s,  millions of IOPS,  100s of PB of data

  FSx for Lustre
    Lustre -- (Linux-Cluster)
    Lustre is a parallel distrubuted FS, for large-scale computing
    for Machine Learning, HPC (High Performance Computing)
        Video Processing, Financial Modeling, Electronic Design automation
    Seamless integration with S3
      - Can read S3 as file system (through FSx)
      - Can write the output of the computations back to S3 (through FSx)
    Can be used from on-premises servers (VPN or Direct Connect)
    Scales up to 100s GB/s, millions of IOPS, sub-ms latencies

  FSx File System Deployment Options
    Scratch File System
      - Temp storage
      - Data is not replicated
      - High burst (6x Faster, 200MBps per TB)
      - Usage:  short-tem processing, optimize costs
    Persistent File System
      - Long-term storage
      - Data is replicated within same AZ
      - Replace failed files within minutes
      - Usage: long-term processing, sensitive data

  FSx for NetApp ONTAP
    Manged
    FS compatible with NFS, SMB, iSCSI
    Move workloads running on ONTAP or NAS to AWS
    Work with: Linux, Windows, MacOS, VMware Cloud on AWS, Amazon Workspaces & AppStream 2.0, Amazon EC2, ECS and EKS
    Storage shrinks or grows automatically
    Snapshots, replication, low-cost, compression and data de-duplication
    Point-in-time instantaneous cloning (helpful for testing new workloads)

  FSx for OpenZFS
    Compatible with NFS (v3,v4,v4.1,v4.2)
    Move workloads running on ZFS to AWS
    Work with: Linux, Windows, MacOS, VMware Cloud on AWS, Amazon Workspaces & AppStream 2.0, Amazon EC2, ECS and EKS
    Up to 1,000,000 IOPS with < 0.5ms latency
    Snapshots, compression and low-cost
    Point-in-time instantaneous cloning (helpful for testing new workloads)


  Storage Gateway

    Hybrid Cloud for Storage

      AWS Storage Cloud Native Options
        Block:  EBS, EC2 Instance Store.
        File:   EFS, FSx
        Object: S3, Glacier

    Bridge Between on-premises and cloud
      Type of Storage Gateway
        - S3 File Gateway
        - FSx File Gateway
        - Volumn Gateway
        - Tape Gatewy
    >> Use case
       disaster recovery
       backup & restore
       tiered storage
       on-premises cache & low-latency file access

    S3 File Gateway
      
                  on-premises                     |       AWS Cloud
      App-server (NFS/SMB) ---[S3 File Gatewy] ---(HTTPS)--- S3 -- Glacier

      Config S3 bucket to be accessible using NFS and SMB
      Most recently used data is cached in the file Gateway
      support different types of S3 (Standard, standard IA, one-zone IA, Intelligent Tiering)
      Transition to S3 Glacier using a lifecycle policy
      Bucket access using IAM roles for each FIle Gateway
      SMB has integration with AD (Active Directory) for user authentication

    FSx File Gateway
      Native access for FSx for Windows Server
      Local Cache for frequent accessed data
      Windows compatibility (SMB, AD, NTFS)
      Useful for group file shares and home directory

    Volume Gateway
      Block storage using iSCSI backed by S3
      Backed by EBS snapshot which can help restore on-premises volumes
      Cached volumes: low latency access to most recent data
      Stored volume: entire dataset is on premise, scheduled backups to S3

                   On-premises                        |              AWS
        App Server ---iSCSI ---[Volume Gateway] ---(HTTPS)--- S3  --- EBS snapshot

    Tage Gateway
      backup using physical tapes
      VTL (Virtual Tape Library) backed by S3 / Glacier
      Backup data using existing tape-based processes (and iSCSI interafce)
      Works with leading backup software vendors

                     On-premises                       |              AWS
        Backup Server ---iSCSI ---[Tape Gateway] ---(HTTPS)--- S3 Virtual Tape Store --- Glacier

    Storage Gateway - Hardware Appliance
      Using Storage Gateway means you need on-premises virtualization
      if no hardware, you can use AWS Storage Gateway Hardware Appliance
      work with File Gateway, Volumn Gateway, Tape Gateway
      has the requird CPU, memory, network, SSD cache ...


    Storage Gateway Overview
      Gateway Deployment Options:
        - VM (VMware, KVM, Hyper-V)
        - Hardware Appliance

      
  AWS Transer Family
    A fully managed service to transfer data in/out AWS S3 or EFS via FTP.
    Suport:  FTP, FTPS (FTP over SSL), SFTP
    Managed infrastructure:  Scalable, Reliable, HA (Multi-AZ)
    Pay per provisioned endpoint per hour + data transfers in GB
    Store and manage users' credential within the service
    Intergrate with existing authentication system (Microsoft AD, LDAP, Okta, Amzaon Cognito, custom)
    >> Usage case:  
       Sharing files,  Public datasets,  CRM,  ERP

    User (FTP client) --- (Router53) --- [AWS Transfer ] ---- IAM Role --- AWS S3 | EFS
   

  AWS DataSync
    Move large amount of data to / from places:
      On-premises/other cloud to AWS (NFS,SMB, HDFS, S3) -- needs agent
      AWS to AWS (no need agent)
    Can synchronize to :  S3/Glacier,  EFS,  FSx
    Replication tasks can be scheduled hourly/daily/weekly
    File permissions and metadata are preserved (NFS POSIX SMB)         **
    One agent task can use 10 Gbp, can setup bandwidth limit
      
    
      NFS/SMB to AWS: 
      NFS/SMB Server  ---(NFS/SMB)--- [AWS DataSync Agent] ----(TLS)---- [AWS DataSync] ---- AWS Storage Resource
                                         [AWS Snowcone]  | --------------|                   (S3, EFS, FSx,)
                                      (agent preinstalled)

      AWS DataSync
      S3, EFS, FSx -----[AWS DataSync]----- S3, EFS, FSx
                        (copy metadata)


  AWS Transfer Family vs DataSync
    Transfer Family:
      Purpose: Transfer files into and out of Amazon S3 using SFTP, FTPS, and FTP protocols.
      Support protocol:  FTP, FTPS, FSTP
      Use case: replacing traditional file trnsfer workflows with cloudbased solution
    DataSync:
      Purpose: Move large amounts of data online between on-premises storage and AWS services.
      Supported protocol: DataSync's own optimized protocol
      Use case: Data migration, data processing, and regular data backup to AWS
                Transferring data between NFS server, SMB file shares and AWS storage servcies.



  Storage Comparison
    S3:               Object storage
    S3 Glacier:       Object Archival
    EBS Volume:       Network Storage for one EC2 instance at a time.  (Multi-attach to io1/io2 volume)
    Instance Store:   Physical storage for EC2 instance (High IOPS)
    EFS:              Network File System for Linux instance, POSIX filesystem
    FSx for windows:  Network File System for Windows Servers
    FSx for Lustre:   High Performance Computing Linux File System
    FSx for NetApp ONTAP: High OS Compatibility
    FSx for OpenZFS:  Managed ZFS file system
    Storage Gateway:  S3/FSx File Gateway, Volume Gateway (cache & store), Tape Gateway
    Transfer Family:  FTP, FTPS, SFTP interface on top of S3/EFS
    DataSync:         Scheduled data sync from on-premises->AWS or AWS->AWS
    Snowcone/Snowball/Snowmobile: to move large amount of data to cloud, physically.  (Snowcore with datasync agent)
    Database:         for specific workloads, usually with indexing and querying





/////////////////////////////////
AWS Messaging Services  (DeCoupling applications: SQS, SNS, Kinesis, Active MQ)

  Application communication pattern:  Synchronous / Asynchronous
  
  AWS Servcies:
    SQS:     Queue modlel
    SNS:     pub/sub model
    Kinesis: real-time streaming model
  There servcies can scale independently from applications


  Amazon SQS
    Queue
    Oldest offering (> 10 years)
    Fully managed servcie, used to decouple applications

    Producer(s) ---(message)---> SQS Queue ---(poll message)---> Consumer

    Attributes:
      - Unlimited throughput, Unlimited number of messages in Queue
      - Default retention: 4 days, max 14 days
      - Low latency (<10ms on publish & receive)
      - Limitation of 256KB per message sent
    Can have duplicate messages (at least one delivery, occasionally -- a message may be delivered twice)
    Can have out of order message (best effort ordering)

    
    Producing messages:
      Produce to SQS using the SDK (SendMEssage API)
      The message is persisted in SQS until a consumer deletes it.
      Retention: <=14 days. default 4
      Unimited throughput.

    Consuming messages:
      Consumer running on EC2 instances, servcers, or AWS Lambda
      Poll SQS for messages (receive up to 10 messages at a time)
      Multiple consumers can receive / process messages in parallel
      at least once delivery
      best-effort ordering
      Consumer delete messages after processing
      Can scale consumers horizontally to improve throughput of processing

    SQS with Auto Scaling Group (ASG)
      Scale based on CloudWatch Metric -- Queue Length (ApproximateNumberOfMessages)


    SQS to decouple between application Tiers
    requests --> [Front-end web Apps] ---(SendMessage)---> SQS Queue ---(ReceiveMessage)---> [Backend Apps]
                        x M                                                                      x N (Auto Scaling)

    SQS - Sequrity
      Encryption:
        In-flight encryption using HTTPS API
        At-rest encryption using KMS keys
        Client-side encryption if the client wants to perform encryption/decryption itself

    SQS - Message Visibility Timeout
      After a message is polled by a consumer, it becomes invisible to other consumers
      by default 30 sec
      means the message has 30 sec to be processed
      after the message visibility timeout is over, the message is 'visible' in SQS.
      if a message is not processed winthin the visibility timeout, it will be processed twice.
        -- consumer can call 'ChangeMessageVisibility' API to get more time for processing
      if visibility timeout is high (hours) and the consumer crashes, it will take long time to re-processing
      if visibility timeout is too low, we may get duplicates

    SQS - Long Polling
      Long Poll: when a consumer requests messages from a queue,
                 it can optionally 'wait' for message to arrive if there are none in the queue
      decreases the number of API calls to SQS -> increase the efficiency and latency of the app.
      he wait time 1~20 sec
      Long polling is preferable to short polling
      Long polling can be enabled at the queue level, or at the API level using WaitTimeSeconds

    SQS - FIFO Queue 
     Limited throughput 300 msg/s without batching, 3000 msg/s with batching
     Exactly-once send capability (by removing duplicates)
     Consumer recevie order is guarenteed

     a FIFO queue is named with the end of '.fifo'
     'content-based duplication' -- 

    SQS with ASG
      use SQS with Auto Scaling Group (for consyners)

      if the load is too big, some transactions may be lost. to solve it, use SQS as a buffer.

      Request(s) --- Enqueue ---(SendMessage)--- [SQS] ---(ReceiveMessge)--- Dequeue
                      (ASG)                                                   (ASG)



  Amazon SNS
    Simple Notification Service   (Pub/Sub)
    -- sned one message to many receivers

      Event producer ---- [SNS topic] ----> subscriber(s) x N (max 12.5M)

    The event producer only send event to one SNS topic
    The event receiver (subscribers) listen to the SNS topic
    each subscriber to the topic will get all the messages (new feature to filter messages)
    Up to 12.5 million subscriptions per topic
    Topic limit: 100000  (0.1M)
    Subscriber types:  email, SMS, HTTP/HTTPS endpoints, SQS, Lambda, Kinesis Data Firehose

    Many AWS servcies can send messages directly to SNS for notification

    SNS - How to publish
      Topic Publish (using SDK)
        - create topic
        - create subscription(s)
        - publish to topic
      Direct Publish (for mobile apps SDK)
        - create a platform application
        - create a platform endpoint
        - publish to the platform endpoint
        - works with Google GCM, Apple APNS, Amazon ADM

    SNS - Security
      Encryption:
        - in-flight encryption using HTTPS API
        - at-rest encryption using KMS keys
        - client-side encryption 
      Access Controls:  
        IAM policies to regulate access to the SNS API
      SNS Access Policies 
        - Useful for cross-account access to SNS topics
        - Useful for allowing other services (S3, ..) to write to an SNS topic

    SNS - FIFO Topic
      features:
        - Ordering by Message Group ID (all messages in the same group are ordered)
        - Decuplication using Deduplication ID or Content Based Dedeplication
      Can have SQS Standard and FIFO queue as subscribers
      Limited throughput (same as SQS FIFO)


  SNS + SQS: Fan Out Pattern
    Push once in SNS, receive in all SQS queues that are subscribers
    Fully decoupled, no data loss
    SQS allows data persistence, delayed processing and retries
    Ability to add more SQS subscribers over time
    Make sure your SQS queue access policy allows for SNS to write
    Cross-region Delivery: works with SQS Queues in other regions


    event(s) ---> SNS ---(xN) subscriber --- SQS --- service-1
                                 ...
                              subscriber --- SQS --- service-n

    >> Use case:
       Application: S3 Events to multiple queues
         For the same combination of: Event type (e.g. object create) and prefix (e.g. images/) you can only have One S3 Event rule
         Id you want to send the same S3 evnet to multiple recievers, use SNS -> SQS Fan Out pattern
                                          - SNS
                                         /
           S3 --- (event) ---> SNS Topic -- SNS
                                         \
                                          - SNS

       Application: SNS to S3 through KDF (Kinesis Data Firehose)
         SNS can send to Kinesis

         Service --- SNS Topic ---> Kinesis Data Firehose ---> S3
                                                          \--> (Any KDF supported Destination)


    SNS FIFO + SQS FIFO: Fan Out
      In case you need:  Fan Out + Ordering + Deduplication

        Servcie --- SNS FIFO Topic --- SNS FIFO Queue ---> consumer
                                    \- SNS FIFO Queue ---> consumer


    SNS - Message Filtering
      JSON Policy used to filter messages snet to SNS topic's subscribers
      if a subscriber doesn't have a filter policy, it receives every message










---
The Study Note is from the udemy tutorial "Ultimate AWS Certified Solutions Architect Associate SAA-C03"


AWS Region encompasses multiple Availability Zones, and each AZ is designed to be independent and resilient



Region:
   A physical location around the world where AWS clusters data centers

AZ (Availability Zone)
  A set of data centers located within an AWS Region





/////////////////////
[IAM]


Group only contain users, no sub-Group
IAM is a global service, not regional
AMIs are built for a specific AWS Region, they're unique for each AWS Region. 


# Security Group:

Security Group can be attached to multiple EC2 instances within the same AWS Region/VPC

Security Group vs ACL:
- Security group is the firewall of EC2 Instances.
- Network ACL is the firewall of the VPC Subnets

Security Group	                            Network Access Control List
--------------------------------------------------------------------------
operate at instance level.	                operate subnet level.
support only allow rules.	                support allow & deny rules.
stateful                                    stateless, it return traffic must be allowed explicitly.
cannot block specific IP                    can block specific IP Address using NACL.
All rules evaluated before decision.     	Rules are processed in number order when deciding whether allow traffic.
start with instance launch configuration.	assigned to subnet for all instance.
applies when someone specifies SGs          do not depend on user it automatically apply all instances with subnet.
  when launching the instance.	



//////////////////////
[EC2]

Options:


- Spot Fleet
  a set of Spot Instances and optionally On-demand Instances. It allows you to automatically request Spot Instances with the lowest price.



Elastic IP:  5 per account
But suggest to use random public IP + DNS, or use LB.

- EC2 Instance Placement Groups:
  Cluster:   in a single AZ (Availability Zone)
             High network performance, 
  Spread:    instances spread across hardware, limit max 7 instance per AZ per PlacementGroup
             High availability
  Partition: instances spread across particions across AZs, (max 100 instance, up to 7 partitions per AZ)
             Medium network, medium availability

- ENI
  logic Network Card in VPC bound to a specific AZ.
  Can attach to EC2 instances on the fly.  (failover case)

- Hibernate
  Dump RAM to EBS.

- EBS (Elastic Block Store)
  EBS is a network drive attach to an EC2 instance.
  One EBS can only be mounted to one instance at a time (at CCP level), but EC2 can have multiple EBS attached.
  EBS is bound to a specific AZ
  Delete on termination:
    root EBS is deleted by default on EC2 termination (but canb e configured to preserve)
    other EBS is not deleted by default on EC2 termination.
  Use EBS Snapshot to transfer (copy) volume across AZ or Region.

  EBS Volume Type
  - gp2/gp3 (SSD):               General Purpose.   (gp3: independent IOPS/Throughput)
                                 IOPS: 3000 ~ 16000, 
                                 throughput: 125~1000 MB/s
  - io1/io2 Block Express (SSD): Low latency High throughput
                                 IOPS: [io1] 32000/64000(Nitro EC2), [io2] 256000
                                 Throughput: 
                                 Support Multi-attach
  - sl (HDD):                    Low cost throughput indensive
                                 IOPS: 500
  - sc (HDD):                    Lowest cost low frequent access (Cold)
  Bootable EBS:gp2/gp3, io1/io2; (HDD cannot be used as bootable disk)


  - EBS instance-store,          ephemeral, physically attached to EC2, very high IOPS/Throughput
                                 IPOS: ~1400000
                                 Throughput
  - EBS Multi-attach (io1/io2)   EBS:EC2(within same AZ) = 1:N, 
                                 max=16, 
                                 cluster-aware file system

- EFS (Elastic File System)
  EFS,  multi EC2 concurrent access, across-AZ/Region, scalable, 

  Storage-class:
  - Standard
  - EFS-IA (Infrequent Access),  with Lifecycle Policy
           Availability: Regional (Across AZ) | One-Zone (single AZ)

  - EBS               vs               EFS    
    one instnce                      multi-instances (100)
    (except multi-attach io1/io2)
    same AZ                          Across AZ




///////////////////////////
[ELB]

LB listener:
  A listener is a logical entity within a load balancer that checks for incoming traffic based on the configured protocol and port.

Type of LB
- CLB (Classic LB),  old Gen, to obsolete
  HTTP/HTTPS, SSL, TCP
- ALB (Application LB),  L7
  LB to multiple application within or across machines
  >> Great fit to micro-service/containerized-application
- NLB (Netowrk LB)
  TCP, UDP, TLS,
- GWLB (Gateway LB)
  L3 (IP)


LB SG (Security Group)
EC2 only allow traffic from LB.

ALB (Application LB)
  L7 HTTP/HTTPS, websocket
  Supprot port-mapping
  Direct traffic to Target-groups: EC2 instances/EC2 tasks/Lambda functions/Private-IP
    >> ALBs can route traffic to different Target Groups based on URL Path, Hostname, HTTP Headers, and Query Strings.
    >> ALB can redirect all HTTP requests to HTTPS by config listener rule -
       on HTTP lister, redict HTTP to HTTPS, and create HTTPS listener to handle client requests (been redirected)
  ALB terminate client IP, (using ALB IP to communicate with target executor)
  ALB comes with a fixed hostname/url
  Latency:  400ms
  >> You can't attach an Elastic IP address to Application Load Balancers.

NLB (Network LB),  
  L4 TCP/UDP
  IP: One static IP per AZ, support Elastic IP   (==> support 1,2,3 IPs to access)
  Direct traffic to Target-groups: EC2 instances, IP addresses (Private), ALB
  Health check support: TCP, HTTP, HTTPS
  RPS:     million
  Latency: 100ms
  >> NLB provides the highest performance and lowest latency if your application needs it.

  Use cases:   We can deploy NLB infront of ALB.   NLB provide fixed static IP

GWLB
  L3 IP
  Function: GW and LB
            Deploy a fleet of 3rd party Network Virtual Apps in AWS (eg. FW, IDP, DPI)
  Protocol: GENEVE on port 6081,
  Target Groups:  EC2 instances, IP Addresses (private)

  Users (source) ----> =GWLB= ---> Application (dest)
                       |    ^
              (GENEVE) |    |
                       |  Security APPs
                        > (Firewall, Intrution Detect, Deep Packet Instpection ...)


Access LB via DNS/IP
Only Network Load Balancer provides both static DNS name and static IP. 
Application Load Balancer provides a static DNS name but it does NOT provide a static IP.


ELB reserved cookie names:  AWSALB, AWSALBAPP, AWSALBTG


Sticky Sessions
  client --stick--> backend instances
    (cookie)
  work for CLB, ALB, NLB.

  Cookie type:
  - Application-based Cookies
    >> Customer cookie:
       Generated by target, 
       use target group specific name. (not LB reserved name)
    >> Application cookie: 
       Generated by LB.
       name: AWSALBAPP
  - Duration-based Cookies
    Generted by LB
    name: AWSALB (for ALB), AWSELB (for CLB)

Client IP:
  When using an ALB, the IP address the backend receives requests from will be the ALB's private IP .
  To get the client's IP, ALB adds an additional header called "X-Forwarded-For" contains the client's IP address.


Cross-Zone LB
  each LB distribute evently accross all the registered instances in all AZ.

  By default:
    ALB: enabled. no charge for inter-AZ data
    CLB: disabled. no charge for inter-AZ data
    NLB: disabled. charge for inter-AZ data
    GWLB: disabled. charge for inter-AZ data

  Cross-Zone enabled:  LB per instances of all AZs
             disabled: LB per AZs

  eg.
    AZ1 (2 instances), AZ2 (8 instances):
    Cross-zone LB enabled:  each instance gets 1/10
                 disabled:  each AZ get 1/2


ELB SSL Certificate
  ACM (AWS Certicifate Manager)
  SNI (Server Name Indication) 
    SNI allows you to expose multiple HTTPS applications each with its own SSL certificate behind the same listener (LB). 
       by binding multiple certs to the same secure listener on the LB
    >> allow loading multiple SSL cert for one web server
       only works for ALB, NLB, CloudFront.

  CLB(v1): support only one SSL Certificate
  ALB(v2): support multiple listensers with multiple SSL certs, with SNI
  NLB(v2): support multiple listensers with multiple SSL certs, with SNI


Connection Draining:
(The timer for graceful termination of existing connection)
typical use cases:
  scaling (up/down), graceful shutdown, deregistration at health-check failure 


ASG (Auto-Scaling-Group)
  goal:
    - scale out (+) / scale in (-) per load
    - add/remove EC2 instances on health check
    - ensure min/max number of EC2 instances
  setting: 
    min-capacity, desired-capacity, max-capacity
  Lauch Template:
    - AMI, Instance type, EBS volumes, User data, SG, IAM role, ssh key-pairs, network/subnet, LB, 
  Scale triggering:
    - metrics,  e.g. EC2 requests target, network I/O, 
    - CloudWatch Alarms (e.g. Avg CPU, or customize metrics)
    - health check
  Scaling Policy
    - Dynamic Scaling:
        Target Tracking Scaling (e.g avg CPU stay at 40%)
        Simple/Step Scaling: CloudWatch alram trigger. e.g (CPU>70% +1, CPU<30% -1)
    - Scheduled Scaling:
    - Predictive Scaling:(based on continous forcast)
  Scaling Cooldown (stablize).   default 300s

  >> You can configure the Auto Scaling Group to determine the EC2 instances' health based on Application Load Balancer Health Checks instead of EC2 Status Checks (default).
     When an EC2 instance fails the ALB Health Checks, it is marked unhealthy and will be terminated while the ASG launches a new EC2 instance



////////////////////////////////
[RDS]
(Amazon Relational Database Service)

Support:
  Postgres, MySQL, MariaDB, Oracle, Microsft SQL Server, IBM DB2, Aurora (AWS)

Advantages:
  - Auto provision, OS patch
  - Auto backup/restore
  - Read replicas
  - Auto scaling (Horizontal & Vertical)
  - Multi Availability zones and DR (Disaster recovery)
  - Storage backed by EBS (gp2 or io1)
  - Mornitoring Dashboard
Limitation:
  - User cannot ssh into the DB instances

Storage Auto-Scaling.
  trigger: e.g.
  - free storage < ?%
  - low storage at least 5 min
  - xx hours since last modifiction


RDS Read Replicas 
  - up to 15
  - within AZ or cross AZ/Region
  - Replication is Asynchronous, -- eventually consistent   *
  - can be promoted to their own DB.
  - need to update the connection string to leverage the read replicas   *
    (however, Multi-AZ keeps the same connection string regardless of which database is up.)

  replication traffic is free within same region, even if across AZ

RDS Multiple AZ 
(for Disaster Recovery)
  - Synchronous Replication.   *
    When enable Multi-AZ, RDS automatically creates Standby replica in different AZ within the same Region.
  - One same DNS name, auto APP failover
  - Not used for scaling

  >> Multi-AZ deployments are not designed for cross-region HA; they focus on ensuring availability within a single Region12
  
  >> The read replicas can be setup as Multi-AZ for Disaster Recovery.
  >> How to make RDS from single-AZ to multi-AZ, with zero downtime
     - modify DB setting to change SZ to MZ. behind the scene
       1> DB snapshot taken
       2> Standby DB created from DB snapshot
       3> Sync replication start b/w master & standby
  >> Read Replicas will help when analytics query slows down the main RDS DB,
     as your analytics application can now perform queries against read-replica(s), and these queries won't impact the main production RDS database.


Synchronous or Asynchronous Replication:
  RDS support both Synchronous and Asynchronous replication.
  - RDS Multi-AZ uses synchronous replication for high availability on multi-AZs within the same AWS Region.
    Replication occurs synchronously between the primary and standby replica WITHIN the same Region
    (Multi-AZ only support same Region)
  - Read replica:
    RDS Read Replica use Asynchronous replication -- enventual consistent
  - Cross-region read replica: 
    Asynchronous replication.
    Cross-region read replicas provide global read scalability and disaster recovery, but with replication lag.

  >> For multi-AZ RDS replication, synchronous replication is used.
  >> For non-Multi-AZ deployments or cross-region replication, asynchronous replication is used.
  >> In contrast, Aurora uses synchronous replication for both within-region and cross-region deployments.

RDS Custom
  support Oracle and MS SQL DB.
  Access the underlying DB and OS, to
  - config setting, instal patches, enable native features, 
  - access the underlying EC2 instance using SSH or SSM Session Manager
  Need to de-activate automation Mode to perform the customization
  RDS s RDS custom:
  - RDS: DB and OS fully managed by AWS
  - RDS Custom: full admin access to the underlying OS and DB.


Amazon Aurora
  AWS proprietary .
  Aurora support both Postgres and MySQL
  AWS cloud optimized:  Performance x5 (MySQL on RDS), x3 (Postgres on RDS)
  Auto expanding (storage grow), 10G ~ upto 128T.
  Master + up to 15 replicas, faster replicatin process than MySQL (sub 10 ms replica lag)
  Auto scaling (read replicas) and push-button scaling
  HA native, auto failover/backup & recovery (backtrack: to restore at any point of time)
  cost +20% than RDS.

  Aurora HA:
  - 6 copies across 3 AZ (1 primary, 5 secondary DB cluster)
  - 4/6 for Write, 3/3 for read
  - self healing with peer-peer replication
  - storage striped across 100s of volumes. (Shared storage volume). Autom expand.
  - within-region replication: synchronous between primary and secondary clusters
  - support cross region replication (Synchronous)
  - failover < 30s

  RDS Multi-AZ vs Aurora Global Database:
  - Choose RDS Multi-AZ for simpler HA needs within a single region.
    RDS Provides HA on different AZ within the same AWS Region
    RDS cross region read replicas are Asynchronous and serve READ workloads.  (so it is not a fully R/W HA across region)
  - Choose Aurora Global Database for global HA, low-latency reads, and disaster recovery across multiple AWS Regions. (Synchronous replication)


  Write Endpoint (point to Master)
  Reader Endpoint (Conntion Load Balancing)
             Client 
           /        \ 
      (Write)      (Read)
        /              \
    Write EP     Read EP1, EP2, ...
       |               |   |
     [===Shared Storage Volume=== ]


  Aurora Replicas Auto Scaling

  Custom Endpoint
  - define a subnset of Aurora instances as a Custom Endpoint
    e.g. Run analytical queries on specific replicas.
  - Reader Endpoint is generally not used after defining Custom Endpoints
  >> we can setup multiple Custom Endpoints for different type (subsets) of queries.

  Aurora Serverless
  - 'managed Aurora'
  - Automated DB instantiation and auto-scaling ased on actual usage
  - no capacity planning needed.
  - pay per sec
  - good for in-frequent, intermittent or unpredicatable workloads.

  Global Aurora
  - Cross Region Read Replicas
  - 1 Primary Region (R/W) + up to 5 Secondary (RO) regions. Replica lag < 1s cross region
    up to 15 replicas in each secondary region
  - Promoting another region (for disaster recovery), RTO < 1min
  >> max 5 secondary region, max 15 replicas in each region.   *

  Aurora Machine Learning
  - ML based prediction to applications
  - optimized and secure integration b/w Aurora and AWS ML services
  - Support: Amazon SageMaker (any ML model), Comprehend (for sentiment analysis)
  use cases: frad detection, ads targeting, product recommendation, sentiment analysis etc

  Aurora DB cloning
  - Create a new DB cluster from the existing one
  - Faster than Snapshot & restore, 
  - using copy-on-write protocol
    init new DB using the same DB volume (fast and no copying)
    when updates are made, additional storage is allocated and data is copied to be separated.
  - Fast & cost-effective
  >> Use case: user create 'staging' DB from a 'production' DB without impacting the prod DB

RDS & Aurara Backup & restore
  Backup
  - Automated backups.  (1~35 days of retention), 
        Can be disabled in RDS, cannot be disabled for Aurora.
    Full backup (daily)
    Transaction log backup every 5s
    Restore to any point
  - Manual DB Snapshots. (retain as long as you want)
  >> You will still pay for storage in a stopped RDS DB.
     Use snapshot & restore if you plan to stop it for long time. (cost much less)
  Restore
  - Restore RDS/Aurora backup or snapshot to create a new DB.
  - Restore MySQL RDS DB/Aurora cluster from S3
    Use case:
      Create backup of on-premises DB, (For Aurora needs to use Percona XtraBackup)
      store in AWS S3
      Restore the backup into a new RDS instance/Aurora cluster
  
RDS & Aurora Security
  - At-rest encryption
    DB master & replicas encryptioned using AWS-KMS
    must define at launch time
    if master is not encrypted, the read replicas can NOT be encrytped
  - In-flight encryption
    TLS-ready by default.
    use AWS TLS root certificate clients
  - IAM Authentication - connect DB using IAM roles (instead of usr/pwd)
  - Security Groups -  control network access to RDS/Aurora
  - No SSH except on RDS Custom
  - Audit logs to CloudWatch


RDS Proxy
  Fully managed DB proxy for RDS
  - Allows apps to pool and share the DB connections
  - improve efficiency
  - Serverless, auto-scaling, HA (multi-AZ)
  - Redulce RDS & Aurora failover time by up 66%
  - No code change required for most apps
  - Enforce IAM AUthentication for DB, securely store credentials in AWS Secrets Manager
  - RDS Proxy is NEVER public accessible. must access from VPC
    VPC:  Lambda functions ---(private subnet)--- RDS Proxy -- RDS DB Instance(s)


ElastiCache
  Managed Redis or Memcached.
  - In-memory DB (High performance, low latency)
  - Reduce read-intensive workloads
  - Helps to make your application stateless
  - AWS take care of OS maintentance/patching, optimization, setup, config, monitor, backup, failure recovery
  - Using ElastiCache involves HEAVY APP code changes.

  Solution
  - DB Cache
  - User session store (App writes session data into ElastiCache)
  >> Storing Session Data in ElastiCache is a common pattern to ensuring different EC2 instances can retrieve your user's state if needed.

  Redis vs Memcached
  Redis is an in memory DB with HA (using Replication);   
    Rich feature, support complex data types. 
    Replication, Data persistent, HA, Backup & Restore
  Memcached is a distributed in memory DB WITHOUT HA (using Sharding).  
    Simiple (only string data), Fast Key-Value storage. Multi-threaded performance
    Sharding, No Data-persistent, No HA, No Backup & Restore

      REDIS                                 MEMCACHED
    ----------------------------------------------------------  
                                            Sharding (Multi-node for Data partitioning)
                                            Multi-thread architecture
    Multi-AZ/Auto-Failover                  No multi-AZ/Failover
    ReadReplicas & HA                       No replication, No HA
    Data Durability with AOF persistance,   No data persistent. Data lost at restart
    Backup & Restore                        No backup & restore
    Support Set/Sorted-sets

  'AOF': Append Only File.  -- every write operation is logged into file, the cache can be re-constructed automatically at catastrophic event.

  ElastiCache Security
  - support IAM Authentication
    to enhance the security of ElastiCache Redis Cluster by allowing users to access your ElastiCache Redis Cluster using IAM Identities
  - IAM policy on ElastiCache is only used for AWS API-level Security
  - Redis AUTH:    set password/token for Redis Cluster
  - MemCached: suport SASL-based authentication 

  Patterns for ElastiCache
  - Lazy loading.  all read data is cached. (data can become stale in cache)
  - Write Throughput.  (no stale data)
  - Session Store.   store temporary session data (using TTL feature)

  Use Case:
  Redis:
    Gaming Leaderboards.
    Redis Sorted set




//////////////////////////////
Route53
  A HA, Sclable, fully managed and Authoritative DNS
  Route53 is also a Domain Registrar
  Support reource health check
  The only AWS service with 100% availabiity SLA

Concept:
  TLD: Top Level Domain
  SLD: Second Level Domain
  Authoritative: => the customer can update the DNS records.


Route53 - records
  Record contain:
  - Domain/sub-domain name
  - Record Type: (basic) A/AAAA/CNAME/NS     *
                 (advanced) CAA/DS/MX/NAPTR/SOA/TXT/SPF/SRV
  - Value,  e.g. 1.2.3.4
  - Routing Policy
  - TTL

DNS Records Type:
- A:     IPv4
- AAAA:  IPv6
- CNAME: map a hostname to another hostname
    The target domain must have an A or AAAA record
    Can NOT create a CNAME record for the top node of a DNS namspace (Zone Apex)
    e.g.  you cannot create CNAME for example.com,
          you can create CNAME for www.example.com
- NS:    Name Servers for the hosted Zone

Route53 - Hosted Zones
  A container for records that define how to route traffic to a domain and its subdomains
  - Public Hosted Zones: 
    contains records specify how to route to the Internet (public domain names)
  - Private Hosted Zones:
    contains records specify how to route within VPC(s) (private domain names)
  - cost:  $0.5 per month per Hosted Zone

  Public vs Private Hosted Zones

Route53 - Register A Domain

Route53 - Records TTL (Time To Live)
  The time the client cache the DNS query result.
  TTL is mandatory for DNS record except for Alias records.

Route53 - Alias Records
  - map a hostname to an AWS resource
  - An extension to DNS functions
  - Automatically recorgnizes changes in the resource's IP address
  - Can be used for the TOP(Root) node of a DNS namespace (Zone Apex). e.g. example.com
  - Can NOT set TTL for Alias record. (Instead it is set automatically by route53)
Alias Records Targets:
  ELB, CloudFront Distributions, API Gateway, Elastic Beanstalk Env, S3 websites, VPC Interface Endpoint, Global Accelerator, Route53 record in the same hosted zone
  >> you CANNOT set an Alias record for an EC2 DNS name

Route53 CNAME vs Alias
  CNAME:  point the hostname to any other hostname. (ONLY for NON Root domain)
  Alias:  point the hostname to an AWS Resource. (works for both Root and Non-Root domain)
          Free


Route53 - Routing Policies
  Define how Route53 responds to DNS queries

  Policies:
  - Simple: 
    route traffic to a single resource
    can specify multiple values in the same record -- client pick a random one
    when Alias enabled, specify only one AWS resource
    cannot associate with Health Checks
  - Weighted
    control the x% of the requests that go to each specific resource
      weight 0 to a record will stop the traffic to a specific resource
      all records weight 0 means all records same weight, will be returned equally.
    weights don't need to sum up to 100
    DNS records must have the same name and type         
    Can be associated with Health Checks
    Use cases: load balancing between regions
  - Latency-based
    redirect the traffic with the least latency
    based on the traffic between users and AWS regions    *
    can be associated with Health Check (has failover capability)
  - Multiple-value
  - Geolocation
  - Failover
  - 

  Route53 - Health Checks
  - HTTP Checks are only for public resourcs
  - Health Check => Automated DNS Failover
    monitor an endpoint (app, server, AWS resource)
    monitor other Gealth Checks (Calculated Health Check)
    monitor CloudWatch Alarms
  - Health Checks are integrated with CW (CloudWatch) metrics


    Health Checks - Monitor an Endpoint
    - 15 global health checkers for endopint health
      Failure Threshold - 3 (default)
      interval - 30 sec (standard), 10 sec (fast)
      supoorted protocol: HTTP, HTTPS, TCP
      if > 18% Health Checkers report the endpoint is healthy, Route 53 consider it Healthy.
      Ability to choose the location you want Route 53 to use
    - Health Checks pass only when the endpoint responds with 2xx or 3xx status codes
    - Health Checks can be setup to pass/fail based on the first 5120 Bytes of the response
    - Configure router/firewall to allow incoming requests from Route53 Health Checkers

    Calculated Health Checks
    - Combine the results of multiple Health Checks into a single Health Check
    - Support OR/And/Not logic
    - Can monitor up to 256 Child Health Checks.
      (Parent Health Checke -- (1:N) -- Child Health Check)
    - specify how many of the health checks need to pass to make the parent pass
    >> Use case:  perform maitenance to website without causing all health checks to fail.

    Health Checks - Private Hosted Zones
    - Route53 Health Checkers are outside the VPC
      Health Checkers cannot access private endpoint (in private VPC or on-premises resource)
    - Solution: Create CloudWatch Metric and associate CloudWatch Alarm,
                then create Health Check on the alarm


  Routing Policies - Failover (Active-Passive)
    Primary/Secondary EC2 Instances are associated with Route53 Health Check.
    When DNS requests comes, the response is based on Health Check + Primary/Secondary


  Routing Policies - Geolocation
    Routing is based on user location
    Specify location by Continent, Country or US State (If overlapping, most precise location selected)
    Should create 'Default' record (in case there is no match on location)
    Can be associated with Health Checks
    >> Use cases: website localization, restrict content distribution, load balancing

  Routing Policies - Geoproximity Routing
    Route traffic based on the geographic location of users and resources.
    Ability to shift more traffic to resources based on the defined bias
      bias: expand (1~99) - more traffic to the resource
            shrink (-1~-99) - less traffic to the resource
    Resources can be:
      AWS resource (specify region)
      Non-AWS resource (specify Latitude & Longitude)
    Must use Route 53 Traffic Flow to use this Feature. 
 
  Routing Policies - IP-based Routing
    based on client IP
    client CIDRs -> endpoint/location (user-IP-to-endpoint mapping)
    >> Use cases: route particular clients to specific endpoints

  Routing Policies - Multi-Value
    Route traffic to multiple resource
    Route 53 return multiple values/resourcs
    Can be associated with Health Check (return only values for healthy resources)
    Up to 8 healthy records are returned / query
    Multi-Value is NOT suitable for ELB            *
  

  Domain Registar vs DNS Service
    they are different services
      Domain Registar -- register your domain name, typically payiing annual charges.
                         Domain Registar usually provides DNS service as well
      DNS Service -- manage DNS records
    You can buy Domain Registar from SP A and use DNS service from SP B. 
      -- Configure the DNS servers in the Domain Registar
    So you can combine Amazon Route53 with 3rd party Domain Registar.
      - create a Public Hosted Zone in Route53.   (Has to be Public Host Zone since it needs to be accessible by 3rd party)
      - config NS (NameServer) records on 3rd party Domain Registar to use Route53 Name Servers


Elastic Beanstalk
  an orchestrated service for deploying and scaling web application and servcies.
  a managed service than automatically handles the
    capacity provisioning, LB, scaling, app health monitor, instance configuration, ...
  so that allow user to focus on the Web App itself.
  User still have full control over the configuration.
  Beanstalk is free, but user needs to pay the underlying instance/components

  Elastic Beanstalk - components
  - Application
    collection of ElasticBeanstalk componets: env, versions, configurations, ...
  - Application Version
    a iteration of app code
  - Environment
    collection of AWS resources running an application versions
    Tiers: Web Server (FE) Environment Tier & Worker (BE) Environment Tier
    Support multiple Env (Dev, prod, test, ...)
    Flow:   Create Application -- Upload Version -- Launch Env -- Manage Env
  
  Elastic Beanstalk - Supported Platforms
    Go, Java, .NET (Lnx/Win), Node.js, PHP, Python, Ruby, Packer Builder, 
    Single-Container Docker, Multi-Container Docker, Preconfigured Docker

  Elastic Beanstalk Deployment Modes
    - Single Instance (e.g for Dev)
    - HA & LB  (for Prod)



Instantiating Applications quickly
  EC2 Instances:
  - Golden AMI. 
    Ready made AMI with App&Env, lauch EC2 instances from the Golden AMI directly.
  - Bootstrap using User Data. 
    For Dynamic configuration, use User Data Scripts
  - Hybrid: Elasitc Beanstalk (mix Golden AMI and User Data)
  RDS Databases:
  - Restore from snapshot (Schemas & data ready)
  EBS Volumes:
  - Restore from snapshot.

Reduce the cost for the deployment with a known min# of EC2 instance --> Reserved Instances.




///////////////////////////////
S3
  Infinitly scaling storage


  S3 - Buckets
    Store 'objects' (files) in 'Buckets' (directories)
    Buckets have globally unique name (accross regions all accounts) -- the only globally unique thing in AWS
    Buckets are defined at the region level
    Name convention:  
      No uppercase, no undersore
      3-63 char long
      not an IP
      must start with lowercase letter or number
      must not start with the prefix 'xn--'
      must not end with the suffix '-s3alias'
    
    
  S3 - Objects
    - Objects (files) have a Key -- a Full Path
      Key is a long name that contains '/'
      Objects Key = Prefix + ObjectName.   eg. s3://my-bucket/my-folder/sub-folder/my-file.TXT
                                                              ---------------------'''''''''''
                                                                      prefix        objectName
    - Object value -- the content of the body
      max object size: 5TB
      if uploading more than 5GB, must use 'multi-part' uploading
    - Metadata: list of text key/value pairs, system or user Metadata
    - Tags:  unicode key/value pair, up to 10.  useful for security / Lifecycle
    - Version ID (if versioning is enabled)

  S3 - Security
    - User-based
      IAM Policy
    - Resource-based
      Bucket Policies: allows cross account
      Object ACL -- finer grain   (can be disabled)
      Bucket ACL -- less common   (can be disabled)
    - Encryption
      encrypt objects in S3 using encryption keys
    >> an IAM principle can access an S3 object if
       (The user IAM permission Allows it OR the resource policy Allows it) AND (there is no explicit DENY)

    S3 - Bucket Policies
      JSON based Policies 
      - Resources: buckets and object
      - Effect: Allow / deny
      - Actions: set of API to Allow or Deny
      - Principal: the account or use to apply the policy to
      e.g.
          {
            "Version":"2012-10-17",
            "Statement":[
              {
                "Sid":"AddPerm",
                "Effect":"Allow",
                "Principal": "*",
                "Action":["s3:GetObject"],
                "Resource":["arn:aws:s3:::examplebucket/*"]
              }
            ]
          }
      Use S3 bucket policy to 
      - Grant public access to the bucket
      - Grant access to another account (Cross Account)
      - Force object to be encrypted at uploading

      Bucket settings for Block Public Access: to prevent company data leaks    
        (a default overall setting, disable this and apply other access policies)
        Can be set at account level

    Use cases
    >> Public Access - Use Bucket Policy
    >> User Access to S3 - IAM permissions
    >> EC2 instance access - User IAM Roles
    >> Cross-Account Access - User Bucket Policy
  

  S3 - Static Website Hosting
    S3 can host static websites accessible on Internet

    the website URL will be:
      http://<bucket-name>.s3-website-<aws-region>.amazonaws.com    or
      http://<bucket-name>.s3-website.<aws-region>.amazonaws.com

    >> if get 403 Forbidden error, typically that's access issue and you need to set the bucket policy to allow public read.


  S3 - versioning
    Version files in S3
    enabled at bucket level
    same key overwrite will change the version: 1,2,3...
    protect against unintended deleted
    can roll back to previous version
    Any file that is not versioned prior enabling versioning will havbe version 'null'
    suspending versioning does not delete the previous versions
    delete operation is 'adding a delete marker' if versioning enbled.


  S3 - Replication (CRR & SRR)  
    Must enable versioning in source and dest buckets
    Must hav eproper IAM permission to S3
    CRR (Cross-Region Replication)
    SRR (Same-Region Replication)
    Buckets can be in different AWS accounts
    Copying is asynchronous

    S3 Back Replication:
      After enabling Replication, only NEW objects are replicated.
      to replicate existing objects, use S3 Batch Replication

    delete marker can be configured to be replicated or Not be replicated.
    when replicating, delete operation is not replicated, only delete marker is. 
      so when deleting a specific version (which is a permenant deletion), this operation will NOT be replicated.    *

    Use Case
    >> CRR - Compliance, low latency access, replication across accounts
    >> SRR - log aggregation, live replication b/w production and test accounts


  S3 Storage Classes
    Standard - General Purpose
    Standard-Infrequent Access (IA)
    One Zone-Infrequent Access
    Glacier Instant Retrieval
    Glacier Flexible Retrieval
    Glacier Deep Archive
    Intelligent Tiering

    Can move between classes manually using S3 Lifecycle configurations

  
  S3 Durability & Availability
    Durability:
      High Durability (99.999999999% 9s) of objects across multi AZ 
      same for all storage classes
    Availability:
      Measure how readily available a service is
      Varies depend on storage class


  S3 Standard
    99.99% availabiity
    Used for frequently accessed data
    Low latency and high throughput
    Sustain 2 concurrent facility failures
    >> Use cases:  big data analystics, mobile/gaming app, content distribution

  S3 Standard - IA  (infrequent Access)
    99.9% Availability
    lower cost than Standard but has retrieve cost
    >> Use cases: Disaster Recovery, backups

  S3 One Zone-IA  (infrequent Access)
    99.5% Availability
    High Durability in a single AZ only 99.999999999%
    >> Use case: store secondary backup copies of on-premise data, or data that can be recreated.

  S3 Glacier
    Low cost for archiving / backup
    Pricing:  storage + object retrieval cost

    Glacier Instant Retrieval
      ms retrieval, 
      mini storage duration: 90 days
      >> Use case: data accessed once a quarter  

    Glacier Flexible Retrieval
      Expedited (1~5 min), Standard (3~5 hours), Bulk (5~12 hours) - free
      mini storage duration: 90 days
      >> Use cases:

    Glacier Deep Archive
      Standard (12 hours), Bulk (48 hours)
      Mini storage duration 180 days
      >> Use cases:

  S3 Intelligent-Tiering
    Small monthly monitoring and auto-tiering fee
    Move objects automatically between Access Tiers based on usage
    No retrieval charges

    Freqent Access tier (auto): default
    Infreqent Access tier (auto): object not accessed for 30 days
    Archive Instant Access tier (auto):  object not accessed for 90 days
    Archive Access tier (optional):  configurable from 90~700+ days
    Deep Archive Access tier (optional): confgurable from 180-700+ days


  S3 - Lifecycle Rules 
    Transition Action:  move objects to <Class XX> <days> after creasion
    Expiration Action:  config object to expire (delete) after some time
      - Access log can be deleted after 365 days
      - Can be used to delete old version of file (if versioning is enabled)
      - Can be used to delete incomplete multi-part uploads
    Rules can be created for a certain prefix
    Rules can be created for a certain objects Tags
    >> Use cases:  
       S3 objects need to be recoverable immediately for 30 days (rarely happen); 
       then after 365 days, deleted objects should be recoverable within 48 hours.
       Solution:
         Enable S3 versioning -> deletion recoverable
         Transition rule 1: non-current versions to Standard IA,
         Transition rule 2: (after rule 1) non-concurrent versions to Glacier Deep archieve
    >> Use case: cleanup the old unfinished parts from multi-part upload
       Solution: S3 Lifecycle Policy to automate the old/unfinished parts deletion
       

  S3 Analytics - Storage Class analysis  
    Report updated daily, with the objects StorageClass, date, age, etc
    Help to decide when to transition objects so the right storage class
    Recomendations for Standard and Standard IA. (Not work for One-zone or Glacier)
    24-48 hours to start seeing the data analysis

S3 - Requester Pays.
  'Requester Pays buckets'
  The requester must be authenticated in AWS (not anonymous)


S3 Event Notification
  Events: 
    e.g. S3:ObjectCreated, S3:ObjectRemoved, S3:ObjectREstore, S3:Repication
  Object Name filtering possible
  Can create as many S3 events as desired
  S3 events can be delivered to SNS, SQS, Lambda Function, etc
  S3 event notification typically deliver events in seconds, but can take a min or longer sometimes.
  for the notification: define  Event, object Filter, destinations (with Access Policy)

  >> Use case: generate thumbnails for images uploaded to S3.


  S3 Event Notification - IAM permissions
    Defie Resource Access Policy on the target resources (SNS, SQS, Lambda Funciton, etc)
    e.g 
      SNS Resource (Access) Policy
      SQS Resource (Access) Policy
      Lambda Resource Policy

  S3 Event Notification - Amazon EventBridge
    EventBridge can be turned on/off
      when enabled, all S3 Event Notification are sent to Amazon EventBridge
    Features:
      - Advanced filtering options with JSON rules (object name, size, metadata, ...)
      - Multiple Destinations
      - EventBridge Capabilities - Archive, Replay Events, Reliable delivery, ...

  S3 - Baseline Performance
    S3 auto scales to high reqeust, latency 100-200ms
    3500 PUT/COOPY/POST/DELETE or 5500 GET/HEAD Requests per sec per prefix in a bucket
    No limits to the number of prefixes in a bucket
    Spreading reads across N prefixes evently, you can achieve N * 5500 reqeust/s for GET/HEAD.

    Multi-part upload:
      Parallelize uploads
      Recommended for file > 100MB, Must use for files > 5GB
      >> Speed up uploading
    
    Transfer Acceleration:
      Increase transfer spped by transferring file to AWS Edge location, which will forward data to S3 bucket in the target region.
      Files ---(public)---> Edge location in target Region ---(private AWS)---> Bucket in target Region
      Compatible with Multi-part upload
      >> Speed up uploading

    S3 Byte-Range Fetches
      Parallelize GETs by requesting byte ranges
      better resilience in case of failures
      >> speed up downloading
         or used in retrieving only partial data (e.g. File header)

  S3 Select & Glacier Select
    Retrive less data using SQL by performing Server-Side Filtering
    Can filter by rows & columns (simple SQL statements)
    less data -> less network transfer / CPU / Cost


  S3 Batch Operations
    Perform bulk operation on S3 objets with a single request.
    A job consists of a list of objets, the action to perform and optional parameters
    -- Use S3 Inventory to get object list and    *
       Use S3 Select to filter object
       S3 Inventory --(Objects list)--> S3 Select --(filtered objects)--> S3 Batch Operations --(processed Objects)
    benefits:
      S3 Batch Operation manages retries, tracks progress, send completion notifications, generate reports.

    e.g.
      Modify object metadata/properties
      Copy objects
      Encrypt un-encrypted objects
      Modify ACLs, tags, ..
      Restore object form Glacier
      Invoke Lambda function for custom action on each object


  S3 - Storage Lens
    Analyze, optimize storage
    Aggregate data for Organization/Accounts/Regions/Buckets/prefixes
    Default/Customized dashboard
    can be configured to export metrics daily to an S3 bucket (CSV, Parquet)

    Default Dashboard
      Visualized summarized insights
      Shows multi-Region & multi-accounts data
      Cannot be deleted but can be disabled

    metrics
      Summary metrics
        e.g. StorageBytes, Object counts
        >> Use case:  identify fastset-growing or not-used buckets/prefixes

      Cost-Optimizaiton metrics
        NonCurrentVersionStorageBytess, etc
        >> Use case: identify buckets incomplete multi-part uploaded loder than 7 days
                     idnetify the objects could transition to lower-cost storage class

      Data-Proetction metrics 
        e.g. VersioningEnabledBucketCount
        >> Use case: identify buckets that are not following data-protection best practices

      Access-management metrics
        insight for S3 Object Ownership
        >> Use case: which object Owership setting the buckets use

      Event metrics
        >> 

      Performance metrics

      Activity metrics

      Status Code metrics

      Free vs Paid metrics
      - Free metrics
        contains 38 usage metrics
        available for query for 14 days
      - Advanced metrics
        Advance metrics: Activity, advanced cost optimizatin, advanced data protection, status code
        Cloudwatch publishing
        Prefix aggregation (Collect metrics at the prefix level)
        Data available for query for 15 months

  S3 - Object Encryption
    S3 Object Encryption methods:
    SSE: (Server-Side Encryption)
    - SSE-S3:  (Server-Side Encryption with S3 Manged Keys) 
      Encrypt S3 Objects using keys handled, managed and owned by AWS
      Enabled by Default
    - SSE-KMS: (Server-sdie Encryption with KMS Keys stored in AWS KMS)
      Leverage AWS Key Management SErvice to manage encryption keys
    - DSSE-KMS: (Dual-layer Server-Side Encryptin with KMS)
    - SSE-C:   (Server-side Encryption with Customer-Provided Keys)
      customers manage their own encryption keys.
    CSE (Client-Side Encryption)

    S3 Encryption - SSE
    - Keys handled, managed and owned by AWS
    - Server-Side encryption
    - AES-256
    - Must set header 'x-amz-server-side-encryption':'AES256'
    - Enabled by default for new buckets & new objects

    S3 Encryption - SSE-KMS
    - Keys handled and managed by KMS
    - KMS advantages:  user control + audit key usage using CloudTrail
    - Server-Side encryption
    - Must set header 'x-amz-server-side-encryption':'aws:kms'
    Limitation:
      impacted by KMS limit
      when uplad, call GenerateDataKey KMS API
      Count towards the KMS quota per second (5500,10000,30000 req/s based on region)
      Can request a quota increase using the Service Quota Console

    S3 Encryption - SSE-C
    - Keys handled and managed by Customer outside AWS
    - AWS S3 does NOT store the encryption key
    - Server-Side encryption
      AWS S3 use the client provided key (in every HTTPS header) to encrypt the data
    - HTTPS must be used
    - Encryption Key must provided in HTTP headers, for every HTTP request mode
    - HTTP header set 'x-amz-server-side-encryption-customer-algorithm':'true'

    S3 Encryption - CSE
    - Client fully manage the keys and encryptioned
    - Client encrypt data before sending to S3
    - Client decrypt data when retrieving from S3
    - Use client side lib e.g. AWS S3 Client-Side Encryption Library

  S3 - Encryption in transit (SSL/TLS)
    AWS S3 exposes 2 endpoints:  
      HTTP Endpoint
      HTTPS Endpoint 
    HTTPS is mandatory for SSE-C
    Most client use HTTPS by default

  Force Encryption in Transit:  
    set S3 bucket policy to deny HTTP
    connection option of 'aws:SecureTransport' == True|False


  S3 - Default Encryption vs Bucket Policies
    Default Encryption: SSE-S3
    Optionally, force encryption using bucket policy
      -> Refuse API call of PUT (S3 Object) without encryption header
      -> Deny condition: 'x-amz-server-side-encryption-customer-algorithm':'true'
    >> Bucket Policies and evaluated before 'Default Encryption' setting


  CORES (Cross-Origin Resource Sharing)
    -- Web browser based mechanism to allow requests to other origins while visiting the main origin
    "Acces-Control-Allow-Origin: <origion>"
    "Access-Control-Allow-Method: <Methods>"
      Origin = schema (protocol) + host (domain) + port
         eg:    https               example.com     443  

    Alow the 'other origin' requests using CORES Headers (e.g. Access-Control-Allow-Origin)

            Origin Web Server                          Other Web Server
    (origin.com)                                          (other.combine)
    HTTPS Req ---->
                    --------- Preflight Request -------->
                    OPTIONS/
                      Host: other.com
                      Origin: origin.com
                    
                    <-------- Prefilight Response ---------
                     Access-Control-Allow-Origin: https://origin.com
                     Access-Control-Allow-Method: GET, PUT, DELETE

                    <---(CORS Header received by the origin)--> 
                    GET / 
                      Host: other.com
                      Origin: origin.com
    
    S3 - CORS
      Needs to enable the correct CORS headers for client requests to S3 bucket.      *
      allow specific origin or * (all origin)

      S3 CORS setting:
      e.g
        [
          {
            "AllowedHeaders"  [ "Authorization" ],
            "AllowedMethods": [ "GET" ],
            "AllowedOrigins": [ "<url of origin (without slash at the end)>" ],
            "ExposeHeaders":  [],
            "MaxAgeSeconds": 3000
          }
        ]


  S3 - MFA Delete
    MFA (Multi-Factor Authentication)
      force user to generate a code on device beofre doing important operations on S3
    To use MFA delete, Versioning must be enabled on the bucket
    Only the bucket owner (root account) can enable/disable MFA Delete

    MFA will be required when, e.g.
    - permanent delete on an object version
    - Suspend Versioning on the bucket


  S3 Access logs
    log can be enabled on S3 buckets
    The log date can be analyzed using data analysis tool
    the target logging bucket must be in the same AWS Region
    Do NOT set the logging bucket to be the monitoring bucket! -- will create logging loop

  S3 - Pre-Signed URLs
    Users given a pre-signed URL inherit the permissions of the user that generated the URL for GET/PUT
    Generate pre-signed URLs using S3 console, AWS CLI or SDK
    URL Expiration
      - S3 console - 1~720 min (12 hours)
      - AWS CLI - configure expiratin with --expires-in parameter in sec (default 3600 sec, max 604800sec)

    >> Use cases:
       allow only logged-in users to download a premium video from you S3 bucket
       allow an ever-changing list of users to download files by generating URLs dynamically
       allow temporarily a user to upload a file to a precise location in your S3 bucket


  S3 Glacier Vault Lock
    adopt WORM (Write Once Read Many) model
    Create a Vault Lock Policy
    Lock the policy for future edits (no longer be changed or deleted)
    Helpful for compliance and data retention

  S3 Object Lock (versioning must be enabled)
    adopt WORM model
    Block an object deletion for a specificed amount of time
    modes:
    - Retention mode - Compliance
      Object version can't be overwritten or deleted by any user, incl. root
      Object retentin modes can't be changed, and retention period can't be shortened
    - Retention mode - Governance:
      Most users can't overwrite or delete an object vertion or alter its lock settings
      Some users have special permissions to change the retention or delete the object
    Retention Period: protect the object for a fixed period, it can be extended
    Legal Hold:
      protect the object indefinitely, independent from retention period
      can be freely placed and removed using the s3:PutObjectLegalHold IAM permission


  S3 - Access Points
    (A solution to group user/access, a layer on top of bucket (bucket policy) that can organize the access control)
    
    Access Policy simplify security management for S3 buckets
    Each access Point has:
      - its own DNS name (internet origin or VPN origin)
      - an access point policy (similar to bucket policy) - manage security at scale

    S3 - Access Points - VPC Origin
      access point only accessible from within the VPC
      must create a VPC Endpoint (Gateway or Interface Endpoint)
      the VPC Endpoint Policy must allow access to the target bucket and Access Point

      --------------------------
      |          VPC           |    Access Point
      | EC2 --- VPC Endpoint --+-->  VPC Origin  ----> S3 Bucket
      |      (Endpoint policy) |   (Access Point)   (Bucket Policy)
      |              |         |      Policy   
      ---------------+----------
                     |
          Policy to allow:
          - S3 bucket access
          - Access Endpoint access


  S3 Object Lambda
    Use AWS Lambda Functions to change the obejct before it is retrieved by the caller application
    Only one s3 bucket is needed, on top of which we can create S3 Access Point and S3 Object Lambda Access Point(s)

         [S3 Bucket]
              |
      [S3 Access Point]
              |
              |---- Lambda Function 1 ---- [S3 Object Lambda Access Pint] ---- [APP 1]
              |
              |---- Lambda Function 2 ---- [S3 Object Lambda Access Pint] ---- [APP 2]

    >> Use case:
       convert across data formats, like XML to JSON
       resizing and water-mark images on the fly using the caller-specific details




///////////////////////////////////
AWS CloudFront

  Content Delivery Netowrk (CDN)

  Improve read performance by caching the content at the Edge, improve user experience.
  216 Point of persence globally (edge location)
  DDoS protection (because worldwide), integration with Shield, AWS WAF

  Origins:
  - S3 Bucket
    Enhanced security with CloudFront Origin Access Control (OAC)
      OAC is replacing OAI (Origin Access Identity)\
    CloudFront can be used as an ingress (for files uploading to S3)
  - Custom Origin (HTTP)
    ALB (Application Load Balancer)
    EC2 instance
    S3 website
    Any HTTP backend


    ------------ CloudFront -------------
    |                                   |
                            _____ Edge-1:    clients (public)
                           /       ...         ...
      [Origin (S3 bucket)] ------ Edge-x:
               |           \_____ Edge-n:    clients (public)
               |
      {Origin Access Control}
        (S3 bucket policy)

    
  CloudFront - ALB or EC2 as origin

    EC2 origin
      The EC2 instance MUST be Public (for CloudFront to access)
    ALB origin
      The ALB MUST be public, The EC2 instance can be private (behind ALB)


  CloudFront Geo Restriction          *
    Restrict who can access the CloudFront distribution.
      AllowList: to allow users from the listed countries
      BlockList: to block users from the listed countries
    The 'country' is determind using a 3rd party Geo-IP database
    >> Use case: Copyright Laws to control access to the content      


  CloudFront - Pricing
    The cost of data out per edge location varies
    Can recude the edge locations for cost reduction

    Price Classes:
      Price Class All: all regions, best performance
      Price Class 200: most regions, excluding the most expensive regions
      Price Calss 100: only the least expensive regions


  AWS Global Accelerator
    Leverage AWX internal global network to route traffic to application.
    -- User traffic been directed (via Anycast IP/route) to the nearest Edge Location,
       then from the Edge location to then ALB then the Application throught AWX global network
    work with Elastic IP, EC2 instances, ALB, NLB, public or private.
    consistent performance
      - intelligent routing to lowest latency and fast failover
      - No issue with client side cache (IP doesn't change)
      - use internal AWS network
    Health Checks
      - Global Accelerator performs health check to the applications
      - helps make the application global (failover < 1 min ofor unhealthy)
      - greate for disaster recovery
    Security
      - only 2 external IP need to be whitelisted
      - DDoS protection (AWS Shield)


    Global Accelerator vs CloudFront
      Same part:
        - both use AWS global netowrk and its edge locations
        - both integrate with AWS shield for DDoS
      differece:
        CloudFront:
        - content cache, (content served from the edge)
        - improve performance for both cacheable content and Dynamic content 
          e.g API acceleration (cache API result) and site delivery
        Global Accelerator
        - No caching (proxy packets at the edge to application in AWS)
        - improves performance for applications over TCP/UDP
        - Good for non-HTTP use cases, e.g.  VOIP, gaming, IOT
        - Good for HTTP use cases that require static IP addresses
        - Good for HTTP use cases that require deterministic, fast regional failover



/////////////////////////////
AWS SNOW

  Highly-secure, portable devices to collect and process data at the Edge and migrate data into/out of AWS.

  AWS Snow Family: offline devices to perform data migrations, 
  if it takes more than a week to transfer over the network, use Snowball devices.

  Data migration: Snowcone, Snowball Edge, Snowmobile
    For large data migratoin
  Edge computing: Snowcone, Snowball Edge
    For the scenario where there is lack of internet access or computing power
    Can run EC2 Instances & AWS Lambda functions (using AWS IoT greengrass)


    Snowcone:   up to 24TB, online or offline (pre-installed DataSync agent)
    Snowball:   up to PB,  offline
    Snowmobile: up to EB,  offline


    Snowball Edge (for data transfer)
      Physical data transprt solution.    (large)
      Pay per data transfer job
      
      Storage:
      - Snowball Edge Storage Optimized: 
        40 vCPUs, 80G mem 80T HDD 
        104 vCPUs, 416G mem, 210T NVMe for block volume and S3 compatible object storage
      - Snowball Edge Compute Optimized: 
        104 vCPUs, 416G mem
        42T HDD or 28T NVMe for block volumn and S3 compatible object storage
      Storage Clustering available (up to 16 nodes)
      >> Use case:
        large data cloud migrations
        DC decommision
        disaster recovery

    Snowcone & Snowcone SSD
      Device for edge computing, storage and data transfer.      (Small)
      Small, portable computing, anywhere, rugged & secure, withstands harsh environments

      2 CPUs, 4G mem, USB-C power or optinal battery
      - Snowcone: 8T HDD
      - Snowcone SSD: 14T SSD
      Must provide your own battery/Cables
      Can sent back to AWS offline, or connect to internet and use AWS DataSync to send data

      >> Use cases:
        space contrained environment where Snowball does not fit.

    Snowmobile
      A Truck
      Transfer exabytes of data (1EB = 1000PB = 1000000TB)

  Process:
    Request snowball device
    Install Snowball client / AWS OpsHub on your servers
    Connect snowball and copy files
    Shipback the device


  AWS OpsHub
    a software with UI for Snow Family


  Snowball into Glacier
    Snowball can NOT import to Glacier directly
    It can go to S3 first, then use S3 Lifecycle Policy to transfer into Glacier




//////////////////////////////////////
Amazon FSx

    Lauch 3rd party high-performance file system on AWS
    Fully managed service
      FSx for Lustre, FSx for NetApp ONTAP, FSx for Windows File Server, FSx for OpenZFS
    

  FSx for Windows
    Fully managed Windows FS share drive
    Support SMB & NTFS
    Microsoft Active Directory integration, ACL, user quotas
    Can be mounted on Linux EC2 instances
    Support Microsoft DFS (Distributed File System) Namespaces -- group files across multiple FS
    Can be accessed from on-premises infrastructure (VPN or Direct Connect)
    Can be configured to be Multi-AZ (HA)
    Data is backed-up daily to S3
    Scale up to 10s of GB/s,  millions of IOPS,  100s of PB of data

  FSx for Lustre
    Lustre -- (Linux-Cluster)
    Lustre is a parallel distrubuted FS, for large-scale computing
    for Machine Learning, HPC (High Performance Computing)
        Video Processing, Financial Modeling, Electronic Design automation
    Seamless integration with S3
      - Can read S3 as file system (through FSx)
      - Can write the output of the computations back to S3 (through FSx)
    Can be used from on-premises servers (VPN or Direct Connect)
    Scales up to 100s GB/s, millions of IOPS, sub-ms latencies

  FSx File System Deployment Options
    Scratch File System
      - Temp storage
      - Data is not replicated
      - High burst (6x Faster, 200MBps per TB)
      - Usage:  short-tem processing, optimize costs
    Persistent File System
      - Long-term storage
      - Data is replicated within same AZ
      - Replace failed files within minutes
      - Usage: long-term processing, sensitive data

  FSx for NetApp ONTAP
    Manged
    FS compatible with NFS, SMB, iSCSI
    Move workloads running on ONTAP or NAS to AWS
    Work with: Linux, Windows, MacOS, VMware Cloud on AWS, Amazon Workspaces & AppStream 2.0, Amazon EC2, ECS and EKS
    Storage shrinks or grows automatically
    Snapshots, replication, low-cost, compression and data de-duplication
    Point-in-time instantaneous cloning (helpful for testing new workloads)

  FSx for OpenZFS
    Compatible with NFS (v3,v4,v4.1,v4.2)
    Move workloads running on ZFS to AWS
    Work with: Linux, Windows, MacOS, VMware Cloud on AWS, Amazon Workspaces & AppStream 2.0, Amazon EC2, ECS and EKS
    Up to 1,000,000 IOPS with < 0.5ms latency
    Snapshots, compression and low-cost
    Point-in-time instantaneous cloning (helpful for testing new workloads)


  Storage Gateway

    Hybrid Cloud for Storage

      AWS Storage Cloud Native Options
        Block:  EBS, EC2 Instance Store.
        File:   EFS, FSx
        Object: S3, Glacier

    Bridge Between on-premises and cloud
      Type of Storage Gateway
        - S3 File Gateway
        - FSx File Gateway
        - Volumn Gateway
        - Tape Gatewy
    >> Use case
       disaster recovery
       backup & restore
       tiered storage
       on-premises cache & low-latency file access

    S3 File Gateway
      
                  on-premises                     |       AWS Cloud
      App-server (NFS/SMB) ---[S3 File Gatewy] ---(HTTPS)--- S3 -- Glacier

      Config S3 bucket to be accessible using NFS and SMB
      Most recently used data is cached in the file Gateway
      support different types of S3 (Standard, standard IA, one-zone IA, Intelligent Tiering)
      Transition to S3 Glacier using a lifecycle policy
      Bucket access using IAM roles for each FIle Gateway
      SMB has integration with AD (Active Directory) for user authentication

    FSx File Gateway
      Native access for FSx for Windows Server
      Local Cache for frequent accessed data
      Windows compatibility (SMB, AD, NTFS)
      Useful for group file shares and home directory

    Volume Gateway
      Block storage using iSCSI backed by S3
      Backed by EBS snapshot which can help restore on-premises volumes
      Cached volumes: low latency access to most recent data
      Stored volume: entire dataset is on premise, scheduled backups to S3

                   On-premises                        |              AWS
        App Server ---iSCSI ---[Volume Gateway] ---(HTTPS)--- S3  --- EBS snapshot

    Tage Gateway
      backup using physical tapes
      VTL (Virtual Tape Library) backed by S3 / Glacier
      Backup data using existing tape-based processes (and iSCSI interafce)
      Works with leading backup software vendors

                     On-premises                       |              AWS
        Backup Server ---iSCSI ---[Tape Gateway] ---(HTTPS)--- S3 Virtual Tape Store --- Glacier

    Storage Gateway - Hardware Appliance
      Using Storage Gateway means you need on-premises virtualization
      if no hardware, you can use AWS Storage Gateway Hardware Appliance
      work with File Gateway, Volumn Gateway, Tape Gateway
      has the requird CPU, memory, network, SSD cache ...


    Storage Gateway Overview
      Gateway Deployment Options:
        - VM (VMware, KVM, Hyper-V)
        - Hardware Appliance

      
  AWS Transer Family
    A fully managed service to transfer data in/out AWS S3 or EFS via FTP.
    Suport:  FTP, FTPS (FTP over SSL), SFTP
    Managed infrastructure:  Scalable, Reliable, HA (Multi-AZ)
    Pay per provisioned endpoint per hour + data transfers in GB
    Store and manage users' credential within the service
    Intergrate with existing authentication system (Microsoft AD, LDAP, Okta, Amzaon Cognito, custom)
    >> Usage case:  
       Sharing files,  Public datasets,  CRM,  ERP

    User (FTP client) --- (Router53) --- [AWS Transfer ] ---- IAM Role --- AWS S3 | EFS
   

  AWS DataSync
    Move large amount of data to / from places:
      On-premises/other cloud to AWS (NFS,SMB, HDFS, S3) -- needs agent
      AWS to AWS (no need agent)
    Can synchronize to :  S3/Glacier,  EFS,  FSx
    Replication tasks can be scheduled hourly/daily/weekly
    File permissions and metadata are preserved (NFS POSIX SMB)         **
    One agent task can use 10 Gbp, can setup bandwidth limit
      
    
      NFS/SMB to AWS: 
      NFS/SMB Server  ---(NFS/SMB)--- [AWS DataSync Agent] ----(TLS)---- [AWS DataSync] ---- AWS Storage Resource
                                         [AWS Snowcone]  | --------------|                   (S3, EFS, FSx,)
                                      (agent preinstalled)

      AWS DataSync
      S3, EFS, FSx -----[AWS DataSync]----- S3, EFS, FSx
                        (copy metadata)


  AWS Transfer Family vs DataSync
    Transfer Family:
      Purpose: Transfer files into and out of Amazon S3 using SFTP, FTPS, and FTP protocols.
      Support protocol:  FTP, FTPS, FSTP
      Use case: replacing traditional file trnsfer workflows with cloudbased solution
    DataSync:
      Purpose: Move large amounts of data online between on-premises storage and AWS services.
      Supported protocol: DataSync's own optimized protocol
      Use case: Data migration, data processing, and regular data backup to AWS
                Transferring data between NFS server, SMB file shares and AWS storage servcies.



  Storage Comparison
    S3:               Object storage
    S3 Glacier:       Object Archival
    EBS Volume:       Network Storage for one EC2 instance at a time.  (Multi-attach to io1/io2 volume)
    Instance Store:   Physical storage for EC2 instance (High IOPS)
    EFS:              Network File System for Linux instance, POSIX filesystem
    FSx for windows:  Network File System for Windows Servers
    FSx for Lustre:   High Performance Computing Linux File System
    FSx for NetApp ONTAP: High OS Compatibility
    FSx for OpenZFS:  Managed ZFS file system
    Storage Gateway:  S3/FSx File Gateway, Volume Gateway (cache & store), Tape Gateway
    Transfer Family:  FTP, FTPS, SFTP interface on top of S3/EFS
    DataSync:         Scheduled data sync from on-premises->AWS or AWS->AWS
    Snowcone/Snowball/Snowmobile: to move large amount of data to cloud, physically.  (Snowcore with datasync agent)
    Database:         for specific workloads, usually with indexing and querying





/////////////////////////////////
AWS Messaging Services  (DeCoupling applications: SQS, SNS, Kinesis, Active MQ)

  Application communication pattern:  Synchronous / Asynchronous
  
  AWS Servcies:
    SQS:     Queue modlel
    SNS:     pub/sub model
    Kinesis: real-time streaming model
  There servcies can scale independently from applications


  Amazon SQS
    Queue
    Oldest offering (> 10 years)
    Fully managed servcie, used to decouple applications. 
      e.g. auto scale by default

    Producer(s) ---(message)---> SQS Queue ---(poll message)---> Consumer

    Attributes:
      - Unlimited throughput, Unlimited number of messages in Queue
      - Default retention: 4 days, max 14 days
      - Low latency (<10ms on publish & receive)
      - Limitation of 256KB per message sent
    Can have duplicate messages (at least one delivery, occasionally -- a message may be delivered twice)
    Can have out of order message (best effort ordering)

    
    Producing messages:
      Produce to SQS using the SDK (SendMEssage API)
      The message is persisted in SQS until a consumer deletes it.
      Retention: <=14 days. default 4
      Unimited throughput.

    Consuming messages:
      Consumer running on EC2 instances, servcers, or AWS Lambda
      Poll SQS for messages (receive up to 10 messages at a time)
      Multiple consumers can receive / process messages in parallel
      at least once delivery
      best-effort ordering
      Consumer delete messages after processing
      Can scale consumers horizontally to improve throughput of processing

    SQS with Auto Scaling Group (ASG)
      Scale based on CloudWatch Metric -- Queue Length (ApproximateNumberOfMessages)


    SQS to decouple between application Tiers
    requests --> [Front-end web Apps] ---(SendMessage)---> SQS Queue ---(ReceiveMessage)---> [Backend Apps]
                        x M                                                                      x N (Auto Scaling)

    SQS - Sequrity
      Encryption:
        In-flight encryption using HTTPS API
        At-rest encryption using KMS keys
        Client-side encryption if the client wants to perform encryption/decryption itself

    SQS - Message Visibility Timeout
      After a message is polled by a consumer, it becomes invisible to other consumers
      by default 30 sec
      means the message has 30 sec to be processed
      after the message visibility timeout is over, the message is 'visible' in SQS.
      if a message is not processed winthin the visibility timeout, it will be processed twice.
        -- consumer can call 'ChangeMessageVisibility' API to get more time for processing
      if visibility timeout is high (hours) and the consumer crashes, it will take long time to re-processing
      if visibility timeout is too low, we may get duplicates

    SQS - Long Polling
      Long Poll: when a consumer requests messages from a queue,
                 it can optionally 'wait' for message to arrive if there are none in the queue
      decreases the number of API calls to SQS -> increase the efficiency and latency of the app.
      he wait time 1~20 sec
      Long polling is preferable to short polling
      Long polling can be enabled at the queue level, or at the API level using WaitTimeSeconds

    SQS - FIFO Queue 
     Limited throughput 300 msg/s without batching, 3000 msg/s with batching
     Exactly-once send capability (by removing duplicates)
     Consumer recevie order is guarenteed

     a FIFO queue is named with the end of '.fifo'
     'content-based duplication' -- 

    SQS with ASG
      use SQS with Auto Scaling Group (for consyners)

      if the load is too big, some transactions may be lost. to solve it, use SQS as a buffer.

      Request(s) --- Enqueue ---(SendMessage)--- [SQS] ---(ReceiveMessge)--- Dequeue
                      (ASG)                                                   (ASG)


    >> Use case                                            **
      You have configued a Lambda function to run each time an item is added to a DynamoDB table using DynamoDB Streams.
      The function is meant to insert messagte into the SQS queue for further long processing job.
      Each time the Lambda function is invoked and able to read from the DynamoDB Stream,
      but it isn't able to insert the message into the SQS Queue.
      What is the problem?
        a. The SQS Security group must be edited to allow AWS Lambda
        b. The Lambda security group must allow outbound access to SQS
        c. The Lambda Execution IAM role is missing permissions.
      explain:
      - Security Group cannot be attached to SQS queues
      - Security Group cannot poiont to SQS queues
      so c is correct


  Amazon SNS
    Simple Notification Service   (Pub/Sub)
    -- sned one message to many receivers

      Event producer ---- [SNS topic] ----> subscriber(s) x N (max 12.5M)

    The event producer only send event to one SNS topic
    The event receiver (subscribers) listen to the SNS topic
    each subscriber to the topic will get all the messages (new feature to filter messages)
    Up to 12.5 million subscriptions per topic
    Topic limit: 100000  (0.1M)
    Subscriber types:  email, SMS, HTTP/HTTPS endpoints, SQS, Lambda, Kinesis Data Firehose

    Many AWS servcies can send messages directly to SNS for notification

    SNS - How to publish
      Topic Publish (using SDK)
        - create topic
        - create subscription(s)
        - publish to topic
      Direct Publish (for mobile apps SDK)
        - create a platform application
        - create a platform endpoint
        - publish to the platform endpoint
        - works with Google GCM, Apple APNS, Amazon ADM

    SNS - Security
      Encryption:
        - in-flight encryption using HTTPS API
        - at-rest encryption using KMS keys
        - client-side encryption 
      Access Controls:  
        IAM policies to regulate access to the SNS API
      SNS Access Policies 
        - Useful for cross-account access to SNS topics
        - Useful for allowing other services (S3, ..) to write to an SNS topic

    SNS - FIFO Topic
      features:
        - Ordering by Message Group ID (all messages in the same group are ordered)
        - Decuplication using Deduplication ID or Content Based Dedeplication
      Can have SQS Standard and FIFO queue as subscribers
      Limited throughput (same as SQS FIFO)


  SNS + SQS: Fan Out Pattern
    Push once in SNS, receive in all SQS queues that are subscribers
    Fully decoupled, no data loss
    SQS allows data persistence, delayed processing and retries
    Ability to add more SQS subscribers over time
    Make sure your SQS queue access policy allows for SNS to write
    Cross-region Delivery: works with SQS Queues in other regions


    event(s) ---> SNS ---(xN) subscriber --- SQS --- service-1
                                 ...
                              subscriber --- SQS --- service-n

    >> Use case:
       Application: S3 Events to multiple queues
         For the same combination of: Event type (e.g. object create) and prefix (e.g. images/) you can only have One S3 Event rule
         Id you want to send the same S3 evnet to multiple recievers, use SNS -> SQS Fan Out pattern
                                          - SNS
                                         /
           S3 --- (event) ---> SNS Topic -- SNS
                                         \
                                          - SNS

       Application: SNS to S3 through KDF (Kinesis Data Firehose)
         SNS can send to Kinesis

         Service --- SNS Topic ---> Kinesis Data Firehose ---> S3
                                                          \--> (Any KDF supported Destination)


    SNS FIFO + SQS FIFO: Fan Out
      In case you need:  Fan Out + Ordering + Deduplication

        Servcie --- SNS FIFO Topic --- SNS FIFO Queue ---> consumer
                                    \- SNS FIFO Queue ---> consumer


    SNS - Message Filtering
      JSON Policy used to filter messages snet to SNS topic's subscribers
      if a subscriber doesn't have a filter policy, it receives every message



  Kinesis
    Collect, process, analyze streaming data in real-time
    Ingest real-time data (e.g. logs, metrics, website clickstreams, IoT telemetry)
    Kinesis services:
      Kinesis Data Streams: capture, process and store data stream
      Kinesis Data Firehose: load data stream into AWS data stores
      Kinesis Data Analysis: analyze data stream with SQL or Apache Flink
      Kinesis Video Streams: capture, process, and store video streams    

    Kinesis Data Streams:
      Shard(s) x n
      Retention 1~365 days
      Ability to reprocess (replay) data
      Immutability: once the data is inserted into Kinesis, it cannot be deleted
      Shard per partition key. (ordering)
      Producers: AWS SDK, KPL (Kinesis Producer Library), Kinesis Agent
      Consumers: Write your own, KCL (Kinesis Client Library), AWS SDK
                 Managed: AWS Lambda, Kinesis Data Firehose, Kinesis Data Analytics

        Producers -----(records)-----> Kinesis Data Streams -----(Record)-----> Consumer 
      (App, clients)                       Shard x n                            Apps (KCL,SDK)
                     Partition Key                            Partiction Key    Lambda
                     Data Blob (<=1M)                         Sequence number   Kinesis Data Firehose
                                                              Data Blob         Kinesis Data Analytics
      
      >> The capacity limits of a Kinesis data stream are defined by the number of shards within the data stream.
         The limits can be exceeded by either data throughput or the number of reading data calls. 
         Each shard allows for 1 MB/s incoming data and 2 MB/s outgoing data. 
         You should increase the number of shards within your data stream to provide enough capacity.

    Kinesis Data Stream - Capacity Modes
      Provisioned mode: (manual)
        Customer choose the number of shards to provision, scale manually or using API
        Each shard get 1MB/s in (or 1000 records per sec)\
        Each shard get 2MB/s out (classic or enhanded fan-out consumer)
        customer pay per shard provisioned per hour
      On-demand mode: (auto)
        No need to provision the capacity
        Default capacity provisioned (4MB/s or 4000 records per second)
        Scale automatically on observed throughpt peak during last 30 days.
        Pay per stream per hour & data in/out per GB

    Kinesis Data Streams Security
      Control access / authorization using IAM policy
      Encryption in flight using HTTPS endpoint
      Encryption at rest using KMS
      Can implement encryption/decryption on client side
      VPC endpoint available for Kinesis to access within VPC
      Monitor API calls using CloudTrail

    
    Kinesis Data Firehose
      Fully managed servcie, auto scaling, Serverless
        dest: AWS Redshift/S3/OpenSearch
              3rd-party partner: Splunk/MongoDB/DatatDog/NewRelic/..
              Custom: to any HTTP endpoint
      pay for data going through Firehose
      Near real-time
        buffer interval: 0~900 sec
        buffer size: >= 1MB
      Support many data format, convesion, transformation, compression
      Support custom data transfromation using AWS Lambda
      Can send failed or all data to a backup S3 bucket
      
                            Lambda Function (optional)
                                         |
      Producers ---(Record)--- [Kinesis Data Firehose] ---(batch write)---> AWS Destination (S3, Redshift, OpenSearch)
                                         |                                  3rd-party Partner Destination
                                         |                                  CUstom Destinatino (HTTP Endpoint)
                          all or failed data --> S3 backup bucket             


    Kinesis Data Stream vs Kinesis Data Firehose
      Stream: process data in real-time.  A customized service.
      Hose:   load data into AWS/3rd-party for future analysis. A managed service

                Kinesis Data Stream                       Kinesis Data Firehose
      Usage:  Streaimng service for ingest at scale     Load streaming data into AWS/3rd-party
      Manage: Custom code (producer/consumer)           Fully managed
      Realtime: real-time (~200 ms)                     Near real-time
      Scaling: manage scaling (shard)                   Auto scaling
      Store:   1~365 days                               No data storage
      Replay:  support replay capability                No suppoort of replay


    Kinesis Data Analytics    




    Ordering data into Kinesis
      -- Partition Key
      the same key will always go to the same shard

    Ordering data into SQS
      For SQS Standard, there is no ordering
      For SQS FIFO, if not using Group ID, emssages are consumed in the order they are sent.
      Using GroupID, the consumer can read by Group
 
    Kinesis vs SQS ordering
      say 100 trucks (partition or group ID), 5 kinesis shards, 1 SQS FIFO
      Kinesis Data Streams:
        20 trucks per shard, ordered in each shard
        max consumers in parallel: 5
        receive up to 5 MB/s of data
      SQS FIFO:
        one FIFO queue
        100 group ID
        up to 100 consumers
        up to 300 message/sec (or 3000/s if using batching)


  SQS vs SNS vs Kinesis          *
    SQS:     
             Pull data
             any number of workers (consumers) 
             data deleted after being consumed (retention 1~14 days)
             no need to provision throughput
             order gurarantees only on FIFO queues
    SNS:     
             Push data, Pub/sub
             up to 12.5 million subscribers, up to 100k topics
             data not persisted (lost if not delivered)
             no need to provision throughput
             FIFO cpability for SQS FIFO
             1:M, integrates with SQS for fan-out architecture pattern
    Kinesis: 
             Pull (standard) or Push (Enhanced-fan out) data
             up to 20 consumers per stream (enhanced fan-out limit)
             Possibility to repay data (retention 1~365 days)
             both 'provisioned mode' or 'on-demand capacity' mode
             order at the shard level
             for real-time big data, analytics and ETL


  Amazon MQ
    Amazon MQ is a managed message broker service for RabbitMQ, ActiveMQ
      SQS, SNS are cloud-native servcies, prioprietary from AWS
      traditional applications from on-premises may use open protocols like MQT, AMQP, STOMP, Openwire, WSS
      Amazon MQ helps to adapt open MQ services to Amazon cloud
    
    Amazon MQ doesn't scale as much as SQS/SNS
    Amazon MQ runs on servers, can run in Multi-AZ with failover
    Amazon MQ has both Queue (like SQS) and Topic feature (like SNS)

    Amazon MQ - HA
      Amazon MQ brokers in active and standby AZs
                Zone 1                     Zone 2
                Active                     Standby
              MQ Broker                   MQ Broker
                  |                           |
                  ----------- EFS -------------




//////////////////////////////////
AWS Containers (ECS, Fargate, ECR)

  ECS:  Elastic Container Service
  ECR:  Elastic Container Repository
  EKS:  Elastic Kubernetes Service
  Fargate: Amazon's Serverless container platform, works with ECS and EKS


  Amazon ECS -Launch Type
    EC2 Lauch Type 
      Lauch Docker containers on AWS == Launch ECS Tasks on ECS clusters
      Must provision & maintain the infrastructure (EC2 instances)
      Each EC2 instance must run the ECS Agent to regiester in the ECS Cluster
      AWS takes care of the starting/stopping containers
    Fargate Lauch Type
      Serverless
      No provision the infrastructure (No EC2 instances to manage)
      AWS just runs ECS tasks for you based on CPU/RAM requirement
      To scale, just increase the number of the tasks.

  ECS - IAM Roles for ECS
    EC2 Instance Profile (EC2 lauch type only)
      Used by ECS agent
      ECS agent make API calls to ECS service
      Send container logs to CloudWatch logs
      Pull docker image from ECR
      Reference sensitive data in Secrets Manager or SSM parameter store.
    ECS Task Role:
      Allows each task to have a specific role
      Use different roles for different ECS servcies
      Task role is defined in task definition

  ECS - Load Balancer integration
    ALB, for most use cases
    NLB, for high throughput/performance
                                          ECS Cluster
    users ---(http/https)---ALB ---- ECS Tasks on ECS Instances
                                \--- ECS Tasks on ECS Instances

  ECS - Data Volumes (EFS)
    Mount EFS onto ECS tasks.    (S3 cannot be mounted as a file system)
    Work for both EC2 and Fargate lauch types
    Task running in any AZ share the same data on EFS
    Fargate + EFS = Serverless    (both Fargate and EFS are serverless)
    >> Use case:
      persistent multi-AZ shared storage for containers


  ECS -- Service Auto Scaling
    Auto increase/decrease ECS tasks
    ECS Auto Scaling uses AWS Application Auto Scaling
      by Metrics:
        - Average CPU Utilization
        - Average Memory Utilization
        - ALB Request Count per target
    
    Scale based on:
      Target Tracking -- target value for a specific CloudWatch metrics
      Step Scaling    -- a specified CloudWatch Alarm
      Scheduled Scaling -- based on date/time

    ECS Servcie Auto Scaling (task level) != EC2 Auto Scaloing (EC2 instance level)
    Fargat Auto Scaling is much easier to setup.

  EC2 Lauch Type - Auto Scaling EC2 Instances
    Approaches:
      - Accommodate ECS Services Scaling by adding underlying EC2 Instances
      - ASG (Auto Scaling Group) Scaling
        Scale ASG based on CPU Utilization
        Add EC2 instances over time
      - ECS Cluster Capacity Provider
        Auto provision and scale the infrastructure for ECS tasks
        Capacity Provider paired with ASG
        Add EC2 instances when missing capacity (CPU, RAM,...)


 >> Use cases
    ESC tasks invoked by Event Bridge
                                                    [AWS Fargate]
      Client ---> S3 ---(event)---> EventBridge ---(Rule: ECS Task)---> ECS task
                  ^                                        | |
                  ----------------(get object)-------------| |---(save result)---> DynamoDB


    ECS tasks invoked by Event Bridge Schedule
                                                         [AWS Fargate]
      [Event Bridge] ---(scheduled rule: Run ECS task)---> ECS Tasks ---(batch processing)---> S3

    ECS - SQS Queue
                                           [ECS] 
                                      |--> Task1
       Messages ---> SQS ---(poll msg)---> ...
                                      |--> Taskn
                                  (ECS Service Auto Scaling)


  >> Use case
     You have an application hosted on an ECS Cluster (EC2 Launch Type) where you want your
     ECS tasks to upload files to an S3 bucket. Which IAM Role for your ECS Tasks should you modify?
     --> ECS Task Role
  >> Use case
     You are deploying an application on an ECS Cluster made of EC2 instances.
     Currently, the cluster is hosting one application that is issuing API calls to DynamoDB successfully.
     Upon adding a second application, which issues API calls to S3, you are getting authorization issues.
     What should you do to resolve the problem and ensure proper security?
     --> Create an IAM task role for the new application
        other inproper solutions
        > Edit EC2 instance role to add permission to S3 -- would allow the 1st app to access S3 as well (security risk)
        > Edit S3 bucket policy to allow the EC2 task -- would allow the 1st app to access S3 as well
  

  ECR (Elastic Container Registry)
    Store and manage DOcker images on AWS
    Public and Private repo. 
      Amazon ECR (Private)
      Amazon ECR Public    (Public Gallery https://gallery.ecr.aws)
    Fully integrated with ECS, backed by S3
    Access controlled through IAM.  (permission errors means policy issue)
    Supports image vulnerabiity scan, versioning, tags, lifecycle



  Amazon EKS
    Elastic Kubernetes Service
    managed Kubernetes Clusters on AWS
    EKS supports 2 modes:
      EC2 (deply worker modes)
      Fargat (deploy serverless containers)
    K8s is cloud-agnostic. 
    >> Use case:
       migrate company's on-premises K8s to AWS.

    
    EKS - Node types
      Managed Node Groups
        - EKS manage nodes (EC2 instance) for you
        - Nodes are part of an ASG managed by EKS
        - Support on-demand or spot instances
      Self-managed nodes
        - self created node and registered to the EKS cluster and managed by an ASG
        - can use prebuilt AMI - Amazon EKS Optimized AMI
        - Support on-demand or spot instances
      AWS Fargate 
        - No maintenace required; no nodes managed

    EKS - Data Volumes
      Need to specify StorageClass manifest on EKS cluster
      Leverages CSI (Container Storage Interface) compliant driver
      Support:
        EBS
        EFS (works with Fargate)
        FSx for Lustre
        FSx for NetApp ONTAP


  AWS App Runner
    Fully manged servcie to deploy web application and APIs at scale
    No infrastructure exprience required
    Start with your source code or container image
        source code or docker image ---> Configure Settings (vCPU, RAM, ASG, Health check) ---> Create & Deploy.
    Automaticlly builds and deploy the web app
    Auto scaling, HA, LB, encryption
    VPC access support
    connect to database, cache, message queue
    >> Use cases
       web apps,  APIs,  microservices,  repid production deployments




//////////////////////////////////////////
AWS Serverless
  'serverless' --  developers don't have to manage servers
                   instead they just depoly code, functions
  
  initially, serverless == FaaS (Function as a Service) -- AWS Lambda
  now also includes anything that's managed:  DBs, messaging, storage, etc.

  Related components: 
    S3, API Gateway, Cognito, Lambda, DynamoDB, SNS & SQS, Kinesis Data Firehose, Aurora Serverless, Step Functions, Fargate


  AWS Lambda
    Key features:
      - Virtual Functions, no servers
      - Limited by time - short executions
      - Run on-demand   (up to 15 min)
      - Scaling is Automated
    Benefits:
      - easy Pricing;  very cheep to run AWS lambda
        per request and compute time
        free tier of 1M AWS Lambda requests and 400kGBs of compute time
      - Integrated with the whole AWS suite of services
      - Integrated with many programming languages
      - Easy Mornitoring through AWS CloudWatch
      - Easy to get more resources per functions (up to 10G RAm)
      - Increase RAM will also improve CPU and Networking

    AWS Lambda language support:
      Node.js, Python, Java, C#, Golang, Ruby, Custom Runtime API (e.g. Rust)

    Lambda Container image
      The container image must implement the Lambda Runtime API
      ECS/Fargate is preferred for running arbitray docker images


    Lambda integrations
      API Gateway, Kinesis, DynamoDB, S3, CloudFront, CloudWatch Events Bridge, CloudWatch log, SNS, SQS, Cognito

    >> Use case:
       Serverless CRON:
         CloudWatch EventBridge ---(Trigger per hour)---> Lambda Function perform a task


  AWS Lambda Limits:
    Execution:
      - Memory allocation: 128MB~10GB
      - Max execution time: 900s (15 min)         *
      - Environment variables (4KB)
      - Disk capacity in the 'function container' (in /tmp): 512MB~10GB
      - Concurrency executions: 1000 (can be increased)
    Deployment:
      - deployment size (compressed zip): 50MB
      - deployment size (non-compressed code+dependencies): 250MB
      - Can use /tmp to load other files at startup
      - environment variable size: 4KB


  Lambda SnapStart
    Improves Lambda functinos performance for Java 11+ up to x10 with no extra cost
    Functions invoked from 'pre-initialized' state if SnapStart enabled
    When publishing a new lambda function, 
      Lambda initializes your function
      Take a snapshot of memory and disk of the initialized function
      Snapshot cached for low-latency access


  Lambda & Customaztion at the Edge
    Many applications execute some form of logic at the edge

    Edge Function:
      attach to CloudFront Distributions
      run close to users to minimize latency
    
    CloudFront function types:
      - CloudFront Function
      - Lambda@Edge
      No management of servers, deploy globally   (

    Fully serverless
    >> Use case: Customzed CDN content

    CloudFront Functions:
      light weight function written in JavaScripts
      Used to change viewer request and response. (Not triggering origin request/resonse)
      For high-scale, latency-sensitive CDN customizations
      Native feature of CloudFront (manage code entirely within CloudFront)

      Client --(viewer request)--> CloudFront --(origin request)--> Origin
             <--(viewer response)--           <--(origin response)--
      
    Lambda@Edge
      Lambda functions written in NodeJS or Python 
      Used to change CloudFront requests and response
        (change the viewer request/resonse and origin request/response)
      Scale up to 1000s of requests/second
      Author you furnction in ONE AWS Region, then CloudFront replicates to its location

    CloudFront Function vs Lambda@Edge
                             CloudFront Function            Lambda@Edge
      Runtime Support:       JavaScript                     Node.js or Python
      num of request:        Millions RPS                   Thousands RPS
      CloudFront trigger:    Viewer req/resp                Viewer and Origin Req/Rsp
      Max execution time:    1 ms                           5~10 s 
      Max memory:            2 MB                           128K~10GB
      Total package size:    10 KB                          1~50 MB
      Network/FS access:     No                             Yes
      Access to request body: No                            Yes
      Pricing:               Free tier available,           No free tier, 
                             1/6 of lambda@edge             charge per erquest & duration

    >> Use Case:
       CloudFront:
         Cache key normalization (transfrom reqeust attributes to create an optimal cache key)
         Header manipulation (insert/modify/delete) HTTP headers in the request / response
         URL rewrite / redirect
         Request authentication & authorization
           - create/validate user token (e.g JWT) to allow/deny requests

       Lambda@Edge:
         Longer execution time
         Adjustable CPU or Memory
         when code depends on 3rd lib (e.g. AWS SDK to access other AWS service)
         Network access to use external services
         File system access
         Body of HTTP Request access


  Lambda in VPC
    Lambda by default -- outside VPC:
      launched outside user VPC (in AWS-owned VPC)
      so cannot access resource in user VPC (RDS, ElasticCache, internal ELB, etc)
    
    Lambda in VPC
      must define VPC ID, subnets and security groups
      lambda will create an ENI (Elastic Network Identifier) in user VPC subnet (private)

    Lambda with RDS proxy
      RDS proxy: 
        improve scalability by pooling and sharding DB connection
        improve availability 
        improve security by enforcing IAM authentication and storing credentials in secret manager
      The Lambda function must be deployed in VPC (RDS Proxy is NEVER publicly accessible)

    Invoking Lambda from RDS & Aurora
      Invoke Lambda function from DB instance
      allows you to process data events within a database
      Must allow outbound traffic to your Lambda function from within your DB instance (Public, NAT GW, VPC Endpoint)
      DB instance must have the required permission to invoke the Lambda function (IAM policy)

      RDS Event notifications
        Notifications that tells information about the DB instance itself.
        （only notification, not any information about the data itself)
        Subscribe to event categories:
          DB instance, DB snapshot, DB paramenter Group, DB Security Group, RDS PRoxy, Custom Engine Version
        Near real-time events (up to 5 min)
        Send notifications to SNS or subscribe to events using EventBridge
        
                            |-- Lambda
        RDS DB ---- SNS ------ SQS
                 |
                 |- EventBridge --- Lambda 


  Amazon DynamoDB
    FUlly managed, HA across multi-AZs
    NoSQL with transaction support
    Scale to massive workloads, distributed database
    Millions of RPS,  Trillions of rows, 100s of TB storage
    Fast and consistent in performance (ms)
    Integrated with IAM 
    Low cost and auto-scaling
    No maintenance or patching, always awailable
    Standard & IA (Infrequent Access) Table Class

    DynamoDB - basic
      DynamoDB is made of Tables
      Each table has a primary Key (must decide at creation time)
      Each table has infinit number of items (rows)
      Each item (row) has attibutes (can be added over time, can be null)
      Max item size: 400kBs         *
      Supported data type:
        - Scalar Type: (string, number, binary, bool, null)
        - Document Type: List, map
        - Set Type: String set, Number set, Binary set
      in DynamoDB, you can rapidly evolve schemas           **
      
      Primary Key:  Partition Key + Sort Key （optional）

      Read/Write Capacity Modes:
      - Provisioned Mode (default)
        Specify the number of reads/write per sec
        plan capacity before hand
        Pay for provisioned Read Capacity Units (RCU) & Write Capacity Units (WCU)
        Possibility to add auto-scaing mode for RCU & WCU
        RCU and WCU are decoupled, so you can increase/decrease each value separately.
      - On-Demand Mode
        Read/write automaticlly scale up/down with workloads
        No capacity planning needed
        Pay for what you use, more expensive
        Great for unpredictable workloads, steep sudden spikes

    DynamoDB Accelerator (DAX)
      Fully managed, HA, seamless in-memory cache for DynamoDB
      solve read congestion by caching
      ms latency for cached data
      no require application logic (compatible with DynamoDB API)
      TTL defaut 5min (configurable)

    DynamoDB Accelerator (DAX) vs ElasticCache
      ElasticCache:  Store Aggregation result
      DAX:           Individual Objects cache;  Query & Scan cache

      DynamoDB Accelerator delivers up to 10x performance improvement. thus offloading the heavy reads on hot keys off your DynamoDB table,
        hence preventing the "ProvisionedThroughputExceededException" exception. 

    DnyamoDB - Stream Processing
      Oredered stream of item-level modification (CRUD) in table
      Use cases:
        React to changes in realtime (welcome email to users)
        Real-time usage analytics
        Insert into derivative Tables
        Implement cross-region replication
        Invoke Lambada on changes to DynamoDB table

        DynamoDB Streams            vs         Kinesis Data Stream (newer)
      ----------------------------------------------------------------------
      24 hour retension                        1 Year retension
      Limited # of consumers                   High # of consumers 
      Process using AWS Lambda Triggers        Process using AWS Lambda, Kinesis Data Analytics, Firehose, Glue Streaming ETL...
        or DynamoDB Stream Kinesis Adapter

    DynamoDB - Global Table
      Make a DynamoDB table accessible with Low Latency in multiple-Regions
      Active-Active repliction (2-way replication between multiple regions)
      Application can read/write table in any region
      Must enable DynamoDB Streams as a pre-requisite      *
       -> DynamoDB Streams enable DynamoDB to get a changelog and use it to replicate data across replica tables in other AWS Regions.


    DynamoDB - TTL (Time To Live)
      Auto delete item after an expiry timestamp
      >> Use case:
         reduce stored data by keeping only the current items, 
         adhere to regulatory obligations,
         web session handling

    DynamoDB - Backup for disaster recovery
      Condintuous backups using point-in-time recovery (PITR)
        optionally enabled for the last 35 days
        point-in-time recovery at any time within the backup window
        the recovery process create a new table
      On-demand backups
        Fully backup for long-term retention, until explicitely delete
        Doesn't affect performance or latency
        Can be configured and managed in AWS Backup (enable cross-region copy)
        The recovery process creates a new table

    DynamoDB - Integration with S3
      Export to S3 (must enable PITR)
        works for any point of time in the last 35 days
        doesn't affect the read capacity
        perform data analysis on top of DynamoDB
        Retain snapshots for auditing
        ETL (extract, transform, load) on top of S3 data before importing back into DynamoDB
        export in DynamoDB JSON or ION format
      Import from S3
        Import CSV, DynamoDB JSON or ION format
        Doesn't consume any write capacity
        create new table
        import error are logged in CloudWatch logs

    Solution with DynamoDB
      Client  --(REST API)-- API Gateway --(PROXY REQUESTS)-- Lambda --(CRUD)-- DynamoDB



  AWS API Gateway
    AWS Lambda + API Gateway: No Infrastructure to manage
    Support websocket protocol
    handle API versioning
    handle different environmenbts (dev, test, prod)
    handle security (authentication & authorization)
    Create API keys, handle request throttling
    Swagger/Open API import to quickly define APIs
    Transform and validate requests and responses
    Generate SDK and API specifications
    Cache API Responses

    API Gateway - Integratin High Level
      Lambda Function
        Invoke lambda function
        Easy way to expose REST API backed by AWS Lambda
      HTTP
        Expose HTTP endpoints in the backend
        e.g.
          Internal HTTP API on premise, ALB.  
          (On the API GW, add rate limiting, caching, auth, API keys, etc)
      AWS Service
        Expose any AWS API throught the API Gateway
        e.g.
          start and AWS Step Function workflow, post a messge to SQS.
          (The API GW add authentication, rate control, deploy publicly, )

      API Gateway - AWS Service Ingegration
        e.g
        Client --http-- API Gateway ---- Kinesis Data Streams --(record)-- Kinesis Data Firehose --(store .json)-- S3

    API Gateway - Endpoint types
      Edge-Optimized (default): for global clients
        Requests are routed through ClientFront Edge location s
        The API Gateway still lives in only one region
      Regional:
        For clients within the same region
        Could manually combine with CloudFront (more control over caching strategies and distribution)
      Private:
        Can only be accessed from VPC using an interface VPC Endpoint (ENI)
        Use resource policy to define access

    API Gateway - Security
      User Authentication through
        - IAM Role (useful for internal applications)
        - Cognito (for extermal user, e.g. mobile users)
        - Custom Authorizer (your own logic, and Lambda function)
      Custom Domain Name HTTPS security throught integration with ACM (AWS Certificate Manager)
        if using Edge-Optimized endpoint, the certificate must be in us-east-1
        if using Regional endpoint, the certificate must be in the API Gateway region
        Must setup CNAME or A-alias record in Route53

    AWS Step Functions (a visual design tool)
      Build serverless visual workflow to orchestrate Lambda functions
      Features: sequence, parallel, conditions, timeout, error handling, etc
      Can integrate with EC2, ECS, API Gateway, SQS, etc
      Possibility of implementing human approval feature
      >> Use cases
         order fullfillment, data processing, web application, any workflow

  Amazon Cognito
    Give users an identity to interact with web or mobile application
    Cognito User Pools:
      Sign in function for app users
      Integrate with API Gateway & ALB
    Cognito Identify Pools (Federated Identity)
      Provide AWS credentials to users so they can access AWS resources directly
      Integrate with Cognito User Pools and an identity provider
    Cognito vs IAM:
      Cognito - hundreds of users, mobile users, authenticate with SAML


    Cognito User Pools (CUP) -- User Features
      Create a serverless database of ser for your web & mobile apps
      Simple login: username/email / password combination
      Password rest
      Email & Phone verification
      Multi-factor authentication (MFA)
      Federated Identifies: user from Facebook, Google, SAML

    Cognito Identity Pools (Federated Identities)
      Get idnetities for 'users' so they obtain temporary AWS credentials.
      Users source can be Cognito User Pools, 3rd party logins, etc..

      User can access AWS services directly or through API Gateway
      The IAM policies applied to the credentials are defined Cognito
      Can be customized based on the user_id for the grained control
      Default IAM roles for authenctate and guest users


        Web/Mobile App ----------(login & get token)----------- 3rd Party (Google/FB/SAML/etc)
              |        \                                                                ^
              |         \---(exchange token for--- Cognito Identity Pools ---validate --| 
              |             tepm AWS credential)
        (direct access)
              |__AWS S3/DynamoDB etc




///////////////////////////////////////
Serverless Solution Architecture

  #
  e.g. Mobile APP for TODO-list, 
    requirements:
    - REST API with HTTPS
    - Serverless architecutre
    - user can directly interact with their own folder in s3 
    - user authenticate through a managed serverless service
    - user can read/write to-do, but mostly read
    - the database should scale, and have some high read througput

              Amazon Cognito
            / (auth)    | (verify auth)
           /            |
    Client --(REST)-- API Gateway ---- Lambda ---- DAX ---- DynamoDB
           \           (cache)                   (cache)
            \ S3

  #
  e.g Serverless Website: my-blog.com
    requirements:
    - scale globally
    - blogs are rarely written, but often read
    - some websites are purely static files, the rest are dynamic REST API
    - Caching must be implemented where possible
    - Any new user subscribes should receive a welcome email
    - Any photo uploaded to the blog should have a thumbnail generated

                            (Origin Access Control)
                                    OAC         upload             thumbnail
                CloudFront (Global) ------ S3 --(trigger) ---- Lambda ---- S3 --- SNS
  upload photos/ (Transfer acceleration)                                      \-- SQS
              /            
        Client --(REST)-- API Gateway ---- Lambda ---- DAX ---- DynamoDB
                           (cache)                   (cache)       |           new user
                                                            DynamoDB_Streams --(trigger)-- Lambda --- SES (EmailService)
              
    Use S3 + CloudFront for Static web content
    The REST API is serverless, no need Cognito becuase it is public
    Use OAC to control the user access to S3, S3 setup bucket policy to only authrize from CloudFront Distribution
    Use DynamoDB (Could also be Aurora Global DB)
    Use DynamoDB Streams + Lambda to trigger new user emails (SES: Simple EMail Service in serverless way)

  # 
  Micro Service Architecture
    Synchronous Patterns:
      API Gateway,  Load Balancers
    Asynchronous Patterns:
      SQS, Kinesis, SNS, Lambda triggers (S3)
  
  # 
  Software updates offloading
    -- CloudFront





////////////////////////////////////////
Databases

  Database Types:
    RDBMS (SQL/OLTP):  RDS, Aurora
    NoSQL:  DynamoDB, ElasticCache, Neptune, DocumentDB, Keyspace,...
    Object Store:  S3/Glacier
    Data Warehourse (SQL Analytics/BI):  Redshift (OLAP), Athena, EMR
    Search:  OpenSearch (JSON)
    Graphs:  Amazon Neptune
    Ledger:  Amazon Quantum Ledger Database (QLD)
    Time servcies: Amazon Timestream


  Amazon RDS
    Managed PostgresQ/MySQL/SQL Server/Oracle/DB2/Custom

  Amazon Aurora
    Compatible API for PostgresQ/MySQL, separation of storage and compute
    HA (6 Replica/3 AZ), Auto-scaling, self-healing.
    Aurora serverless
    Aurora Global  (~16 Read instance/region)
    Aurora Database Clone
    Aurora Machine Learning

  Amazon Elastic Cache
    Managed Redis/Memcached
    Requires some application code changes to be leveraged

  Amazon DynamoDB
    Managed Serverless NoSQL,  (table)
    Capacity mode: Provisioned / on-demand
    DynamoDB Streams, (Event processing)
    DAX  read cache (ms read latency)
    Global Table (Active-Active)
    Repidly evolve schemas

  Amazon DocumentDB
    Fully managed native JSON document DB with MongoDB compatibility.
    ('an AWS implimentation of MongoDB')
    HA with replications across 3 AZ
    DocumentDB storage automatically grows in increments of 10GB
    Auto scales to workloads with millions of RPS.
    DocumentDB does NOT have a Serverless option           *

  Amazon Neptune
    Fully managed graph database
    HA across 3 AZ up to 15 read replicas
    Can store up to billions of relations and query the graph with ms latency
    >> Use Case
       Social network, knowledge graphs (Wikipedia), fraud detection, recommendation engines

    Amazon Neptune - Streams
      Real-time ordered sequence of every change to your graph data
      No duplication, strict order
      Changes are available immediately after writing
      Streams data is accessible in HTTP REST API
      >> Use case:
         Send notification when certain changes are made
         mainain graph data synchronized in another data store (S3, OpenSearch, ElastiCache)
         Replicate data across regions in Neptune

  Amazon Keyspaces (for Apache Cassandra)
    A fully managed Apache Cassandra-compatible DB service
    Serverless, Scalable, HA, 
    Auto scale tables up/down based on the application's traffic
    Tables are replicated 3 times across multiple AZ
    Using Cassandra Query Language (CQL)
    n ms latency at any scale, 1k RPS
    Capacity mode:  provisioned with auto-scaling / on-demand
    Encryption, backup, PITR (Point-in-time Recovery) up to 35 days
    >> User case:
       store IoT devcies info, 
       time-series data

  Amazon QLDB (Quantum Ledger Database)
    ledger -- recording financial transactions
    Fully managed, Serverless, HA, Replication across 3 AZ
    Immutable system. no entry canb e removed or modified, cryptographically verifiable
    2-3x better performance than common ledger blockchain framework, manipulate data using SQL
    Used to review histroy of all the changes made to your applicatin data over time
    vs Amazon Blockchain: no decentralization components

  Amazon Timestream
    Fully managed, scalable, serverless time series DB.
    Auto Scale up/down to adjust capacity
    Store and analyze trillions of events per days
    1000s times faster and 1/10 the cost of RDS
    Scheduled queries, multi-measure records, SQL compatibility
    Data storage tiering: recent data in memory, histyorical data in cost-optimized storage
    Built-in time series analytics functions (identify patterns in data in near real-time)
    Encryption in transit and at rest
    >> Use case:
       IoT app, operatinal app, real-time analysitcs

    Architecture:
      AWS IOT          ------------------------------->                              Amazon QuickSight
      Kinesis Data Stream ---->        Lambda     ---->                            / Amazon SageMaker
                          \__ Kinesis Data Analysics-->  --> Amazon Timestream ---|  Grafana
               Amazon MSK /                                                        \ Any JDBC connection
      Prometheus       ------------------------------->
      telegraf         ------------------------------->




//////////////////////////////////////////////////
Data Analytics


  Amazon Athena
    Serverless query (SQL) service to analyze data store in S3
    ----------        ----                                  ---
    Use SQL to query the files (built on Presto -- OpenSource SQL Query Engine)
    Supports CSV, JSON, ORC, AVro, Parquet
    Pricing:  $5/TB data scanned
    Conmmonly used with Amazon Quicksight for reporting/dashboards

    load data ---> S3 --(Query&Analyze)--> Amazon Athena --(dashboard)--> QuickSight

    >> Use Case:
       Business intelligence /analytics / reporting
       analyze & query VPC flow logs, ELB logs, CloudTrail trails

    Athena - Performance improvement
      Use columnar data for cost-saving
        Apache Parquet or ORC is Recommended
        Use Glue to convert data to Parquet or ORC
      Compress data
      Partition datasets in S3 for easy querying on virtual columns
        e.g.  s3://bucket/path-to-table/<partition_column_1>=<value1>/<partition_column_2>=<value2>/...
      Use larger files (>128M) to minimize overhead

    Athena - Federated Query
      Run SQL queries across data stored in relational, non-relational object and custom data sources (AWS/on-premises)
      Use data source connectors that run on Lambda to run Federated Queries.
      Store the result back in S3
                                            / DynamoDB / DocumentDB / Aurora / ElastiCach / RDS / Redshift
      S3 --- Amazon Athena --- Lambda ------   Database on-premises
                    (data srouce connector) \ HBase in EMR


  RedShift
    OLAP (Online Analytical Processing), for analytics and data warehousing
    based on PostgreSQL, but not used for OLTP (Online Transaction Processing)
    10x better performance than other data warehouses, scale to PBs of data
    Columnar storage of data (instead of row based) & parallel query engine
    Pay as you go based on the instances provisioned
    Has a SQL interface for performing the query
    BI tools such as Amazon QuickSight or Tableau integrate with it
    vs Athena:  Redshift support indexs -> faster queries / joins / aggregations

    Redshift Cluster:
      Leader node
      Compute node
      Provision node size in advanced
      can use reserved instances for cost saving.

      Query --(JDBC/ODBC)-- Redshift Cluster
                              Leader Node
                            Compute-node x m


    Redshift - Snapshots & DR
      Redshift has multi-AZ mode for some cluster
      Snapshots are PIT (point-in-time) backups of a cluster, stored internally in S3
      Snapshots are incremental (only save chagnes)
      You can restore a snapshot into a new cluster
      Automated: every 8 hours, every 5GB, or on schedule, set retention
      Manual:    snapshot is retained until you delete it

      You can configure Redshift to automatically copy snapshots (automated or manual) of a cluster to another region

      [Region 1] Redshift Cluster (original) --(snapshot)--> Cluster Snapshot
                                                                |(auto/manual copy)
      [Region 2] Redshift Cluster (new) <-------(restore)-------


    Loading data into Redshift:  (Large inserts are Much better)
      - Kinesis Data Firehose
        Kinesis Data Firehose ---> Redshift (through S3 copy)
      - S3 copy
                      (internet)
        S3 Bucket  ----------------> RedShift Cluster
                    (through VPC)
        the 'Enhandced' VPC routing forced all COPY and UNLOAD traffic moving between cluster and data repo through VPCs
      - EC2 Instance JDBC driver
        EC2 Instance ---(JDBC)---> RedShift Cluster

    Redshift Spectrum
      Query data that is already in S3 without loading it
      Must have a Redshift cluster available to start the query
      The query is then submitted to thousands of Redshift Spectrum nodes

      Query --(JDBC/ODBC)-- Redshift Cluster
                              Leader Node
                            Compute-node x m
                               /        \
                          Redshift Spectrum x n
                                |   |   |
                                Amazon S3

  
  Amazon OpenSearch Servcie
    OpenSearch is successor to ElasticSearch
    in DynamoDB, query only exist by primary key or indexes.
    but OpenSearch can search any field, even partially matches            *
    it is common to use OpenSearch as a component to another database
    2 modes:
      Managed cluster
      Serverless cluster
    Does not natively support SQL (can be enabled via a plugin)
    Ingestion from Kinesis Data Firehose, AWS IoT, CloudWatch logs
    Security throught Cognito & IAM, KMS encryption, TLS
    COmes with OpenSearch dashboards

    OpenSearch Patterns
      DynamoDB
        --(CRUD)-->DynamoDB Table ----> DynamoDB Stream ----> Lambda ----> Amazon OpenSearch
                      ^ (API:retrieve)                                         | (API:search)
                      |------------------------- App (@EC2)---------------------

      CloudWatch log                                            (real time)
        CloudWatch logs -----> subscription Filter ----> Lambda -----------> OpenSearch
          or                                                                (near real-time)
        CloudWatch logs -----> subscription Filter ----> Kinesis Data Firehose ----------> OpenSearch

      Kinesis Data Stream & Data Firehose
                                                     (near real-time)
        Kinesis Data Streams ----> Kinesis Data Firehose ----------> OpenSearch
                                            | (data transformation)
                                          Lambda
          or
                                         (real-time)
        Kinesis Data Streams ----> Lambda ----------> OpenSearch



  Amazon EMR (Elastic MapReduce)
    EMR helps creating Hadoop clusters (big data) to analyze and process vast amount of data
    The clusters can be made of hundreds of EC2 instances
    EMR comes bundled with Apache Spark, HBase, Presto, Flink
    EMR takes care of all the provisioning and configuration
    Auto-scaling and integrated with Spot instances
    >> User case:
       data processing, ML, web indexing, Big data

    EMR - Node type & purchasing
      - Master Node: Manage cluster, coordinate, manage health -- long running
      - Core Node:   Run tasks and store data -- long running
      - Task Node (optional):  run tasks, usually spot instances
      Purchasing options:
        On-demand: reliable, predicatable, won't terminate
        Reserved:  min 1 year. cost saving, 
        Spot Instances: cheaper, less reliable, for task nodes
    Can have long-run cluster, or transient (temp) cluster

    
  Amazon QuickSight
    Serverless ML powered business intelligence servcie to create interactive dashboards
    ----------                                                    ----------------------
    Fast, automatically scalable, embeddable, 
    per-session pricing
    Integrated with RDS, Aurora, Athena, Redshift, S3
    In-momery computation using SPICE engine, (if data is imported into QuickSight)
    Enterprise edition: support Column-level security (CLS)

    QuickSight Integration:
      RDS, Aurora, Redshift, Athena, S3, OpenSearch, Timestream
      3rd party, e.g. JIRA
      on-premises DB (JDBC)
      Data source import (CSV,JSON,ELF/CLF, etc)

    QuickSight - Dashboard & Analysis
      Define Users (standard versions) and Groups (enterprise version)
        These users/groups only exist within QuickSight, not IAM
      Dashboard:
        a read-only snapshot of an analysis that you can share
        preserves the configuration of the analysis (filtering, parameters, controls, sort)
      You can share the analysis or the dashboard with users or groups    *
      To share the dashboard, you must first publish it
      Users who see the dashboard can also see the udnerlying data


  AWS Glue
    Managed extract, transform, and load (ETL) service
    Useful to prepare and transform data for analystics
    Fully serverless servcie
    e.g
      S3 ------____ (extract)__ Glue ETL ---(load)--->Redshift
      RDS -----                (transform)

    Glue - convert data into Parquet format
    e.g.
      --(S3 Put)--> S3 Bucket --(import CSV)--> Glue ETL --(Parquet)--> S3 --(Analyze)--> Amazon Athena
                       |                           ^
                       |                           |
              Event Notification --------------> Lambda
                                           or EventBridge

    Glue Data Catalog: catalog of datasets
                                                                   Glue jobs (ETL)
      S3/RDS/DynamoDB ----Glue Data Crawler --(write metadata)--> Glue Data Catalog ---> Athena/Redshift Spectrum/EMR


    Glue - things to knowledge
      Glue JOb Bookmark:  prevent re-processing old data
      Glue Elastic Views:
        Combine and replicate data across mutiple data stores using SQL
        No custom code, Glue monitors changes in the source data, serverless
        Leverage a 'virtual table' (materialized view)
      Glue DataBrew:    Clean and normalize data using pre-built transformation
      Glue Studio:      new GUI to create, run and monitor ETL jobs in Glue
      Glue Streaming ETL: (build on Apache Spark Structured Streamoing)
        Compatible with Kinesis Data Streaming, Kafka, MSK (managed Kafka)


  AWS Lake Formation
    Data lake =  central place to have all your data for analytics purposes
    Fully managed servcie
    Discover, cleanse, transform, ingest data into Data Lake
    Automates many complext manual steps (collecting, cleaning, moving, cataloging data) and de-duplicate (using ML Transforms)
    Combine structured and unstructured data in the data lake
    Out-of-the-box source blueprints: S3, RDS, NoSQL, ..
    Fine-grained Access Control for app (row/column)
    Build on top of AWS Glue

      Data Source (S3,RDS, Aurora, On-premises)
                                  |---(ingest)---> Data Lake Formation ---> Athena/Redshift/EMR ---> Users
                                                          |
                                                       Data Lake
                                                     (stored in S3)

    AWS Lake Formation Centralized permissions
    e.g.

      Data Srouces (S3, RDS, Aurora, ...) --(ingest)--> Data Lake Formation ---> Athena/Redshift/EMR --> QuickSight---> Users
                                                          Access Control
           

  Kinesis Data Analytics for SQL Application
    Realtime analytics on Kinesis Data Streams & Firehose using SQL
    Add reference data from S3
    Fully managed, no servers to provision
    auto scaling
    pay for actual consumption retained
    output:
      Kinesis Data streams:  create streams out of real-time analytics queries
      Kinesis Data Firehose: send analystic query result to destination

         Source                     SQL Statement
    Kinesis Data stream     ---> Kinesis Data Analystics ---> Kinesis Data stream  ---> Lambda/App
    Kinesis Data Firehose          for SQL Application        Kinesis Data Firehose --> S3/Redshift/..
                                          |
                                  Reference Data in S3
    >> Use case:
       Time-series analytics
       Real-time dashboards
       Real-time metrics

  Kinesis Data Analytics for Apache Flink
    Use Flink (Java, Scala, SQL) to process and analyze streaming data
      Kinesis Data Streams  ---\___ Kinesis Data Analytics for Apache Flink
      Amazon MSK            ---/
    Run any Apache Flink App on a managed cluster on AWS
      Provisioning computer resources, parallel computation, automatic scaling
      app backups (checkpoints and snapshots)
      use any Flink programming features
      Flink does NOT read from Firehose (use Kinesis Analytics for SQL instead)


  Amazon MSk (Managed Streaming for Apache Kafka)
    Alternative to Kinesis
    Fully managed Apache Kafka on AWS
      allow you to create, update, delete clusters
      MSK cereate & manage kafka broker node & Zookeeper nodes
      Deploy the MSK cluster in VPC, multi-AZ (up to 3 for HA)
      Auto recovery from common Apache Kafka failures
      Data stored in EBS volumes for as long as you want
    MSK Serverless
      Run Kafka on MSK without managing the capacity
      MSK auto provision resources and scales computer & storage

    Kinesis Data Stream      vs      Amazon MSK
    -----------------------------------------------
    1MB message size limit           1MB default (up to 10MB)
    Data Streams with Shards         Kafka Topics with partitions
    Shard Splitting & Merging        Can only add partition to topic
    TLS in-flight encryption         PlainText or TLS in-flight encryption
    KMS at-rest encryption           MKS at-rest encryption

    MSK Consumers
                Kinesis Data Analytics for Apache Flink
      MSK ----> AWS Glue Streaming ETL jobs (by Apache Spark Streaming)
                Lambda
                APPs on EC2


  Big Data Ingestion Pipeline
    requirements
      ingestion pipeline to be fully serverless
      collect data in real time
      transform data
      query the transformed data using SQL
      create reports in S3 for the queries
      laod data into warehouse and create dashboards

    e.g.
                                                                               ________(Pull data)________
               (real-time data)                                               |                           |     (reporting)
      IoT devices -------> Kinesis Data Streams --> Kinesis Data Firehose --> S3 --> SQS ---> Lambda --- Athena ---S3 ---> QuickSight
                                                            |                                                         \___ Redshift   
                                                          Lambda




/////////////////////////////////////////
Machine Learning

  Rekognition: face detection, labeling, celebrity recognition
  Transcribe:  audio to text (e.g. subtitle)
  Polly:       text to audio
  Translate:   translate
  Lex:         build conversation bots - chatbots
  Connect:     cloud contact center
  Comprehend:  NLP (Natual Language Processing)
  SageMaker:   Machine Learning for every developer and data scientist
  Forcast:     build high accurate forcasts
  Kendra:      ML-powered search engine
  Personalize: real-time personalized recommendation
  Textract:    detect text and data in documents
  



//////////////////////////////////
Amazon CloudWatch

  Amazon CloudWatch Metrics
    Metrics belong to Namespaces
    Dimension is an attribute of a metric (instance id, environment, etc)
    Up to 30 dimensions per metric
    Metric has timestamps
    Can create CloudWatch dashboard for metrics
    Can create CloudWatch Custom Metrics

  CloutWatch Metric Streams
    Continually steam Cloudwatch metric to a destination.
    with near-real-time delivery and low latency
    - Kinesis Firehose
    - 3rd party servcie provider: Datadog, Dynatrace, New Relic, Splunk, Sumo Logic, etc
  Option to filter metric to only stream a subset of them

  CloudWatch logs
    Log groups: arbitray name, representing an application
    Log stream: instances within application / log files / containers
    Can difine log expriation policies (never/ 1day ~ 10years)
    CloudWatch can send logs to
      S3 (export)
      Kinesis Data streams
      Kinesis Data Firehose
      Lambda
      OpenSearch
    Logs are encrypted by default
    Can setup KMS-based encryption with your own keys

  CloudWatch Logs - Source
    SDK, CloudWatch Logs Agent, ClodWatch Unified Agent
    Elastic Beanstalk: collection of logs from application
    ECS: collection from containers
    Lambda: collection from function logs
    VPC flow logs: VPC logs
    API Gateway
    CloudTrail based on filter
    Route53

  CloudWatch Logs Insights
    Visualizatin tool
    Search and analyze log data in CloudWatch logs
    Provide a purpose-built query language
      Auto discover fields from AWS servcies and JSON log events
      Fetch desired event filed, filer base on condition, calculate aggregatin statistics, sort events, limit number of events.
      Can save queries and add them to CloudWatch Dashbaords,
    Can query multiple Log Groups in different AWS account
    A query engine, not a real-time engine

  CloudWatch Logs - S3 Export
    Log data can take up to 12 hours to be available for export
    API:  CreateExportTask
    Not (near-)real-time, use Logs subscriptions instead for 'real-time'

  CloudWatch Logs subscriptions
    Real-time log event
    Send to Kinesis Data Steams/Firehose, Lambda
    Subscription Filter
                                                     /--> Lambda --(real-time) --> OpenSearch 
    CloudWatch Lgos --(logs)--> subscription Filter ----> Kinesis Data Streams --> KDF/KDA/EC2/Lambda
                                                     \--> Kinesis Firehose     --> S3

    Cross-Account Subscription: send log event to resource in differnt AWS account (KDS, KDF)
            Account (Sender)                                    Account (Receiver)
      CloudWatch logss --- subscription filter --(log)--> Subscription Dest ---> target(e.g. Kinesis)
                                                            (Dest Access policy)  
                            can be assumed (------------>) IAM role (allow put to target)

  CloudWatch Logs Aggregation Multi-Account & Multi-Regions
    e.g
    Account-A/Region-1 ---> Subscription-Filter --\
    Account-B/Region-2 ---> Subscription-Filter ---> Kinesis Data Stream
    Account-C/Region-3 ---> Subscription-Filter --/

  CloudWatch Agent
  
    e.g. CloudWatch log for EC2
      by default, no logs from EC2 to CloudWatch.
      Need to run CloudWatch agent on EC2 to push log
      Need IAM permission (allow EC2 to send log to CloudWtach)
      CloudWatch log agent can be setup on-premises

    2 types:
    - CloudWatch Logs Agent (old)
      Can only send log to CloudWatch logs
    - Cloudwatch Unified Agent
      Collect additional system-level metrics, e.g. RAM, processes
      Collect log to send to CloudWatch Logs
      Centralized configuration using SSM Parameter Store

    CloudWatch Unified Agent - Metrics
      Collect directly on your Linux / EC2 instance
      e.g
        CPU, DIsk, RAM, NetStat, Processes, swap space, etc


  CloudWatch Alarms
    Alarms used to trigger notification for any metric
    Various options (sampling, %, max, min, etc)
    Alarm status:
      OK, Insufficient_data, ALARM
    Period:
      in sec
      High resolution custom metrics: 10 sec, 30 sec, nx60 sec

    CloudWatch Alarm Targets
      EC2:  Stop / Terminate / Reboot, Recover EC2 instance
      ASG:  trigger Auto Scaling Action
      SNS:  Send notification

    CloudWatch Alarms - Composite Alarms
      -- a combinatino of metrics
      Composite Alarms are monitoring the state of multiple other alarms
      And and OR conditions
      Helpful to reduce 'alarm noise' by creating complex composite alarms

    EC2 Instance Recovery
      

  Amazon Eventbridge (formerly CloudWatch Events)
    
    >> USe case.
      Schedule: Cron jobs.
          e.g. schedule <time> trigger lambda 
      Event Pattern: Event rule to react to a service
          e.g. IAM Root user sign in event --> SNS Topic with Email notification
      Trigger Lambda function.
          e.g. -> trigger SQS/SNS

    Amazone EventBridge Rules.
      Source:  eg. EC2 instance, S3, CodeBuild, CloudTrail, Schedule/Cron, Trusted Advisor
      Filter:
      EventBridge: -> JSON
      Dest:    eg. Lambda, SQS, SNS, KDS, ECS Task, AWS Batch

    Default Event Bus
    Partner Event Bus
    Custom Event BUs

    EventBus can be accessed by other AWS accounts using Resource-based Policies
    Can archive events (all/filter) sent to an event bus (indefinitely or periodically)
    Ability to replay archived events

    EventBridge - Schema Registry
      EventBridge can analyze the events in the bus and infer the schema
      Schema Registry: allow you to generate code for application, that knows how data is structured in the event bus
      Schema can be versioned

    EventBridge - Resource based Policy
      Manage permission for a specific Event Bus
        (allow/deny events from another AWS account/region)
      >> Use case:
        Aggregate all events from your AWS Organization in a single AWS accont or AWS region

    EventBridge - Archive and Replay               *
      >> Use case:
        A serverless app on AWS use EventBridge between services within the app. There is a requirement to store the events > 6 month. 
        What is the most efficent and cost-effective way to store EventBridge events and use them later?
        --> EventBridge Archive and Replay feature


  CloudWatch Insights and Operational Visibility
    Cloudwatch Container Insights
      Collect , aggregate, summarize metrics and logs from containers
      Available for containers on :  ECS, EKS, K8s on EC2, Fargat (both ECS and EKS)
      In Amazon EKS and K8s, CloudWatch Insights is using a containerized version of the CloudWatch Agent to discover containers.

    CloudWatch lambda Insights 
      Mornitoring and troubleshoting solution for serverless applications running on Lambda
      COllects/Aggregates/summarizes system-level metrics and diagnostic information.
      Lambda Insights is provided as a Lambda Layer

    CloudWatch Contributor Insights
      Analyze log data and create time series and display contributor data.
        See metrics about top-N contributors                            *
        The total number of unique contributors and their usage
      works for any AWS generated logs (VPC, DNS, etc)
      build your rules or use AWS sample rules
      CloudWatch provides built-in rules that you can use to analyze metrics from other AWS services

    CloudWatch Application Insights
      automated dashboard to show potential problems with monitored application, help isolate ongoing issues
      applications running on EC2 instances with selected technologies
      Can use other AWS resources
      powered by SageMaker
      Enhance visibility into application
      Findings and alerts are sent to Amazon EventBridge and SSM OpsCenter


  CloudTrail
    Provide governance, compliance and audit for AWS account
    Enabled by default
    Get history of event/API call made with AWS Account: Console, CLI, SDK, AWS Services
    Can put logs from CloudTrail into CloudWatch Logs or S3,  to keep it > 90 days
    A trail can be applied to All Regions (default) or a single Region
    If a resource is deleted in AWS, investigate CloudTrail first

    CloudTrail Event
      - Management Events
        Operations that are performed on resource in AWS account
        By default, trails are configured to log management events
        Can seperate Read / Write event
        E.g  Configuring security, routing rules, logging ...
      - Data Events
        BY default, NOT enabled in trail
        Amazon S3 object-level Activity
        Lambda function execution activity (Invoke API)
        Can seperate Read / Write event
      - CloudTrail Insight Events
        detect unusual activity in account
          (inaccurate resource provisioning, hitting servcie limits, bursts of IAM action, gaps in periodic)
        CloudTrail Insights analyzes normal management events to create a Baseline
        And then continuously analyzes write events to detect unusual patterns
          Anomalies appear in the CloudTrail console, Event is sent to S3, An Eventbridge event is generated

          Management Events --(continues analysis)--> CloudTrail Insights --(generate)--> Insights Events CloudTrail console/S3/EventBridge


    CloudTrail Events Retention
      Events are stored for 90 days in CloudTrail
      To keep events > 90 day, log them to S3 and use Athena



  AWS config
    Helps record configuration and changes over time
    Helps with auditing and recording compliance of AWS resources
    Can receive alert (SNS notification) for any changes
    AWS config is a per-region service
    Can be aggregated across regions and accounts
    Possibility of storing the configuration data into S3 (analyzed by Athena)

    Config Rules:
      AWS managed config rules (over 75)
      customized config rules (must be defined in AWS Lambda)
      Rules can be evaluated/triggered:  for each config change (AND, OR) at regular time intervals
      AWS config rules does not prevent actions from happening
      Pricing: 
        no free tier,  $0.003 per configuration item recorded per region
                       $0.001 per confg rule evaluation per region

    Config Resource
      View compiance of a resource over time
      View configuration of a resource over time
      View CloudTrail API calls pf a resource over time

    Config Rules - Remediations
      Automate remediation of non-compliant resrouces using SSM Automation document
      Use either AWS-Managed Automation Documents or Custom Automation Documents
        - can create custom Automation Documents that invokes Lambda Function
      Can set Remediation Retries if the resource is still non-compliant after auto-remediation

        non-compliant config <--(monitor)-- AWS Config --(trigger)--> Auto-Remidiation Action
              ^                                                                   |
              |-------------------------------------------------------------------

    Config Rules - Notifications
      Use EventBridge to trigger notifications when AWS resources are no-compliant
      Ability to send configuration chagnes and compliance state notifications to SNS

    CloudWatch vs CloudTrail vs Config
      CloudWatch:  for Performance monitoring & dashboards, Event, Log
      CloudTrail:  Record API calls, Global Service
      Config: Record config changes, Evaluate resources against compliance rule, 

      


////////////////////////////////////////////
IAM Advanced

  Organization
    Global service
    Allows to manage multiple AWS accounts
    the main account is the management account, others are member accounts
    Member accounts can only be part of one organization
    Consolidated billing across all accounts - single payment method
    Pricing benefits from aggregated usage (Volume discount for EC2, S3)
    Shared reserved instances and Savings Plans discounts across account       *
    API is available to automate AWS account creation

    Root OU (Root Organization Unit)
      Management Account
      Member Accounts
      Sub OU
        Member Account
    
    Advantages:
      Multi Accounts vs On Account Multi-VPC
      Use tagging standards for billing purposes
      Enable CloudTrail on all accounts, sned logs to central S3 account
      Send CloudWatch logs to central logging account
      Establish Cross Accont Roles for Admin purpose
    Security:  SCP (Service Control Policies)
      IAM policies applied to OU or accounts to restrict Users and Roles
      They do not apply to the management account (full admin power)
      Must have an explicit allow (no allow anything by default)

      SCP: Blacklist , Allowlist strategy

    e.g.
      Root OU:  FullAWSAccess SCP
      Management Account:  DenyAccessAthena SCP  --> Can do anything. it does not impact the access. (Management has full access)
      Sub-OU-1:  DenyRedshift SCP                
        Account A: AuthorizedRedshift SCP        --> Can do anything except Redshift (deny + authorized = deny)
        Sub-OU-1-1: DenyAWSLambda SCP
          Account B:                             --> Can do anything except Redshift and Lambda
        Sub-OU-1-2:
          Account C:                             --> Can do anything except Redshift


  IAM Conditions:
    
    aws:SourceIp        - restrict the client IP from which the API calls are made
    aws:RequestedRegion - restrict the region the API calls are made to
    ec2:ResourceTag     - restrict based on tags
    aws:MultiFactorAuthPresent  - to force MFA

    IAM for S3
      bucket level permission:
        e.g. s3:ListBucket,    arn:aws:s3:::test
      Object level permission:
        e.g. s3:GetObject, s3:PutObject, s3:DeleteObject,   arn:aws:s3:::test/*     *
                                                                            ~~~~
    Resource Policies & aws:PrincipleOrgID
      aws:PrincipalOrgID can be used n any resource policies to restrict access to accounts that are member of an AWS Organization


    IAM Roles vs Resource Based Policies
      2 ways of Cross account access
        - IAM Roles
          when assume a role (user, app or service), you give up your original permission and take the permission assigned to the role
                                                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        - Resource Based Policies
          when using a resource-based policy, the principle doesn't have to give up his permissions
      e.g
        User Account A ----(Role Account B)----> S3
        User Account A ----(S3 Bucket Policy)----> S3
      Supported by: Amazon S3 buckets, SNS topics, SQS Queues, etc
      e.g  User A needs to scan a DynamoDB in Account A and dump it in S3 bucket in account B.
           Resource policy is recommended in this case

      Amazon EventBridge - Security
        When a rule runs, it needs permissions on the target.
                                                                                          *
        Resource-based policy: Lambda, SNS, SQS, S3, API Gateway...
        IAM Role: ECS task, Kinesis stream, SYstems Manager Run Command...


    IAM Permission Boundaries
      IAM Permission Boundaries are supported for users and roles (not groups)
      Advanced feature to use a managed policy to set the max permission an IAM entity can get.
      Can be used in combinations of AWS Organizations SCP

      e.g
                 [ Identity-based policy]             
        [ Organizations SCP ]
                      [ Permission boundary ]
                      |     |
                      |<--->|
                effective permissions

    IAM Policy Evaluation Logic
      Deny evaluation - Organization SCPs - Resource-based policies - Identity-based policies - IAM permissions boundaries - Session policies

    IAM Identicy center
    (successor to AWS Single Sign-On)
      One login (single sign-on) for all your:
        AWS accounts in AWS Organization
        Business cloud applications (e.g. MS 365)
        SAML2.0-enabled applications
        EC2 Windows Instances
      Identity providers
        Built-in identity store in IAM Identity Center
        3rd party: AD OneLogin, Okta

      Browser Interface --(login)--> IAM Identity Center ---(SSO)---> AWS Cloud / Business CloudApp / Custom SAML2.0-enabled app
                                             |
                                             |(store/retrieve user identities)
                                             |
                                     AD (User & Group)
                                IAM built-in Identity Store


      Permission-set
        -- a collection of one or more IAM Policies assigned to users and groups to define AWS access
        assign specific permission-set to the group in IAM Identity Center,

      Fine-grained permissions and assignments
        Multi-Account permissions
          Permission-set
        Application assignments
          SSO access to many SAML2.0 apps
          provide required URLs, cers, metadata
        Attribute-Based Access Control (ABAC)
          Fine-grained permissions based on users' attributes stored in IAM Identity Center Identity Store


...




  ACM (AWS Certificate Manager)
    provision, manage and deploy TLS Certificate
    Support both private and public TLS certificates
    Auto TLS certificate renewal
    Integration with:  ELB, Cloud Front, API Gateway
    Cannot use ACM with EC2

    ACM - Requesting Public Certificates
      List domain names
      Select Validation Method: DNS / Email
      Public Certificate will be enrolled for auto renewal -- 60 days before expiry

    ACM - Importing Public Certificate
      No auto renewal
      ACM sends daily expiration event starting 45 days prior to expiration
      AWS Config has a managed rule (acm-certificate-expiration-check) to check for expiring certs (configurable number of days)

    ACM - Integration with ALBs
      Redirect HTTP to HTTPS

    API Gateway - Endpoint Types
      Edge-Optimized (default) -- for global clients. Requests routed through CloudFront,  The API Gateway lives in one region
      Regional -- for client within same region
      Private  -- accessed from VPC using ENI with resource policy

    ACM Integration with API Gateway
      - Need to create a Custom Domain Name in API Gateway
      - Edge-Optimized GW (default): 
        The API Gateway lives in only one region
        The TLS Certificate must be in the same region as CloudFront (us-east-1)
        Then setup CNAME or A-Alias (better) record in Route53
      - Regional:
        For client within the same region
        The TLS certificate must be imported on API Gateway, the the same region as API stage
        Then setup CNAME or A-Alias (better) record in Route53



  WAF (Web Application Firewall)
    Layer &7 in HTTP

    Deploy on:                             *  
      - ALB
      - API Gateway
      - CloudFront
      - AppSync GraphQL API
      - Cognito Pool

    Web ACL Rules:
      IP Set: up to 10k IP
      HTTP header/boday, URI, XSS (Cross-site Scripting)
      geo-match (block countries)
      Rate-based rules - for DDoS
      Web ACL are regional except for CloudFront
      A rule group is a reusable set of rules that can be added to web ACL

    WAF - Fixed IP while using WAF with a Load Balancer
      WAF does not support Network Load Balancer (L4)
      We can use Global Accelerator for fixed IP and WAF on the ALB

        Users --- Global Accelerator (fixed IP) --- ALB ------- EC2
      WAF in the same region as the ALB              |                                       |
                                                    WAF

  AWS Shield: protect from DDoS (Distributed Denial of Service)
    AWS Shield Standard:
      Free
      protection from attacks like SYN/UDP floods, Reflection attacks and other L3/L4 attacks
    AWS Shield Advanced:
      $3000 / month per organization
      advanced protection (ALB, CLB, NLB, Elastic IP, CloudFront)
      24/7 support team - DRP (DDoS Response team)
      auto create/evaluate/deploy WAF rules to mitigate L7 attacks.


  AWS Firewaall Manager
    Manage rules in all accounts of an AWS organization.

    Security policy: common set of security rules:
      WAF rules
      AWS Shield Advanced
      Security Groups for EC2, ALB and ENI resource in VPC
      AWS Network Firewall (VPC level)
      Route53 Resover DNS Firewall
      Policies created at the region level

    Rules are applied to new resources as they are created across all and future accounts in your organization

  VAF vs Firewall Manager vs Shield
    WAF, Firewall Manger and Shield are used together for comprehensive protection

    WAF -- Define Web ACL rules
           For granular protection on your resources
    Firewall Manager
        -- Use WAF across accounts, 
           accelerate WAF configuration, 
           auto protect new resources
    Shield Advance 
        -- additional features on top of WAF to protect from L7 attacks


  Amazon GuardDuty
    Intelligent Threat discovery to protect AWS Account
    Uses Machine Learning algorithms, anomaly detection, 3rd party data
    Input data includes:
      CloudTrial Events (Manangement/S3 Data events) Log -- unusual API calls, unauthrized deployemnts
      VPC Flow logs -- unusual internal traffic, unusual IP addresses
      DNS Logs      -- compromized EC2 instances seding encoded data within dNS queries
      Optional Features -- EKS Audit Logs, RDS & Auraora, EBS, Lambda, S3 Data Events...
    Can setup EventBridge rules to be notified fin case of Findings
    EventBridge rules can target AWS Lambda or SNS
    Can protect agiainst CryptoCurrency attack.


  Amazon Inspector
    Automated Security Assessments
    Only for EC2 instances / Container Images / Lambda Functions
      EC2: 
        leverage SSM
        Analyze unintended network accessibility
        Analyze OS vulnerabilities 
      Container Imanges
        Assessment of container images
      Lambda Functions
        identify software vulnerabilities in code and package dependencies

    Reporting & integration with Security Hub
    Send findings to Event Bridge

    Continueous scanning of the infrastructure only when needed
    Package vulnerabilities (EV2, ECR & Lambda)  -- database of CVE
    Network reachability
    A risk score is associated with all vunlerablities for prioritization


  Amazon Macie
    a fully managed data security and privacy service
    use Machine Learning and pattern matching to discover and protect sensitive data.
    Macie helps identify and alert yoiu to sensitive data (PII -- Persenal Identifiable Information)
    eg.
      S3 bucket --(analyze)--> Macie --(notify)--> Eventbridge
                           discover PII



    Q:            *
    How to share an AMI that has an encrypted EBS snapshot using KMS CMK?  -- need to share the KMS CMK as well to the other AWS Account
    How long time does the KMS key rotate?    --- 1Y
    How to do 3months rotation for the customer-manged CMK in KMS?    -- manually rotation, create new KMS CMK and use key aliases to refer to new KMS CMK,
                                                                         keep the old CMK to decrypt the old data
    What should you use to control access to your KMS CMKs?    -- KMS Key Policies
    What AWS servcie can be used to store/track-version secret data    -- SSM Parameter Store
    Which service checks your image OS vulnerability    -- AWS Inspector
    Which service centrally manage EC2 Security Groups and AWS Shield Advanced accross all AWS account in you organization?
        -- AWS Filewall Manager
    Which service protect your sensitive data stored in S3?    -- Macie
    Why the SSE-KMS encrypted S3 bucket object failed to be replicated to another bucket in the same AWS regaion but with a differnt KMS Key.
        -- 1) the S3 replicateion, Target bucket and target KMS key need to be configured
           2) have to config permissions for obth source KMS Key (kms:Decrypt) and Target KMS Key (kms:Encrypt) to be used by S3 Replication Service
    Which service can be used to manage security groups and WAF rule across multiple account?   -- AWS Firewall Manager




///////////////////////////

VPC

  NAT-instance vs NAT-Gateway
    NAT-instance:  self managed
                   self maintain the EC2, SG, No auto HA (can use script for failover)
                   use Bastion host
    NAT-Gateway:   AWS managed. 
                   Auto Instance, SG, HA
                   No need basion host


  NACL
    NACL like filewall that control traffic from/to subnets
      (a good way to block specific IP at subnet level)
    1 NACL per subnet
    new subnets are assigned with default NACL (allows everything in/out for the subnet)
    define NACL rules:
      - rule number 1-32766,   lower number higher precidence
      - First rule match will drive the decision
        e.g  #100 has higher precedence over #200, so 10.0.0.10/32 is allowed
          #100 ALLOW 10.0.0.10/32
          #200 DENY 10.0.0.10/32
      - the last rule is *          - that DENY everything
    newly created NACLs deny everything 
    Default NACL -- Accept everything inbound/outbound with the subnets it associate with

    The traffic are check by both NACL and SG. should satisfy both to get pass
    e.g.
        Rule    Type     Protocol  Port    Source   Allow/Deny
        #100  all-IPv4    All      All   0.0.0.0/0   ALLOW
          *   all-IPv4    All      All   0.0.0.0/0   DENY
    
                                  NACL        SG      EC2 Instance
                                       ------------- subnet --------------
                              
      -- inbound traffic --> NACL-inbound -->   SG-inbound -->
                                                                EC2 Instance
     <-- outbound traffic -- NACL-outbound <--  SG-outbound <--
                                                (stateful)

    Ephemeral Ports:
      IANA & Windows:  49152-65535
      Linux Kernels:   32768-60999

    NACL with Ephemeral Ports:
      e.g
        Web Subnet (Public)          NACL                NACL         DB Subnet (Private)
                                allow TCP 3306       Allow TCP 3306
             client    --out--> to DB subnet  -----> from web Subnet -->     DB
          ephemeral-port                                                    instance
                       <--in-- Allow In TCP         Allow out TCP            (3306)
                               port 1024~65535 <--- port 1024~65535  <--  


    SG vs NACL:
      SG:   stateful  (SG inbound rule satisfied, then corresponding outbound allowed) 
            Instance Level 
            support ALLOW rules only  
            All rules evalueated before decision     
            Applies to EC2 instance when specified
      NACL: stateless (explicit/separate inbound and outbound rule)
            Subnet Level
            support ALLOW/DENY rules
            Rules are evaluated in order, first match wins
            Auto applies to all EC2 instances in the subnet it associated with


  VPC Peering

    Privately connect 2 VPCs using AWS network
    make them behave as if they are in the same network
    Must NOT have overlapping CIDRs
    VPC Peering connection is NOT transitive (must be established for each VPC)
    You MUST update route tables in each VPC's subnets to ensure EC2 instances can communicate with each other
    VPC Peering connection can be created ACROSS REGION (between VPCs in different accounts/regions)
    You can reference a security group in a peered VPC (works across account - same region)


  VPC Endpoint
    (AWS PrivateLink)
    To access AWS service via AWS private network instead of going throught the public network.
      (no need for IGW, NAT-GW, etc to access AWS Services)

    Every AWS service is poublic exposed (public URL), so that's one option for accessing. VPC Endpoint is another.
    VPC Endpoints are redundant and scale horizontally.
    In case of issues:
      check NDS for VPC
      check Route tables

    Types of VPC Endpoint
      Interface Endpoint
        Provision an ENI (Private IP) as an entry point (must attach a SG)
        Support most AWS service
        $ per hour + $ per GB of data processed
      Gateway Endpoint
        Provisions a gateway and must be used as a target in the route table (Does not use SG)
        Only support 2:  S3 and DynamoDB
        Free

    Interface Endpoint vs Gateway Endpoint
      Gateway is mostly likely to be preferred.  it is free.
      Interface Endpoint is prefered when:
        access is required from on-premises (site to site VPN or Direct Connect)
        access is required from a different VPC on a different region
      In most cases, Gateway Endpoint is preferred ( in exam)            *


    VPC Flow Logs
      Capture information about the IP traffic going into your interface
        VPC Flow logs
        Subnet Flow logs
        ENI (Elastic Network Interface) Flow Logs
      Flow Logs data can go to: S3, CloudWatch Logs, KDF (Kinesis Data Firehose)
      Caputure information from AWS managed interfaces: ELB, RDS, ElasticCache, Redshift, WorkSpace, NAT-GW, Transit-GW
      Can query VPC Flow Logs on S3-Athena or CloudWatch-Logs.


  Site-to-Site VPN
    
    VGW (Virtual-Private-Gateway)
      VPN Concentrator on AWS side of the VPN Connection
      Possibility to customize the ASN (Autonomous System Number)
    CGW (Customer Gateway)
      Software application or physical device on customer side (on-premises) of the VPN connection

    CGW IP address:
      scenario:
        public IP
        private IP (behind NAT)
      Note: 
        Need to enable the "Route Propagation" for the VGW in the route table that is associated with your subnets.
        Need to enable ICMP on the EC2 inbound SG.


    AWS VPN CoudHub
      a 'hub' to connect multiple sites.
      VPN CloudHub : CGW = 1:N

      Secure communication between multiple sites
      Low-cost hub-and-spoke mode between different locations
      Goes over public internet (using VPN connection)
      to set it up, 
        connect multiple VPN connections on the same VGW,
        setup dynamic routing and configure route tables.


  Direct Connection (DX)
    Provide dedicated private connection from remote network to VPC

    Need to setup VGW (Virtual Private Gateway) on VPC
    Access both public resource (S3) and private (EC2) on same connection
    Support both IPv4 and IPv6
    >> Use Cases:
      Increase bandwidth throughput, and lower cost (private connection)
      More consistent network experience -- application use real-time data feeds
      Support Hibrid environments (on-premises and cloud)

      Region                            Direct Connect                         Corporate (customer)
          VPC                              location                                Data Center 
          Private Subnet
            EC2 instance  --- [VGW] --- [Direct Connect] --- [Curstomer] --- Customer router/firewall
                           ___________/     Endpoint            router 
                     S3  / 


    Direct Connect Gateway
      When connect to multiple VPCs in different region (same account), Direct Connect Gateway is required.

      Region-1::VPC1  \                           (Private inf)
                       |--- [Direct Connect Gateway] ----- [Direct Connect] ---- Customer network
      Regopm-2::VPC2  /

    Direct Connect - Connection Type:
      Dedicated Connection:  1G, 10G, 100G
        Physical eth port dedicated to customer
        Request made to AWS first, then completed by AWS Direct Connect Partners
      Hosted Connections:  50M, 500M, to 10G
        Connection requests are made by AWS Direct Connect Partners
        Capacity can be added/removed on demand
      Lead times >= 1 month to establish new connection

    Direct Connect - Encryption
      Data in transit is NOT encrypted but is private
      AWS Direct Connect + VPN (IPsec-encrypted) solution can provide 'encryption'

    Direct COnnect - Resiliency
      High Resiliency:  1+1 Direct Connect Locations
        One connection at multiple locations   
      Max Resiliency:   2 x (1+1) Connect Locations
        Two connections at multiple locations respectively
 
    Direct Connect + Site-to-Site VPN connection
      1 + 1 connection between VPC and Corporate (on-premises)
        Primary:  Direct Connect (expensive)
        Backup:   Site-to-Site VPN


  Transit Gateway
    'Star' topo
    for having transitive peering between thousands of VPC and on-premises

    Regional resource, can work cross-region (peer Transit Gateway across regions)
    Share cross-account using RAM (Resource Access Manager)
    Route Tables: limit which VPC can talk with wich VPC
    Works with DIerct Connect Gateway and VPN Connection
    Support Multicast (the only service in AWS that support multicast)    *

             Direct Connect
        VPC     Gateway       VPC
            \      |        / 
             Transi Gateway 
            /      |        \
        VPC       VPN         VPC
               Connection

    Transit Gateway: Site-to-Site VPN ECMP
      Routing strategy to allow to forward a packet over multiple best path
      >> Use case:
        create multiple site-to-site VPN connections to increase bandwidth

        VPC(s) ===(multiple routes)=== Transit Gateway --- VPN --- Corp DC

    Transit Gateway - Share Direct Connect between multiple accounts

      Region:Account-1: VPC                                                            Direct Connect
                            \__ Transit Gateway --- Direct Connect GW --- Transit VIF ---  endpoint --- Customer DC
                            /
      Region:Account-2: VPC












---
The Study Note is from the udemy tutorial "Ultimate AWS Certified Solutions Architect Associate SAA-C03"